{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f4e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances,pairwise_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from metric_learn import NCA\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab82adb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>7982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>7990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>7993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>7998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id\n",
       "0             9\n",
       "1            18\n",
       "2            21\n",
       "3            25\n",
       "4            31\n",
       "...         ...\n",
       "2375       7982\n",
       "2376       7990\n",
       "2377       7993\n",
       "2378       7994\n",
       "2379       7998\n",
       "\n",
       "[2380 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_test_df = pd.read_csv('test_ids_in_prediction.csv')\n",
    "id_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235ca638",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_unique.csv')\n",
    "X_val = pd.read_csv('X_val_unique.csv')\n",
    "X_test = pd.read_csv('X_test_unique.csv')\n",
    "\n",
    "y_train = pd.read_csv('y_train_unique.csv')\n",
    "y_val = pd.read_csv('y_val_unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cac0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.set_index('unique_id',inplace=True)\n",
    "X_val.set_index('unique_id',inplace=True)\n",
    "X_test.set_index('unique_id',inplace=True)\n",
    "y_train.set_index('unique_id',inplace=True)\n",
    "y_val.set_index('unique_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e06d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_experienced = X_train[X_train['experience_flag']==1]\n",
    "y_train_experienced = y_train[y_train.index.isin(list(X_train_experienced.index))]\n",
    "X_val_experienced = X_val[X_val['experience_flag']==1]\n",
    "y_val_experienced = y_val[y_val.index.isin(list(X_val_experienced.index))]\n",
    "X_test_experienced = X_test[X_test['experience_flag']==1]\n",
    "\n",
    "X_train_cold = X_train[X_train['experience_flag']==0]\n",
    "y_train_cold = y_train[y_train.index.isin(list(X_train_cold.index))]\n",
    "X_val_cold = X_val[X_val['experience_flag']==0]\n",
    "y_val_cold = y_val[y_val.index.isin(list(X_val_cold.index))]\n",
    "X_test_cold = X_test[X_test['experience_flag']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef482445",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under = pd.read_csv('X_train_under.csv')\n",
    "y_train_under = pd.read_csv('y_train_under.csv')\n",
    "\n",
    "X_train_under.set_index('unique_id',inplace=True)\n",
    "y_train_under.set_index('unique_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c531cf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_experienced_under = pd.read_csv('X_train_experienced_under.csv')\n",
    "y_train_experienced_under = pd.read_csv('y_train_experienced_under.csv')\n",
    "\n",
    "X_train_experienced_under.set_index('unique_id',inplace=True)\n",
    "y_train_experienced_under.set_index('unique_id',inplace=True)\n",
    "\n",
    "y_train_experienced_under['female_label'].sum()/y_train_experienced_under['female_label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce078ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cold_under = pd.read_csv('X_train_cold_under.csv')\n",
    "y_train_cold_under = pd.read_csv('y_train_cold_under.csv')\n",
    "\n",
    "X_train_cold_under.set_index('unique_id',inplace=True)\n",
    "y_train_cold_under.set_index('unique_id',inplace=True)\n",
    "\n",
    "y_train_cold_under['female_label'].sum()/y_train_cold_under['female_label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3affd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class coarse_class_classifier_numeric:\n",
    "    \n",
    "\n",
    "    def __init__(self, min_class_size,target_name='y',target_mean=None,coarse_class_table=None):\n",
    "                    \n",
    "        self.min_class_size = min_class_size\n",
    "        self.target_name = target_name   \n",
    "        self.target_mean = 0\n",
    "        self.coarse_class_table = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        self.target_mean = y.mean()\n",
    "        \n",
    "        for col in X.columns:\n",
    "            dt = DecisionTreeClassifier(min_samples_leaf=self.min_class_size)\n",
    "            dt.fit(X[X[col].isnull()==False][[col]],y[y.index.isin(X[X[col].isnull()==False].index)])\n",
    "\n",
    "            dt_df = pd.merge(pd.DataFrame(dt.predict_proba(X[X[col].isnull()==False][[col]])[:,1],columns=['dt'],index=X[X[col].isnull()==False][[col]].index),X[X[col].isnull()==False][[col]],how='left',left_index=True,right_index=True)\n",
    "            dt_df = pd.merge(dt_df, y[y.index.isin(X[X[col].isnull()==False].index)], how='left',left_index=True,right_index=True)\n",
    "\n",
    "            roc = roc_auc_score(dt_df[self.target_name],dt_df['dt'])\n",
    "            \n",
    "            dt_df = dt_df.groupby('dt').agg({col:[min,max,'count'],self.target_name:np.mean})\n",
    "            dt_df.columns = ['class_min','class_max','class_count','class_target_mean']\n",
    "            dt_df = pd.concat([dt_df,pd.DataFrame({'class_min':[np.nan],'class_max':[np.nan],'class_count':[X[X[col].isnull()][[col]].shape[0]],'class_target_mean':[y[y.index.isin(X[X[col].isnull()].index)].mean()[0]]})])\n",
    "            dt_df.reset_index(inplace=True)\n",
    "            dt_df['class_no'] = np.where(dt_df['class_min'].isnull(),'nulls',dt_df.index + 1)\n",
    "            dt_df['class_percent'] = dt_df['class_count'] / X.shape[0]\n",
    "            dt_df['class_target_deviation'] = (dt_df['class_target_mean'] - self.target_mean[0]) / self.target_mean[0]\n",
    "            dt_df['variable'] = col\n",
    "            dt_df['roc'] = roc\n",
    "            dt_df.drop('index',axis=1,inplace=True)\n",
    "            self.coarse_class_table = pd.concat([self.coarse_class_table,dt_df])\n",
    "            del dt_df\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "\n",
    "        base_df = X.drop(X.columns,axis=1)\n",
    "        for col in X.columns:\n",
    "            #print(col)\n",
    "            cc_table = self.coarse_class_table[self.coarse_class_table['variable']==col]\n",
    "            nulls_count = cc_table[cc_table['class_no']=='nulls']['class_count'].values[0]\n",
    "            nulls_target_mean = cc_table[cc_table['class_no']=='nulls']['class_target_mean'].values[0]\n",
    "            cc_table = cc_table[cc_table['class_no']!='nulls']\n",
    "            nulls_target_mean = float(np.where(nulls_count>=self.min_class_size,nulls_target_mean,self.target_mean))\n",
    "        \n",
    "            x_df = X[[col]]\n",
    "            x_df['Z_'+col] = np.nan\n",
    "            x_df['Z_'+col] = np.where(x_df[col].isnull(),nulls_target_mean,x_df['Z_'+col])\n",
    "\n",
    "            lenn = cc_table.shape[0]\n",
    "            for cc_ind in range(cc_table['class_no'].astype(int).min(),cc_table['class_no'].astype(int).max()+1):\n",
    "                maxx = cc_table[cc_table['class_no']==str(cc_ind)]['class_max'].values[0]\n",
    "                meann = cc_table[cc_table['class_no']==str(cc_ind)]['class_target_mean'].values[0]\n",
    "                x_df['Z_'+col] = np.where((x_df['Z_'+col].isnull()),meann,x_df['Z_'+col])\n",
    "\n",
    "            #if cc_table[cc_table['class_no']==str(cc_ind+1)].shape[0]>1:\n",
    "            #    meann = cc_table[cc_table['class_no']==str(cc_ind+1)]['class_target_mean'].values[0]    \n",
    "            #    x_df['Z_'+col] = np.where((x_df['Z_'+col].isnull())&(x_df[col]>maxx),meann,x_df['Z_'+col])\n",
    "#\n",
    "            x_df.drop(col,axis=1,inplace=True)\n",
    "\n",
    "            base_df = pd.merge(base_df,x_df,how='left',right_index=True,left_index=True)\n",
    "            del x_df\n",
    "\n",
    "        return base_df\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "    def get_coarse_class_table(self):\n",
    "        return self.coarse_class_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5271bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.coarse_class_classifier_numeric at 0x19e45954820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = coarse_class_classifier_numeric(min_class_size=50,target_name='female_label')\n",
    "cc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a288c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-900ac8c47a72>:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_df['Z_'+col] = np.nan\n",
      "<ipython-input-20-900ac8c47a72>:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_df['Z_'+col] = np.where(x_df[col].isnull(),nulls_target_mean,x_df['Z_'+col])\n",
      "<ipython-input-20-900ac8c47a72>:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_df['Z_'+col] = np.where((x_df['Z_'+col].isnull()),meann,x_df['Z_'+col])\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "X_train_cc = cc.transform(X_train)\n",
    "X_val_cc = cc.transform(X_val)\n",
    "X_test_cc = cc.transform(X_test)\n",
    "\n",
    "X_train_under_cc = cc.transform(X_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fec10a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z_sellingprice_count_basket      False\n",
       "Z_sellingprice_count_favorite    False\n",
       "Z_sellingprice_count_order       False\n",
       "Z_sellingprice_count_search      False\n",
       "Z_sellingprice_count_visit       False\n",
       "                                 ...  \n",
       "Z_time_btw_two_orders_mean       False\n",
       "Z_time_btw_two_orders_max        False\n",
       "Z_time_btw_two_orders_min        False\n",
       "Z_transaction_count              False\n",
       "Z_experience_flag                False\n",
       "Length: 1539, dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_cc.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839fb4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1',C=1,solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d529b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_cc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a977f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pd.DataFrame(lr.predict_proba(X_train_cc)[:,1],columns=['pred'],index=X_train_cc.index)\n",
    "pred_val = pd.DataFrame(lr.predict_proba(X_val_cc)[:,1],columns=['pred'],index=X_val_cc.index)\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "pred_train['pred_binary'] = np.where(pred_train['pred']>0.5,1,0)\n",
    "pred_val['pred_binary'] = np.where(pred_val['pred']>0.5,1,0)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])\n",
    "\n",
    "err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "err_rate_val = err_rate1_val+err_rate0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf3fbde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587015591527913"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d78202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48898215055799227"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9d9fb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_TRENDYOLMÄ°LLA</th>\n",
       "      <td>0.736971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_basketand_order_max</th>\n",
       "      <td>0.691350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_PL Woman</th>\n",
       "      <td>0.584203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_Elektronik</th>\n",
       "      <td>0.571614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_Erkek</th>\n",
       "      <td>0.564160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_hourly_favorite_secs_btw_consecutives_median</th>\n",
       "      <td>-0.574231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_daily_favorite_secs_btw_consecutives_median</th>\n",
       "      <td>-0.590596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_KadÄ±n</th>\n",
       "      <td>-0.797179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_min_KadÄ±n</th>\n",
       "      <td>-1.023228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_KadÄ±n</th>\n",
       "      <td>-1.660083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                coefficient\n",
       "Z_sellingprice_count_TRENDYOLMÄ°LLA                 0.736971\n",
       "Z_time_btw_basketand_order_max                     0.691350\n",
       "Z_sellingprice_count_PL Woman                      0.584203\n",
       "Z_sellingprice_median_Elektronik                   0.571614\n",
       "Z_sellingprice_sum_Erkek                           0.564160\n",
       "...                                                     ...\n",
       "Z_hourly_favorite_secs_btw_consecutives_median    -0.574231\n",
       "Z_daily_favorite_secs_btw_consecutives_median     -0.590596\n",
       "Z_sellingprice_median_KadÄ±n                       -0.797179\n",
       "Z_sellingprice_min_KadÄ±n                          -1.023228\n",
       "Z_sellingprice_sum_KadÄ±n                          -1.660083\n",
       "\n",
       "[1539 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lr.coef_.T,columns=['coefficient'],index=X_train_cc.columns).sort_values(by='coefficient',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a017e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.DataFrame(lr.predict_proba(X_test_cc)[:,1],columns=['pred'],index=X_test_cc.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb495c",
   "metadata": {},
   "source": [
    "* Submission 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a881c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.984,0.964,0.976,0.984,0.982,0.993,0.654,0.573,0.858,0.955,0.448,0.127,0.932,0.989,0.984,0.989,0.326,0.125,0.979,0.918,0.989,0.964,0.976,0.977,0.985,0.997,0.898,0.984,0.933,0.955,0.99,0.946,0.948,0.942,0.956,0.988,0.73,0.72,0.945,0.969,0.955,0.777,0.978,0.982,0.989,0.966,0.98,0.596,0.511,0.188,0.298,0.991,0.892,0.049,0.202,0.072,0.951,0.948,0.988,0.979,0.994,0.725,0.141,0.979,0.995,0.649,0.969,0.946,0.932,0.992,0.981,0.959,0.91,0.965,0.836,0.749,0.979,0.992,0.967,0.903,0.444,0.855,0.996,0.193,0.977,0.966,0.748,0.994,0.972,0.961,0.992,0.971,0.788,0.98,0.485,0.872,0.859,0.881,0.598,0.969,0.955,0.989,0.995,0.918,0.579,0.568,0.647,0.753,0.929,0.62,0.949,0.984,0.973,0.988,0.819,0.982,0.966,0.957,0.98,0.812,0.991,0.996,0.556,0.907,0.994,0.904,0.972,0.986,0.16,0.082,0.986,0.639,0.095,0.976,0.93,0.986,0.648,0.983,0.802,0.995,0.258,0.772,0.948,0.969,0.592,0.951,0.972,0.956,0.262,0.912,0.996,0.974,0.975,0.99,0.515,0.902,0.977,0.924,0.982,0.912,0.961,0.974,0.951,0.739,0.994,0.957,0.728,0.943,0.958,0.332,0.987,0.612,0.69,0.991,0.98,0.933,0.958,0.879,0.493,0.872,0.317,0.251,0.895,0.434,0.193,0.986,0.982,0.989,0.994,0.868,0.985,0.349,0.431,0.9,0.908,0.248,0.979,0.444,0.985,0.643,0.852,0.977,0.994,0.988,0.913,0.732,0.305,0.237,0.354,0.221,0.974,0.948,0.505,0.996,0.965,0.545,0.98,0.93,0.531,0.983,0.874,0.951,0.791,0.797,0.95,0.452,0.08,0.855,0.797,0.111,0.05,0.92,0.865,0.987,0.383,0.976,0.988,0.862,0.989,0.979,0.125,0.974,0.935,0.967,0.997,0.312,0.946,0.204,0.97,0.964,0.826,0.122,0.949,0.985,0.114,0.812,0.995,0.987,0.483,0.193,0.975,0.901,0.347,0.941,0.988,0.98,0.818,0.992,0.92,0.139,0.894,0.956,0.986,0.354,0.974,0.878,0.983,0.926,0.972,0.948,0.985,0.992,0.161,0.95,0.882,0.151,0.921,0.938,0.72,0.967,0.274,0.12,0.566,0.897,0.428,0.154,0.988,0.215,0.717,0.956,0.973,0.973,0.961,0.952,0.998,0.987,0.962,0.97,0.954,0.966,0.996,0.976,0.189,0.919,0.825,0.297,0.996,0.44,0.582,0.536,0.7,0.877,0.37,0.507,0.971,0.878,0.285,0.929,0.887,0.703,0.156,0.976,0.165,0.985,0.957,0.832,0.942,0.994,0.079,0.634,0.901,0.951,0.952,0.961,0.802,0.972,0.858,0.085,0.76,0.992,0.362,0.353,0.968,0.934,0.512,0.984,0.848,0.989,0.982,0.992,0.992,0.951,0.426,0.955,0.933,0.865,0.115,0.983,0.98,0.11,0.981,0.854,0.968,0.995,0.863,0.938,0.979,0.994,0.923,0.913,0.926,0.955,0.13,0.852,0.979,0.889,0.151,0.992,0.995,0.905,0.087,0.969,0.727,0.995,0.841,0.985,0.993,0.993,0.995,0.969,0.861,0.983,0.983,0.69,0.952,0.744,0.239,0.169,0.92,0.181,0.153,0.325,0.954,0.892,0.89,0.16,0.9,0.341,0.599,0.687,0.99,0.944,0.957,0.975,0.53,0.599,0.525,0.186,0.971,0.746,0.905,0.168,0.173,0.989,0.463,0.973,0.962,0.861,0.481,0.993,0.355,0.845,0.989,0.428,0.924,0.712,0.595,0.986,0.04,0.522,0.961,0.813,0.238,0.988,0.86,0.913,0.986,0.993,0.988,0.985,0.989,0.665,0.985,0.84,0.915,0.973,0.142,0.808,0.109,0.988,0.748,0.39,0.153,0.987,0.786,0.948,0.959,0.967,0.435,0.885,0.204,0.778,0.988,0.198,0.162,0.375,0.979,0.958,0.82,0.137,0.138,0.873,0.475,0.89,0.618,0.991,0.957,0.588,0.393,0.091,0.993,0.859,0.996,0.647,0.305,0.993,0.474,0.99,0.957,0.706,0.939,0.858,0.9,0.651,0.291,0.282,0.075,0.642,0.8,0.101,0.364,0.205,0.233,0.516,0.97,0.989,0.96,0.993,0.99,0.979,0.058,0.99,0.541,0.063,0.991,0.846,0.639,0.131,0.818,0.781,0.177,0.944,0.566,0.21,0.934,0.279,0.114,0.974,0.587,0.608,0.985,0.981,0.963,0.977,0.886,0.947,0.055,0.595,0.391,0.88,0.99,0.214,0.58,0.98,0.346,0.983,0.869,0.463,0.395,0.899,0.977,0.779,0.07,0.159,0.785,0.978,0.836,0.86,0.935,0.992,0.99,0.094,0.917,0.986,0.694,0.993,0.989,0.676,0.917,0.942,0.961,0.974,0.846,0.212,0.964,0.947,0.993,0.471,0.155,0.965,0.982,0.997,0.678,0.943,0.171,0.877,0.488,0.955,0.904,0.703,0.87,0.983,0.144,0.8,0.205,0.983,0.705,0.12,0.204,0.221,0.986,0.98,0.958,0.973,0.973,0.4,0.102,0.962,0.943,0.995,0.739,0.777,0.609,0.042,0.952,0.655,0.312,0.107,0.971,0.985,0.986,0.986,0.816,0.626,0.935,0.168,0.439,0.746,0.981,0.923,0.883,0.625,0.043,0.961,0.969,0.878,0.923,0.29,0.95,0.977,0.774,0.798,0.189,0.407,0.266,0.196,0.988,0.995,0.975,0.851,0.36,0.861,0.898,0.492,0.736,0.676,0.979,0.961,0.146,0.23,0.562,0.123,0.816,0.967,0.951,0.866,0.119,0.899,0.94,0.746,0.299,0.333,0.906,0.641,0.855,0.78,0.993,0.997,0.304,0.84,0.942,0.986,0.19,0.995,0.906,0.834,0.984,0.392,0.956,0.228,0.988,0.298,0.996,0.188,0.951,0.942,0.959,0.961,0.901,0.13,0.86,0.814,0.978,0.138,0.918,0.914,0.357,0.988,0.404,0.401,0.958,0.74,0.899,0.978,0.867,0.891,0.984,0.963,0.976,0.992,0.985,0.97,0.971,0.974,0.984,0.513,0.739,0.985,0.38,0.984,0.985,0.264,0.922,0.968,0.574,0.719,0.988,0.462,0.578,0.944,0.992,0.991,0.206,0.658,0.992,0.966,0.877,0.202,0.916,0.497,0.119,0.938,0.993,0.155,0.538,0.298,0.577,0.871,0.974,0.794,0.976,0.492,0.98,0.921,0.993,0.89,0.381,0.799,0.992,0.178,0.291,0.964,0.579,0.887,0.82,0.423,0.97,0.161,0.985,0.766,0.215,0.103,0.817,0.059,0.706,0.993,0.959,0.829,0.954,0.98,0.989,0.492,0.977,0.49,0.645,0.962,0.991,0.966,0.964,0.994,0.99,0.989,0.702,0.824,0.901,0.657,0.858,0.865,0.481,0.996,0.918,0.953,0.928,0.587,0.609,0.826,0.522,0.04,0.882,0.645,0.997,0.783,0.99,0.854,0.872,0.977,0.989,0.823,0.972,0.541,0.698,0.959,0.843,0.403,0.488,0.99,0.901,0.405,0.995,0.87,0.975,0.991,0.719,0.991,0.49,0.421,0.157,0.922,0.704,0.175,0.994,0.52,0.979,0.39,0.948,0.902,0.948,0.98,0.056,0.237,0.646,0.279,0.96,0.969,0.693,0.109,0.997,0.638,0.366,0.278,0.973,0.162,0.294,0.62,0.997,0.974,0.931,0.284,0.142,0.797,0.307,0.953,0.824,0.638,0.306,0.879,0.355,0.258,0.856,0.861,0.232,0.34,0.999,0.984,0.593,0.918,0.997,0.787,0.34,0.985,0.865,0.155,0.968,0.896,0.916,0.486,0.974,0.418,0.764,0.07,0.957,0.995,0.986,0.987,0.358,0.874,0.962,0.058,0.753,0.816,0.983,0.281,0.382,0.987,0.975,0.965,0.896,0.989,0.945,0.144,0.994,0.56,0.147,0.991,0.804,0.957,0.979,0.977,0.988,0.648,0.991,0.61,0.521,0.78,0.947,0.961,0.832,0.257,0.948,0.223,0.869,0.201,0.336,0.94,0.993,0.976,0.983,0.966,0.979,0.524,0.962,0.798,0.721,0.623,0.887,0.328,0.977,0.644,0.956,0.996,0.976,0.988,0.89,0.953,0.427,0.98,0.702,0.896,0.872,0.967,0.991,0.771,0.783,0.96,0.976,0.913,0.77,0.201,0.964,0.771,0.161,0.975,0.695,0.94,0.935,0.395,0.218,0.998,0.379,0.987,0.332,0.865,0.427,0.965,0.046,0.553,0.331,0.551,0.256,0.992,0.962,0.962,0.938,0.55,0.987,0.561,0.756,0.114,0.792,0.976,0.083,0.52,0.187,0.656,0.105,0.134,0.966,0.991,0.903,0.97,0.984,0.347,0.922,0.933,0.951,0.962,0.972,0.254,0.54,0.79,0.497,0.502,0.998,0.978,0.412,0.988,0.983,0.953,0.859,0.414,0.974,0.371,0.96,0.974,0.544,0.91,0.751,0.996,0.365,0.986,0.324,0.568,0.455,0.228,0.71,0.95,0.503,0.594,0.67,0.551,0.278,0.512,0.91,0.218,0.402,0.418,0.97,0.986,0.682,0.993,0.833,0.091,0.392,0.533,0.691,0.383,0.511,0.677,0.277,0.574,0.163,0.969,0.987,0.612,0.541,0.524,0.337,0.969,0.452,0.528,0.194,0.998,0.551,0.51,0.781,0.958,0.288,0.445,0.979,0.53,0.073,0.961,0.088,0.989,0.457,0.859,0.983,0.829,0.997,0.102,0.457,0.61,0.608,0.536,0.927,0.272,0.639,0.146,0.985,0.839,0.72,0.293,0.915,0.88,0.897,0.744,0.988,0.981,0.403,0.409,0.46,0.832,0.969,0.986,0.793,0.896,0.673,0.969,0.893,0.746,0.654,0.981,0.204,0.874,0.487,0.988,0.628,0.808,0.065,0.98,0.718,0.505,0.443,0.223,0.971,0.712,0.919,0.129,0.979,0.319,0.066,0.986,0.451,0.214,0.832,0.154,0.933,0.511,0.85,0.673,0.703,0.491,0.846,0.37,0.792,0.262,0.915,0.878,0.628,0.874,0.895,0.378,0.982,0.921,0.946,0.769,0.982,0.917,0.933,0.813,0.724,0.986,0.747,0.407,0.21,0.215,0.508,0.517,0.442,0.597,0.224,0.294,0.878,0.719,0.977,0.969,0.476,0.974,0.38,0.546,0.125,0.256,0.98,0.856,0.179,0.636,0.945,0.981,0.897,0.762,0.654,0.988,0.921,0.198,0.914,0.601,0.934,0.398,0.588,0.543,0.988,0.722,0.974,0.534,0.493,0.312,0.315,0.994,0.995,0.405,0.908,0.056,0.706,0.846,0.919,0.519,0.339,0.988,0.553,0.991,0.977,0.145,0.826,0.979,0.315,0.663,0.836,0.437,0.97,0.906,0.542,0.547,0.288,0.164,0.222,0.835,0.158,0.63,0.987,0.932,0.984,0.69,0.029,0.985,0.274,0.989,0.244,0.995,0.109,0.719,0.629,0.984,0.274,0.171,0.873,0.705,0.666,0.994,0.548,0.089,0.442,0.913,0.972,0.332,0.937,0.292,0.269,0.693,0.753,0.989,0.444,0.97,0.352,0.275,0.779,0.983,0.296,0.593,0.595,0.445,0.404,0.992,0.586,0.843,0.386,0.859,0.892,0.982,0.994,0.147,0.998,0.631,0.68,0.755,0.556,0.435,0.674,0.759,0.614,0.898,0.993,0.201,0.51,0.616,0.926,0.474,0.122,0.971,0.152,0.179,0.925,0.958,0.454,0.872,0.924,0.413,0.522,0.334,0.434,0.698,0.415,0.709,0.316,0.461,0.221,0.982,0.408,0.544,0.19,0.548,0.58,0.895,0.713,0.565,0.611,0.198,0.384,0.295,0.607,0.231,0.85,0.375,0.246,0.893,0.627,0.461,0.601,0.428,0.478,0.989,0.294,0.698,0.624,0.436,0.428,0.472,0.855,0.732,0.603,0.769,0.19,0.557,0.411,0.762,0.875,0.324,0.976,0.89,0.499,0.454,0.124,0.571,0.506,0.788,0.977,0.801,0.632,0.448,0.365,0.479,0.586,0.903,0.476,0.356,0.564,0.959,0.543,0.086,0.192,0.746,0.948,0.269,0.126,0.639,0.84,0.145,0.281,0.918,0.345,0.987,0.902,0.38,0.404,0.941,0.119,0.614,0.513,0.978,0.747,0.567,0.321,0.908,0.758,0.927,0.982,0.745,0.293,0.964,0.108,0.588,0.949,0.952,0.958,0.772,0.808,0.73,0.976,0.259,0.763,0.575,0.795,0.298,0.943,0.907,0.645,0.411,0.963,0.985,0.64,0.931,0.376,0.968,0.992,0.937,0.64,0.963,0.7,0.896,0.87,0.947,0.697,0.743,0.228,0.936,0.257,0.874,0.482,0.969,0.651,0.872,0.7,0.33,0.986,0.977,0.729,0.963,0.996,0.839,0.459,0.911,0.203,0.924,0.979,0.042,0.94,0.145,0.199,0.864,0.292,0.475,0.975,0.712,0.913,0.913,0.08,0.108,0.183,0.766,0.417,0.338,0.176,0.961,0.129,0.468,0.992,0.304,0.977,0.95,0.974,0.952,0.829,0.398,0.982,0.094,0.743,0.954,0.98,0.978,0.33,0.421,0.951,0.983,0.732,0.781,0.949,0.132,0.878,0.169,0.433,0.916,0.856,0.975,0.142,0.787,0.784,0.3,0.946,0.398,0.901,0.883,0.994,0.929,0.285,0.99,0.28,0.813,0.298,0.324,0.866,0.845,0.974,0.762,0.95,0.917,0.193,0.329,0.335,0.063,0.861,0.267,0.135,0.904,0.835,0.578,0.887,0.993,0.282,0.149,0.945,0.182,0.797,0.269,0.966,0.489,0.974,0.103,0.55,0.343,0.637,0.934,0.473,0.143,0.362,0.4,0.712,0.932,0.813,0.828,0.977,0.441,0.956,0.834,0.222,0.275,0.193,0.991,0.965,0.946,0.936,0.961,0.701,0.958,0.978,0.923,0.291,0.965,0.9,0.843,0.952,0.162,0.212,0.986,0.958,0.32,0.869,0.547,0.97,0.955,0.974,0.901,0.882,0.993,0.925,0.959,0.841,0.836,0.281,0.341,0.77,0.732,0.822,0.134,0.95,0.969,0.108,0.495,0.879,0.64,0.962,0.881,0.938,0.648,0.984,0.497,0.189,0.979,0.877,0.255,0.935,0.051,0.385,0.566,0.521,0.723,0.964,0.934,0.585,0.974,0.981,0.921,0.773,0.823,0.139,0.775,0.92,0.89,0.927,0.885,0.787,0.923,0.759,0.807,0.883,0.721,0.696,0.41,0.131,0.315,0.136,0.657,0.978,0.951,0.04,0.878,0.688,0.088,0.923,0.796,0.483,0.364,0.095,0.252,0.636,0.96,0.836,0.133,0.93,0.702,0.543,0.306,0.626,0.204,0.308,0.879,0.592,0.437,0.941,0.989,0.234,0.449,0.701,0.923,0.274,0.617,0.655,0.75,0.824,0.215,0.334,0.426,0.608,0.957,0.842,0.202,0.28,0.874,0.942,0.898,0.114,0.795,0.636,0.867,0.865,0.955,0.978,0.638,0.862,0.936,0.725,0.887,0.623,0.865,0.687,0.883,0.944,0.343,0.137,0.2,0.4,0.939,0.486,0.134,0.8,0.405,0.792,0.318,0.434,0.87,0.164,0.149,0.65,0.525,0.718,0.525,0.531,0.578,0.323,0.213,0.809,0.638,0.594,0.426,0.756,0.701,0.676,0.692,0.618,0.724,0.735,0.162,0.583,0.667,0.791,0.276,0.611,0.369,0.934,0.552,0.557,0.12,0.614,0.636,0.866,0.598,0.488,0.759,0.599,0.718,0.107,0.816,0.529,0.764,0.489,0.638,0.456,0.446,0.5,0.508,0.665,0.73,0.693,0.469,0.602,0.31,0.167,0.811,0.814,0.576,0.636,0.483,0.793,0.549,0.346,0.258,0.222,0.685,0.334,0.137,0.647,0.826,0.807,0.948,0.872,0.813,0.77,0.688,0.701,0.819,0.807,0.897,0.942,0.757,0.776,0.795,0.841,0.942,0.215,0.764,0.776,0.744,0.648,0.291,0.667,0.665,0.329,0.939,0.869,0.921,0.343,0.834,0.928,0.991,0.922,0.879,0.74,0.658,0.175,0.65,0.33,0.334,0.88,0.453,0.497,0.414,0.792,0.631,0.288,0.779,0.316,0.733,0.853,0.578,0.62,0.598,0.483,0.714,0.521,0.755,0.875,0.673,0.398,0.252,0.777,0.461,0.864,0.406,0.333,0.725,0.48,0.895,0.474,0.282,0.788,0.849,0.381,0.68,0.878,0.433,0.883,0.973,0.855,0.744,0.76,0.953,0.885,0.925,0.148,0.763,0.7,0.594,0.472,0.536,0.264,0.454,0.613,0.324,0.355,0.255,0.119,0.867,0.912,0.415,0.533,0.2,0.398,0.514,0.254,0.201,0.431,0.695,0.153,0.645,0.951,0.649,0.427,0.345,0.501,0.826,0.371,0.911,0.484,0.528,0.724,0.806,0.68,0.722,0.583,0.427,0.588,0.783,0.547,0.297,0.656,0.269,0.698,0.81,0.872,0.88,0.787,0.58,0.8,0.785,0.89,0.857,0.484,0.548,0.5,0.369,0.379,0.732,0.858,0.556,0.475,0.397,0.543,0.386,0.846,0.617,0.486,0.495,0.272,0.374,0.564,0.539,0.749,0.906,0.627,0.636,0.856,0.502,0.509,0.628,0.756,0.672,0.745,0.693,0.352,0.508,0.783,0.507,0.564,0.53,0.46,0.493,0.428,0.434,0.627,0.676,0.432,0.91,0.879,0.33,0.866,0.882,0.892,0.604,0.25,0.555,0.603,0.471,0.754,0.303,0.592,0.433,0.198,0.664,0.609,0.632,0.225,0.506,0.296,0.593,0.563,0.567,0.657,0.951,0.552,0.528,0.838,0.549,0.939,0.426,0.099,0.647,0.694,0.856,0.727,0.358,0.369,0.905,0.751,0.588,0.563,0.547,0.291,0.296,0.556,0.416,0.302,0.725,0.532,0.838,0.226,0.319,0.138,0.669,0.829,0.92,0.281,0.848,0.39,0.935,0.809,0.858,0.894,0.253,0.72,0.249,0.543,0.444,0.669,0.571,0.492,0.628,0.68,0.572,0.539,0.871,0.722,0.627,0.449,0.404,0.508,0.385,0.327,0.888,0.466,0.623,0.807,0.605,0.772,0.925,0.37,0.385,0.145,0.562,0.899,0.635,0.312,0.714,0.304,0.903,0.678,0.519,0.483,0.218,0.361,0.383,0.522,0.667,0.568,0.407,0.535,0.444,0.651,0.801,0.58,0.834,0.858,0.445,0.467,0.5,0.472,0.68,0.283,0.682,0.361,0.543,0.739,0.433,0.503,0.515,0.575,0.326,0.629,0.38,0.614,0.644,0.725,0.622,0.696,0.518,0.185,0.276,0.487,0.717,0.541,0.473,0.509,0.911,0.278,0.814,0.551,0.846,0.35,0.74,0.631,0.693,0.905,0.835,0.578,0.885,0.509,0.515,0.468,0.507,0.532,0.689,0.551,0.456,0.598,0.299,0.236,0.327,0.768,0.803,0.43,0.343,0.39,0.562,0.44,0.388,0.359,0.557,0.554,0.778,0.872,0.639,0.308,0.589,0.576,0.61,0.633,0.7,0.656,0.603,0.777,0.601,0.435,0.687,0.594,0.471,0.503,0.48,0.517,0.842,0.332,0.289,0.257,0.891,0.602,0.641,0.647,0.683,0.36,0.535,0.804,0.416,0.718,0.75,0.87,0.547,0.545,0.532,0.695,0.546,0.573,0.539,0.584,0.452,0.62,0.588,0.183,0.889,0.422,0.483,0.461,0.652,0.615,0.633,0.721,0.372,0.587,0.522,0.447,0.574,0.591,0.77,0.61,0.693,0.879,0.43,0.439,0.81,0.277,0.53,0.349,0.447,0.662,0.698,0.688,0.6,0.903,0.57,0.419,0.578,0.479,0.458,0.465,0.397,0.464,0.462,0.557,0.374,0.301,0.53,0.594,0.498,0.628,0.69,0.416,0.606,0.573,0.504,0.607,0.491,0.399,0.463,0.623,0.537,0.562,0.577,0.413,0.688,0.533,0.915,0.566,0.582,0.582,0.591,0.47,0.343,0.425,0.657,0.164,0.709,0.326,0.5,0.431,0.54'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub22 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub22['pred'] = sub22['pred'].apply(lambda x: round(x,3))\n",
    "sub22_txt = ''\n",
    "for prob in list(sub22['pred'].values):\n",
    "    sub22_txt = sub22_txt+','+str(prob)\n",
    "sub22_txt = sub22_txt[1:]\n",
    "\n",
    "sub22_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6e640",
   "metadata": {},
   "source": [
    "* Submission 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d255177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:59:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:59:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=100,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_train_cc,y_train)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_cc)[:,1],columns=['pred'],index=X_train_cc.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_cc)[:,1],columns=['pred'],index=X_val_cc.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_cc)[:,1],columns=['pred'],index=X_test_cc.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286ff943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.999999890494579\n",
      "Val ROC:  0.838832616340426\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54452f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.998,0.98,1.0,0.997,1.0,0.958,0.55,0.977,0.997,0.141,0.041,0.995,0.999,0.998,0.997,0.506,0.005,0.999,0.997,1.0,0.987,0.999,0.989,0.999,0.999,0.99,1.0,0.981,0.995,1.0,0.98,0.997,0.998,0.999,1.0,0.958,0.966,0.992,0.999,0.995,0.972,0.997,0.998,0.999,0.994,1.0,0.212,0.511,0.049,0.19,1.0,0.996,0.0,0.018,0.006,0.993,0.999,1.0,0.997,1.0,0.935,0.068,0.998,0.999,0.905,0.999,0.994,0.967,1.0,0.998,0.999,0.984,0.994,0.913,0.968,0.987,1.0,1.0,0.977,0.827,0.927,1.0,0.087,0.997,0.998,0.911,0.999,0.998,0.984,1.0,0.998,0.741,1.0,0.662,0.941,0.997,0.982,0.901,0.999,0.995,1.0,0.999,0.998,0.231,0.159,0.769,0.968,0.996,0.563,0.996,0.997,1.0,0.999,0.992,1.0,0.955,0.999,0.999,0.979,0.999,1.0,0.186,0.918,1.0,0.99,0.998,0.999,0.004,0.048,0.999,0.699,0.025,0.995,0.954,0.998,0.888,0.999,0.999,0.999,0.003,0.736,0.998,0.998,0.886,0.999,0.999,0.984,0.012,0.947,1.0,0.997,0.999,1.0,0.32,0.997,0.999,0.94,1.0,0.932,1.0,0.999,1.0,0.79,0.999,0.999,0.71,0.992,0.999,0.91,1.0,0.89,0.979,1.0,1.0,0.993,0.997,0.93,0.067,0.794,0.341,0.213,0.993,0.091,0.03,0.999,0.999,1.0,1.0,0.959,0.998,0.52,0.417,0.99,0.999,0.359,0.999,0.895,0.999,0.711,0.615,0.997,1.0,1.0,0.867,0.979,0.062,0.198,0.064,0.015,0.999,0.999,0.593,1.0,0.998,0.765,1.0,0.998,0.508,0.998,0.992,0.999,0.954,0.966,0.999,0.587,0.001,0.925,0.917,0.039,0.034,0.996,0.971,0.999,0.004,1.0,1.0,0.12,1.0,0.998,0.01,1.0,0.993,1.0,0.999,0.093,0.991,0.005,0.999,0.997,0.967,0.025,0.999,0.999,0.007,0.898,1.0,0.999,0.516,0.001,0.996,0.959,0.342,0.998,0.999,0.998,0.733,1.0,0.996,0.052,0.991,0.996,1.0,0.809,0.999,0.999,0.998,0.96,1.0,0.995,0.998,0.999,0.006,0.999,0.996,0.075,0.998,0.992,0.981,1.0,0.002,0.001,0.162,0.994,0.148,0.002,0.998,0.013,0.893,0.995,0.997,0.999,0.974,0.983,1.0,0.999,0.999,0.997,0.996,0.993,1.0,1.0,0.028,0.998,0.958,0.008,1.0,0.677,0.954,0.854,0.534,0.952,0.423,0.352,0.992,0.995,0.014,0.995,0.99,0.967,0.003,1.0,0.029,1.0,0.999,0.998,0.999,1.0,0.039,0.941,0.899,0.998,0.982,0.997,0.996,0.999,0.97,0.006,0.987,1.0,0.071,0.127,0.999,0.993,0.273,0.999,0.995,1.0,1.0,0.999,0.998,0.993,0.251,0.999,0.998,0.997,0.007,0.997,1.0,0.007,0.996,0.998,0.998,1.0,0.975,0.977,0.997,1.0,0.993,0.979,0.999,0.999,0.007,0.988,0.998,0.998,0.089,0.998,1.0,0.996,0.004,0.997,0.851,1.0,0.983,0.999,1.0,1.0,0.997,0.999,0.994,0.998,1.0,0.993,0.999,0.995,0.022,0.04,0.985,0.014,0.088,0.2,0.999,0.992,0.997,0.017,0.988,0.499,0.876,0.934,0.993,0.998,0.995,1.0,0.384,0.775,0.203,0.073,0.995,0.922,0.992,0.03,0.011,1.0,0.707,0.998,0.999,1.0,0.052,1.0,0.076,0.993,1.0,0.255,0.568,0.983,0.857,1.0,0.001,0.552,1.0,0.985,0.032,1.0,0.992,0.994,0.999,0.999,1.0,0.997,0.999,0.904,1.0,0.994,0.98,0.995,0.01,0.826,0.046,0.997,0.982,0.159,0.066,1.0,0.988,1.0,0.978,0.998,0.074,0.808,0.088,0.861,1.0,0.015,0.008,0.05,0.999,0.999,0.997,0.018,0.004,0.987,0.231,0.991,0.158,1.0,0.999,0.937,0.423,0.48,1.0,0.958,1.0,0.712,0.084,1.0,0.187,1.0,0.998,0.73,0.994,0.973,0.995,0.72,0.031,0.178,0.007,0.009,0.98,0.021,0.695,0.018,0.022,0.823,0.994,1.0,0.994,1.0,1.0,1.0,0.003,1.0,0.626,0.001,0.999,0.998,0.962,0.007,0.984,0.929,0.014,0.989,0.473,0.027,0.984,0.062,0.013,0.999,0.769,0.978,0.999,0.999,0.999,1.0,0.992,0.979,0.008,0.946,0.414,0.999,1.0,0.096,0.513,1.0,0.216,0.999,0.948,0.279,0.079,0.916,0.999,0.997,0.001,0.053,0.984,0.999,0.979,0.989,0.999,1.0,1.0,0.23,0.994,0.998,0.408,1.0,1.0,0.703,1.0,0.946,0.987,0.999,0.984,0.511,0.999,0.999,1.0,0.025,0.034,0.999,1.0,0.996,0.025,0.967,0.129,0.757,0.824,0.999,0.998,0.882,0.972,0.998,0.042,0.967,0.005,0.999,0.99,0.015,0.102,0.229,0.998,0.999,0.897,0.998,0.999,0.959,0.113,1.0,0.997,1.0,0.965,0.996,0.797,0.002,0.999,0.681,0.406,0.023,0.998,0.998,0.999,1.0,0.999,0.416,1.0,0.157,0.057,0.982,0.998,0.945,0.496,0.6,0.006,0.996,0.997,0.997,0.999,0.023,0.997,0.999,0.754,0.961,0.011,0.053,0.042,0.143,0.998,1.0,0.999,0.997,0.078,0.983,0.981,0.016,0.965,0.901,1.0,0.958,0.075,0.03,0.942,0.024,0.912,0.998,0.996,0.994,0.065,0.992,0.986,0.827,0.04,0.323,1.0,0.234,0.995,0.937,1.0,1.0,0.059,0.954,1.0,0.998,0.01,1.0,0.999,0.994,0.999,0.092,0.999,0.042,1.0,0.804,1.0,0.05,0.999,1.0,0.999,1.0,0.985,0.004,0.894,0.986,1.0,0.101,0.999,0.99,0.087,0.992,0.053,0.146,0.998,0.963,0.918,0.997,0.998,0.961,1.0,0.999,0.999,1.0,1.0,0.999,0.985,1.0,1.0,0.102,0.909,1.0,0.232,1.0,0.997,0.019,0.922,0.999,0.456,0.953,1.0,0.412,0.566,0.997,1.0,0.999,0.004,0.933,1.0,0.978,0.977,0.012,0.993,0.551,0.034,1.0,1.0,0.007,0.941,0.1,0.337,0.997,1.0,0.904,1.0,0.417,0.998,0.948,1.0,0.997,0.014,0.947,1.0,0.045,0.432,1.0,0.941,0.978,0.966,0.732,0.998,0.018,1.0,0.416,0.357,0.004,0.99,0.065,0.897,0.999,0.989,0.984,0.999,1.0,1.0,0.316,0.999,0.642,0.782,0.993,1.0,0.999,1.0,1.0,0.994,1.0,0.939,0.987,0.987,0.973,0.96,0.989,0.183,1.0,0.975,0.999,0.973,0.374,0.274,0.963,0.877,0.001,0.983,0.816,1.0,0.976,0.999,0.989,0.973,0.999,1.0,0.975,1.0,0.803,0.928,0.998,0.928,0.458,0.075,1.0,0.996,0.048,1.0,0.991,0.998,1.0,0.111,1.0,0.676,0.133,0.714,0.995,0.698,0.004,1.0,0.75,0.999,0.141,0.997,0.957,0.999,0.998,0.003,0.779,0.842,0.004,0.995,0.999,0.981,0.01,1.0,0.652,0.049,0.155,0.999,0.004,0.013,0.831,1.0,0.998,0.995,0.371,0.103,0.951,0.096,1.0,0.93,0.715,0.311,0.993,0.033,0.011,0.954,0.963,0.059,0.179,1.0,1.0,0.009,0.959,1.0,0.958,0.927,1.0,0.967,0.021,0.993,0.996,0.961,0.74,1.0,0.412,0.993,0.025,0.996,1.0,1.0,0.999,0.013,0.994,0.999,0.004,0.992,0.997,0.997,0.018,0.101,1.0,1.0,0.999,0.958,1.0,1.0,0.016,1.0,0.93,0.047,1.0,0.889,1.0,1.0,1.0,1.0,0.911,1.0,0.928,0.149,0.948,0.976,0.998,0.993,0.572,0.999,0.574,0.991,0.037,0.126,0.997,0.994,0.998,1.0,0.999,0.999,0.93,1.0,0.969,0.872,0.807,0.928,0.125,0.999,0.814,0.971,1.0,1.0,1.0,0.997,0.999,0.522,0.999,0.935,0.995,0.996,0.999,1.0,0.97,0.915,1.0,1.0,0.888,0.708,0.732,0.999,0.836,0.066,1.0,0.957,0.999,0.999,0.299,0.308,1.0,0.304,0.999,0.045,0.994,0.532,0.998,0.001,0.786,0.048,0.255,0.014,1.0,0.999,0.995,0.989,0.651,0.998,0.717,0.941,0.019,0.982,0.994,0.05,0.442,0.12,0.645,0.016,0.005,1.0,1.0,0.967,0.986,0.999,0.091,0.998,0.999,0.999,1.0,0.999,0.036,0.914,0.987,0.746,0.757,1.0,0.998,0.691,0.999,0.988,0.976,0.995,0.322,1.0,0.463,0.964,0.999,0.846,0.988,0.922,1.0,0.258,1.0,0.077,0.118,0.716,0.377,0.904,0.999,0.105,0.539,0.954,0.287,0.125,0.539,0.996,0.018,0.009,0.286,1.0,1.0,0.977,1.0,0.8,0.058,0.167,0.927,0.055,0.322,0.546,0.981,0.443,0.259,0.008,0.998,1.0,0.605,0.735,0.929,0.041,0.999,0.625,0.979,0.276,1.0,0.838,0.422,0.976,1.0,0.039,0.242,1.0,0.935,0.279,0.998,0.109,0.999,0.114,0.946,0.995,0.997,1.0,0.006,0.253,0.848,0.82,0.818,0.998,0.021,0.753,0.489,0.997,0.984,0.355,0.153,0.994,0.987,0.95,0.155,0.999,0.999,0.622,0.046,0.06,0.975,1.0,0.999,0.967,0.968,0.761,0.999,0.99,0.939,0.911,1.0,0.035,0.998,0.323,0.999,0.947,0.962,0.001,0.999,0.959,0.186,0.679,0.5,0.999,0.908,0.988,0.146,1.0,0.374,0.003,1.0,0.781,0.341,0.984,0.366,1.0,0.535,0.993,0.209,0.996,0.96,0.987,0.061,0.96,0.289,0.997,0.995,0.176,0.997,0.992,0.041,0.999,0.999,0.998,0.956,1.0,0.992,0.982,0.997,0.984,1.0,0.954,0.096,0.152,0.197,0.185,0.627,0.271,0.967,0.015,0.156,0.999,0.926,1.0,1.0,0.155,0.999,0.08,0.222,0.003,0.016,1.0,0.995,0.021,0.988,0.999,0.999,0.997,0.903,0.492,0.999,0.999,0.013,0.999,0.109,0.996,0.027,0.97,0.408,1.0,0.959,1.0,0.375,0.941,0.609,0.618,1.0,1.0,0.019,0.996,0.015,0.993,0.996,0.997,0.652,0.416,0.999,0.636,1.0,1.0,0.001,0.987,0.999,0.037,0.806,0.993,0.413,0.915,1.0,0.281,0.74,0.14,0.222,0.004,0.99,0.006,0.915,0.996,0.997,1.0,0.963,0.003,1.0,0.012,1.0,0.033,1.0,0.118,0.856,0.912,0.999,0.294,0.039,0.991,0.928,0.979,1.0,0.784,0.003,0.344,0.997,1.0,0.067,1.0,0.019,0.036,0.755,0.954,0.999,0.06,0.999,0.294,0.266,0.958,0.998,0.114,0.602,0.565,0.444,0.159,0.999,0.574,0.988,0.225,0.996,0.999,1.0,1.0,0.013,1.0,0.553,0.526,0.985,0.973,0.029,0.096,0.786,0.087,0.993,1.0,0.026,0.247,0.713,0.998,0.752,0.018,0.998,0.017,0.133,0.985,0.998,0.189,0.993,0.998,0.105,0.146,0.759,0.056,0.939,0.12,0.967,0.36,0.574,0.728,1.0,0.183,0.221,0.013,0.403,0.826,0.992,0.979,0.922,0.759,0.613,0.573,0.064,0.981,0.026,0.991,0.349,0.056,0.995,0.214,0.72,0.492,0.737,0.956,1.0,0.521,0.995,0.641,0.057,0.031,0.165,0.986,0.997,0.394,0.995,0.212,0.017,0.022,0.947,0.961,0.044,1.0,0.979,0.14,0.095,0.028,0.565,0.102,0.913,0.995,0.564,0.879,0.726,0.203,0.286,0.988,0.993,0.693,0.624,0.827,0.995,0.797,0.031,0.551,0.889,0.999,0.398,0.074,0.649,0.941,0.163,0.082,1.0,0.087,0.999,0.995,0.054,0.013,0.987,0.025,0.778,0.111,1.0,0.923,0.634,0.126,0.994,0.899,0.999,1.0,0.624,0.043,0.998,0.398,0.18,1.0,0.989,0.982,0.987,0.899,0.9,1.0,0.529,0.915,0.648,0.979,0.146,0.999,0.851,0.452,0.024,0.998,1.0,0.93,0.999,0.121,0.999,1.0,0.995,0.777,0.999,0.979,0.999,0.989,0.997,0.349,0.932,0.584,0.999,0.662,0.969,0.859,0.979,0.672,0.986,0.98,0.487,0.998,1.0,0.93,0.993,1.0,0.996,0.246,0.976,0.004,0.998,1.0,0.001,0.995,0.003,0.003,0.769,0.097,0.546,0.999,0.809,0.976,0.998,0.01,0.006,0.014,0.989,0.003,0.167,0.079,0.996,0.033,0.745,1.0,0.266,0.999,0.999,1.0,0.999,0.829,0.078,1.0,0.041,0.567,0.972,1.0,0.999,0.707,0.618,0.985,0.999,0.827,0.988,0.94,0.08,0.991,0.124,0.297,0.988,0.996,0.999,0.043,0.982,0.953,0.308,0.999,0.078,0.994,0.995,1.0,0.847,0.295,0.999,0.165,0.991,0.194,0.434,0.963,0.997,0.999,0.967,0.999,0.988,0.034,0.205,0.216,0.009,0.989,0.064,0.026,0.998,0.933,0.894,0.912,0.999,0.052,0.004,0.989,0.01,0.862,0.054,0.999,0.508,1.0,0.648,0.072,0.335,0.14,0.998,0.147,0.002,0.072,0.473,0.984,0.998,0.967,0.957,0.998,0.462,0.999,0.992,0.076,0.119,0.004,0.998,0.997,0.995,0.996,0.995,0.545,0.999,0.998,0.975,0.031,0.986,0.961,0.994,0.995,0.002,0.804,0.998,0.999,0.034,0.999,0.885,0.998,0.999,0.999,0.843,0.983,1.0,0.987,0.999,0.975,0.998,0.483,0.342,0.883,0.839,0.992,0.156,1.0,1.0,0.11,0.059,0.984,0.364,0.999,0.996,0.989,0.964,1.0,0.466,0.042,0.999,0.797,0.01,0.953,0.016,0.105,0.952,0.213,0.659,0.998,0.998,0.058,0.991,1.0,0.998,0.847,0.972,0.023,0.832,0.979,0.999,0.999,0.996,0.967,0.982,0.635,0.99,0.994,0.837,0.9,0.021,0.121,0.005,0.016,0.947,0.999,0.994,0.017,0.986,0.707,0.003,0.991,0.951,0.745,0.145,0.08,0.136,0.037,0.999,0.997,0.324,0.99,0.399,0.345,0.168,0.708,0.037,0.788,0.99,0.512,0.882,0.996,0.999,0.131,0.17,0.989,0.996,0.015,0.713,0.228,0.98,0.999,0.052,0.174,0.282,0.728,1.0,0.996,0.028,0.029,0.982,0.972,0.997,0.607,0.895,0.765,0.981,0.99,0.999,0.998,0.904,0.999,0.997,0.414,0.991,0.233,0.994,0.801,0.993,0.999,0.031,0.004,0.059,0.016,1.0,0.733,0.032,0.989,0.815,0.968,0.048,0.626,0.982,0.011,0.003,0.944,0.986,0.988,0.986,0.438,0.199,0.009,0.09,0.996,0.669,0.709,0.42,0.911,0.662,0.531,0.887,0.674,0.621,0.919,0.006,0.952,0.489,0.99,0.383,0.45,0.153,1.0,0.434,0.843,0.014,0.901,0.927,0.994,0.592,0.785,0.984,0.905,0.979,0.063,0.985,0.311,0.682,0.427,0.974,0.071,0.496,0.073,0.883,0.716,0.88,0.847,0.461,0.682,0.938,0.002,0.975,0.939,0.894,0.912,0.383,0.687,0.708,0.254,0.219,0.029,0.997,0.201,0.058,0.877,0.991,0.996,0.993,0.996,0.823,0.97,0.916,0.943,0.997,0.968,0.999,0.995,0.96,0.992,0.991,0.745,1.0,0.049,0.95,0.641,0.764,0.975,0.719,0.984,0.946,0.155,1.0,0.996,0.992,0.766,0.99,1.0,1.0,1.0,0.989,0.932,0.882,0.004,0.966,0.076,0.26,0.965,0.24,0.539,0.782,0.784,0.99,0.273,0.991,0.056,0.951,0.944,0.662,0.493,0.487,0.065,0.715,0.057,0.6,0.999,0.91,0.048,0.124,0.938,0.329,0.726,0.434,0.084,0.959,0.304,0.999,0.47,0.057,0.998,0.88,0.077,0.938,0.998,0.054,0.757,1.0,0.992,0.96,0.705,0.964,0.977,0.999,0.009,0.99,0.954,0.365,0.857,0.196,0.01,0.106,0.571,0.689,0.143,0.048,0.04,0.986,0.996,0.037,0.037,0.02,0.02,0.096,0.184,0.059,0.512,0.987,0.095,0.926,1.0,0.874,0.213,0.489,0.152,0.716,0.228,0.996,0.039,0.218,0.728,0.995,0.56,0.83,0.691,0.353,0.435,0.959,0.322,0.67,0.873,0.039,0.942,0.964,0.996,0.878,0.959,0.893,0.889,0.909,0.964,0.999,0.965,0.511,0.265,0.009,0.367,0.464,0.99,0.901,0.127,0.443,0.028,0.601,0.993,0.825,0.381,0.092,0.904,0.162,0.587,0.239,0.852,0.998,0.567,0.942,0.99,0.908,0.921,0.917,0.996,0.72,0.998,0.723,0.045,0.102,0.974,0.803,0.581,0.901,0.859,0.022,0.091,0.942,0.175,0.816,0.647,0.999,0.993,0.121,0.874,0.997,0.998,0.93,0.136,0.619,0.654,0.541,0.945,0.019,0.973,0.992,0.041,0.958,0.888,0.26,0.079,0.24,0.497,0.793,0.944,0.394,0.905,0.985,0.9,0.273,0.988,0.717,0.999,0.513,0.274,0.776,0.845,0.986,0.942,0.158,0.176,0.979,0.963,0.83,0.693,0.06,0.179,0.144,0.452,0.816,0.696,0.814,0.453,0.995,0.397,0.046,0.023,0.447,0.818,0.828,0.153,0.992,0.012,1.0,0.968,0.998,0.997,0.051,0.006,0.037,0.345,0.167,0.752,0.951,0.49,0.734,0.592,0.856,0.645,0.975,0.99,0.805,0.016,0.223,0.396,0.77,0.018,0.997,0.259,0.486,0.953,0.743,0.522,0.987,0.116,0.224,0.02,0.127,0.994,0.993,0.586,0.997,0.072,0.997,0.805,0.163,0.021,0.301,0.17,0.033,0.756,0.859,0.739,0.837,0.042,0.714,0.232,0.989,0.005,0.991,0.951,0.627,0.391,0.133,0.659,0.963,0.009,0.9,0.116,0.677,0.842,0.745,0.902,0.783,0.919,0.185,0.823,0.665,0.573,0.063,0.872,0.677,0.725,0.304,0.148,0.184,0.016,0.937,0.873,0.898,0.614,0.994,0.035,0.988,0.645,0.997,0.241,0.951,0.005,0.613,0.996,0.991,0.004,0.991,0.872,0.702,0.697,0.647,0.934,0.942,0.347,0.034,0.115,0.474,0.02,0.526,0.934,0.974,0.274,0.191,0.025,0.004,0.235,0.309,0.252,0.368,0.093,0.787,0.858,0.897,0.2,0.045,0.943,0.951,0.993,0.779,0.681,0.005,0.923,0.591,0.223,0.517,0.463,0.566,0.683,0.532,0.096,0.993,0.054,0.036,0.854,0.998,0.313,0.986,0.994,0.785,0.327,0.396,0.552,0.683,0.991,0.939,0.92,0.753,0.355,0.782,0.89,0.47,0.933,0.362,0.385,0.653,0.445,0.348,0.019,0.989,0.118,0.081,0.124,0.19,0.674,0.908,0.017,0.284,0.315,0.804,0.789,0.777,0.834,0.667,0.417,0.967,0.999,0.134,0.084,0.996,0.006,0.013,0.496,0.021,0.814,0.803,0.618,0.774,0.981,0.115,0.54,0.569,0.941,0.828,0.077,0.385,0.443,0.088,0.959,0.46,0.015,0.398,0.98,0.6,0.725,0.983,0.078,0.851,0.823,0.689,0.919,0.844,0.779,0.005,0.887,0.825,0.779,0.925,0.865,0.897,0.665,0.997,0.001,0.009,0.003,0.002,0.84,0.503,0.904,0.978,0.015,0.108,0.888,0.748,0.903,0.1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub23 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub23['pred'] = sub23['pred'].apply(lambda x: round(x,3))\n",
    "sub23_txt = ''\n",
    "for prob in list(sub23['pred'].values):\n",
    "    sub23_txt = sub23_txt+','+str(prob)\n",
    "sub23_txt = sub23_txt[1:]\n",
    "\n",
    "sub23_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9588904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame(rf.feature_importances_.T,columns=['importance'],index=X_train_cc.columns).sort_values(by='importance',ascending=False)\n",
    "var_list = list(imp_df[imp_df['importance']>0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920d9a1",
   "metadata": {},
   "source": [
    "* LR trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59bd7da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8547291730214328\n",
      "0.5073294556113022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_basketand_order_max</th>\n",
       "      <td>1.529302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_basketand_order_mean</th>\n",
       "      <td>1.357291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_LC Waikiki</th>\n",
       "      <td>1.123619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_Ev Tekstil</th>\n",
       "      <td>1.065566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_hour_sellingprice_mean_4</th>\n",
       "      <td>1.009217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_KadÄ±n</th>\n",
       "      <td>-1.035228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_Ã‡orap</th>\n",
       "      <td>-1.061797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_Apple</th>\n",
       "      <td>-1.109767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_min_KadÄ±n</th>\n",
       "      <td>-1.256233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_KadÄ±n</th>\n",
       "      <td>-1.876540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 coefficient\n",
       "Z_time_btw_basketand_order_max      1.529302\n",
       "Z_time_btw_basketand_order_mean     1.357291\n",
       "Z_sellingprice_count_LC Waikiki     1.123619\n",
       "Z_sellingprice_count_Ev Tekstil     1.065566\n",
       "Z_hour_sellingprice_mean_4          1.009217\n",
       "...                                      ...\n",
       "Z_sellingprice_median_KadÄ±n        -1.035228\n",
       "Z_sellingprice_count_Ã‡orap         -1.061797\n",
       "Z_sellingprice_sum_Apple           -1.109767\n",
       "Z_sellingprice_min_KadÄ±n           -1.256233\n",
       "Z_sellingprice_sum_KadÄ±n           -1.876540\n",
       "\n",
       "[752 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1',C=1000,solver='saga')\n",
    "lr.fit(X_train_cc[var_list],y_train)\n",
    "pred_train = pd.DataFrame(lr.predict_proba(X_train_cc[var_list])[:,1],columns=['pred'],index=X_train_cc.index)\n",
    "pred_val = pd.DataFrame(lr.predict_proba(X_val_cc[var_list])[:,1],columns=['pred'],index=X_val_cc.index)\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "pred_train['pred_binary'] = np.where(pred_train['pred']>0.5,1,0)\n",
    "pred_val['pred_binary'] = np.where(pred_val['pred']>0.5,1,0)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])\n",
    "\n",
    "err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "print(roc_val)\n",
    "print(err_rate_val)\n",
    "pd.DataFrame(lr.coef_.T,columns=['coefficient'],index=X_train_cc[var_list].columns).sort_values(by='coefficient',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.DataFrame(lr.predict_proba(X_test_cc[var_list])[:,1],columns=['pred'],index=X_test_cc.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e1d88",
   "metadata": {},
   "source": [
    "* Submission 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "132cb7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.993,0.971,0.986,0.989,0.989,0.997,0.688,0.581,0.928,0.978,0.353,0.082,0.958,0.993,0.99,0.994,0.276,0.071,0.991,0.925,0.994,0.979,0.982,0.979,0.993,0.999,0.907,0.994,0.949,0.975,0.998,0.938,0.975,0.973,0.972,0.995,0.816,0.812,0.962,0.98,0.977,0.751,0.987,0.979,0.991,0.971,0.988,0.591,0.467,0.139,0.317,0.996,0.919,0.025,0.185,0.061,0.964,0.975,0.992,0.988,0.997,0.817,0.082,0.987,0.997,0.524,0.981,0.96,0.925,0.993,0.988,0.987,0.964,0.987,0.849,0.672,0.988,0.997,0.981,0.922,0.428,0.828,0.998,0.131,0.99,0.974,0.666,0.997,0.984,0.978,0.996,0.985,0.844,0.984,0.41,0.823,0.891,0.9,0.507,0.984,0.966,0.995,0.997,0.947,0.526,0.601,0.691,0.73,0.939,0.605,0.971,0.992,0.989,0.994,0.94,0.99,0.975,0.979,0.984,0.836,0.996,0.999,0.553,0.959,0.997,0.929,0.974,0.992,0.107,0.041,0.996,0.721,0.055,0.987,0.97,0.995,0.631,0.993,0.815,0.999,0.184,0.785,0.967,0.982,0.544,0.935,0.986,0.968,0.273,0.94,0.998,0.986,0.979,0.995,0.626,0.905,0.988,0.939,0.986,0.919,0.979,0.984,0.942,0.698,0.997,0.972,0.731,0.965,0.977,0.323,0.995,0.585,0.753,0.995,0.995,0.971,0.979,0.869,0.446,0.897,0.225,0.192,0.895,0.401,0.137,0.992,0.99,0.994,0.997,0.909,0.99,0.315,0.344,0.936,0.936,0.287,0.98,0.444,0.993,0.737,0.891,0.985,0.996,0.994,0.942,0.686,0.232,0.272,0.159,0.162,0.98,0.942,0.516,0.998,0.964,0.485,0.987,0.933,0.452,0.993,0.893,0.96,0.835,0.811,0.959,0.562,0.033,0.88,0.888,0.1,0.021,0.944,0.911,0.99,0.221,0.985,0.993,0.85,0.989,0.989,0.058,0.988,0.935,0.982,0.998,0.278,0.936,0.163,0.975,0.98,0.871,0.103,0.957,0.99,0.042,0.837,0.996,0.991,0.486,0.104,0.988,0.921,0.414,0.972,0.993,0.99,0.817,0.996,0.973,0.099,0.921,0.967,0.993,0.228,0.979,0.922,0.991,0.941,0.983,0.967,0.994,0.996,0.135,0.97,0.906,0.117,0.933,0.953,0.703,0.979,0.19,0.08,0.778,0.901,0.402,0.133,0.991,0.189,0.728,0.964,0.979,0.977,0.961,0.977,0.999,0.983,0.979,0.974,0.968,0.969,0.998,0.986,0.208,0.965,0.849,0.283,0.998,0.404,0.551,0.487,0.805,0.911,0.258,0.493,0.988,0.9,0.25,0.905,0.874,0.684,0.136,0.985,0.085,0.995,0.956,0.89,0.929,0.994,0.038,0.716,0.923,0.967,0.976,0.98,0.834,0.978,0.816,0.056,0.712,0.997,0.291,0.356,0.985,0.948,0.515,0.992,0.92,0.995,0.994,0.995,0.997,0.964,0.435,0.967,0.953,0.907,0.106,0.988,0.988,0.037,0.987,0.899,0.982,0.995,0.923,0.923,0.988,0.998,0.947,0.901,0.962,0.964,0.109,0.911,0.984,0.893,0.131,0.994,0.997,0.933,0.033,0.971,0.669,0.999,0.898,0.992,0.996,0.993,0.998,0.975,0.897,0.985,0.991,0.745,0.973,0.825,0.296,0.136,0.93,0.098,0.129,0.35,0.964,0.924,0.941,0.104,0.937,0.346,0.564,0.743,0.991,0.97,0.967,0.982,0.378,0.551,0.536,0.234,0.986,0.801,0.908,0.17,0.131,0.99,0.478,0.986,0.984,0.899,0.509,0.997,0.315,0.819,0.994,0.438,0.943,0.803,0.785,0.986,0.015,0.644,0.97,0.769,0.242,0.993,0.921,0.951,0.995,0.997,0.992,0.991,0.996,0.698,0.991,0.916,0.928,0.986,0.134,0.786,0.069,0.994,0.776,0.336,0.094,0.987,0.859,0.965,0.956,0.979,0.434,0.882,0.297,0.81,0.994,0.164,0.158,0.422,0.984,0.969,0.881,0.084,0.091,0.853,0.36,0.874,0.7,0.995,0.975,0.71,0.363,0.061,0.998,0.892,0.997,0.679,0.281,0.996,0.369,0.99,0.974,0.659,0.911,0.866,0.847,0.577,0.202,0.358,0.03,0.614,0.863,0.084,0.229,0.174,0.202,0.546,0.98,0.989,0.958,0.995,0.996,0.989,0.048,0.994,0.485,0.051,0.995,0.853,0.67,0.081,0.869,0.869,0.177,0.958,0.54,0.201,0.945,0.204,0.067,0.975,0.547,0.625,0.988,0.977,0.949,0.988,0.929,0.96,0.02,0.609,0.343,0.925,0.996,0.193,0.588,0.99,0.276,0.991,0.841,0.526,0.314,0.895,0.984,0.846,0.027,0.11,0.858,0.988,0.817,0.933,0.95,0.996,0.993,0.097,0.95,0.992,0.653,0.997,0.994,0.677,0.932,0.945,0.98,0.986,0.884,0.228,0.97,0.943,0.997,0.452,0.094,0.961,0.993,0.999,0.617,0.956,0.155,0.917,0.451,0.981,0.907,0.651,0.906,0.989,0.102,0.855,0.179,0.991,0.737,0.093,0.166,0.253,0.993,0.99,0.971,0.966,0.975,0.341,0.071,0.97,0.944,0.998,0.719,0.805,0.591,0.025,0.975,0.589,0.298,0.051,0.98,0.994,0.994,0.989,0.923,0.57,0.933,0.121,0.488,0.771,0.988,0.959,0.902,0.651,0.034,0.974,0.983,0.923,0.941,0.152,0.965,0.983,0.822,0.836,0.136,0.36,0.251,0.135,0.995,0.998,0.976,0.831,0.356,0.903,0.944,0.475,0.685,0.715,0.986,0.951,0.145,0.19,0.511,0.112,0.84,0.971,0.942,0.84,0.045,0.938,0.967,0.737,0.294,0.279,0.947,0.511,0.891,0.838,0.999,0.998,0.249,0.737,0.933,0.99,0.223,0.998,0.925,0.905,0.993,0.499,0.953,0.203,0.994,0.285,0.998,0.154,0.958,0.965,0.969,0.968,0.933,0.106,0.92,0.808,0.981,0.12,0.926,0.943,0.332,0.992,0.311,0.399,0.977,0.82,0.928,0.986,0.898,0.922,0.991,0.961,0.993,0.994,0.994,0.972,0.978,0.986,0.988,0.455,0.746,0.988,0.47,0.99,0.989,0.15,0.903,0.967,0.592,0.726,0.989,0.552,0.557,0.927,0.996,0.995,0.164,0.654,0.996,0.981,0.883,0.192,0.884,0.452,0.043,0.97,0.997,0.142,0.565,0.282,0.622,0.898,0.991,0.825,0.982,0.5,0.991,0.936,0.996,0.935,0.322,0.832,0.994,0.134,0.217,0.955,0.555,0.873,0.913,0.439,0.986,0.072,0.99,0.796,0.143,0.063,0.852,0.039,0.596,0.996,0.983,0.842,0.974,0.981,0.994,0.496,0.992,0.559,0.553,0.977,0.993,0.971,0.975,0.998,0.995,0.995,0.78,0.778,0.901,0.697,0.832,0.912,0.475,0.998,0.924,0.958,0.933,0.653,0.697,0.803,0.585,0.012,0.886,0.727,0.998,0.823,0.993,0.859,0.89,0.985,0.993,0.825,0.987,0.71,0.744,0.968,0.846,0.414,0.438,0.993,0.874,0.399,0.999,0.841,0.974,0.995,0.737,0.994,0.503,0.405,0.112,0.949,0.745,0.171,0.997,0.474,0.992,0.326,0.971,0.857,0.977,0.992,0.031,0.224,0.681,0.254,0.951,0.986,0.714,0.055,0.998,0.696,0.332,0.275,0.985,0.106,0.202,0.652,0.999,0.97,0.97,0.238,0.13,0.849,0.256,0.967,0.859,0.645,0.274,0.896,0.284,0.13,0.856,0.869,0.143,0.233,0.999,0.989,0.588,0.937,0.998,0.748,0.365,0.991,0.892,0.134,0.983,0.914,0.948,0.556,0.964,0.393,0.789,0.067,0.98,0.998,0.978,0.992,0.465,0.901,0.977,0.028,0.826,0.888,0.979,0.23,0.354,0.992,0.982,0.983,0.931,0.994,0.967,0.085,0.998,0.591,0.102,0.995,0.765,0.966,0.985,0.988,0.996,0.698,0.995,0.606,0.533,0.842,0.962,0.984,0.906,0.234,0.96,0.143,0.938,0.102,0.299,0.945,0.997,0.972,0.988,0.972,0.987,0.492,0.966,0.851,0.651,0.619,0.878,0.337,0.98,0.665,0.966,0.999,0.984,0.993,0.905,0.967,0.322,0.992,0.792,0.914,0.926,0.982,0.997,0.793,0.842,0.966,0.983,0.909,0.758,0.129,0.976,0.749,0.074,0.981,0.703,0.948,0.961,0.532,0.21,0.999,0.372,0.993,0.193,0.891,0.396,0.98,0.03,0.56,0.295,0.501,0.222,0.992,0.955,0.972,0.95,0.557,0.995,0.524,0.783,0.11,0.799,0.989,0.039,0.492,0.125,0.702,0.062,0.106,0.974,0.997,0.949,0.978,0.993,0.334,0.934,0.934,0.952,0.97,0.974,0.21,0.492,0.882,0.456,0.515,0.999,0.99,0.299,0.992,0.991,0.968,0.877,0.299,0.982,0.435,0.983,0.971,0.584,0.951,0.825,0.998,0.3,0.994,0.316,0.554,0.575,0.186,0.772,0.978,0.491,0.603,0.727,0.413,0.274,0.469,0.95,0.205,0.33,0.328,0.981,0.993,0.648,0.997,0.777,0.068,0.4,0.528,0.631,0.421,0.518,0.787,0.23,0.637,0.094,0.983,0.993,0.601,0.436,0.464,0.277,0.989,0.452,0.488,0.207,0.999,0.485,0.457,0.788,0.954,0.33,0.419,0.987,0.57,0.047,0.973,0.082,0.993,0.388,0.886,0.991,0.867,0.998,0.062,0.453,0.626,0.653,0.479,0.964,0.212,0.726,0.087,0.995,0.849,0.761,0.32,0.936,0.867,0.922,0.724,0.993,0.991,0.46,0.276,0.587,0.896,0.963,0.994,0.754,0.915,0.676,0.971,0.925,0.757,0.657,0.987,0.172,0.902,0.442,0.991,0.673,0.827,0.029,0.987,0.815,0.573,0.458,0.249,0.979,0.712,0.857,0.098,0.986,0.317,0.044,0.993,0.542,0.221,0.923,0.113,0.96,0.569,0.849,0.654,0.72,0.501,0.824,0.356,0.808,0.209,0.933,0.91,0.633,0.805,0.937,0.259,0.99,0.953,0.949,0.793,0.988,0.928,0.967,0.871,0.704,0.994,0.805,0.424,0.138,0.176,0.531,0.449,0.386,0.571,0.13,0.311,0.885,0.779,0.987,0.966,0.455,0.987,0.283,0.58,0.103,0.259,0.983,0.926,0.165,0.703,0.983,0.988,0.933,0.798,0.627,0.997,0.94,0.216,0.936,0.572,0.946,0.364,0.455,0.534,0.996,0.644,0.988,0.504,0.469,0.26,0.3,0.997,0.996,0.44,0.897,0.025,0.717,0.879,0.915,0.499,0.257,0.996,0.553,0.994,0.98,0.102,0.805,0.993,0.297,0.686,0.809,0.354,0.964,0.923,0.547,0.602,0.278,0.168,0.225,0.839,0.117,0.61,0.992,0.954,0.991,0.781,0.019,0.995,0.193,0.995,0.321,0.998,0.087,0.754,0.687,0.995,0.256,0.22,0.848,0.701,0.696,0.997,0.625,0.061,0.411,0.925,0.99,0.273,0.956,0.199,0.154,0.701,0.766,0.991,0.36,0.982,0.248,0.246,0.81,0.99,0.244,0.699,0.617,0.477,0.446,0.992,0.703,0.89,0.351,0.901,0.898,0.985,0.997,0.112,0.999,0.725,0.629,0.771,0.659,0.425,0.735,0.752,0.585,0.949,0.992,0.114,0.502,0.634,0.927,0.545,0.102,0.983,0.138,0.146,0.937,0.976,0.436,0.904,0.923,0.362,0.573,0.344,0.432,0.687,0.442,0.671,0.328,0.447,0.183,0.991,0.402,0.464,0.13,0.546,0.544,0.924,0.777,0.62,0.642,0.193,0.28,0.283,0.532,0.157,0.876,0.347,0.275,0.935,0.747,0.437,0.624,0.457,0.513,0.997,0.304,0.711,0.679,0.326,0.378,0.349,0.819,0.766,0.574,0.746,0.269,0.481,0.373,0.746,0.871,0.273,0.978,0.934,0.477,0.489,0.088,0.532,0.577,0.873,0.984,0.831,0.727,0.399,0.385,0.393,0.692,0.923,0.499,0.362,0.592,0.979,0.416,0.071,0.204,0.773,0.958,0.26,0.123,0.574,0.916,0.095,0.221,0.961,0.312,0.991,0.937,0.358,0.343,0.946,0.086,0.556,0.391,0.987,0.703,0.559,0.259,0.939,0.768,0.93,0.992,0.744,0.385,0.964,0.077,0.665,0.949,0.969,0.97,0.776,0.838,0.69,0.987,0.148,0.724,0.557,0.791,0.196,0.976,0.905,0.673,0.238,0.984,0.988,0.65,0.958,0.426,0.984,0.996,0.947,0.571,0.973,0.816,0.928,0.888,0.958,0.715,0.769,0.204,0.95,0.232,0.883,0.492,0.968,0.666,0.858,0.553,0.327,0.994,0.988,0.66,0.973,0.998,0.846,0.392,0.934,0.152,0.931,0.988,0.014,0.923,0.124,0.174,0.923,0.306,0.454,0.988,0.683,0.949,0.905,0.056,0.083,0.146,0.816,0.427,0.318,0.196,0.963,0.111,0.641,0.995,0.252,0.996,0.96,0.982,0.952,0.888,0.458,0.993,0.055,0.673,0.968,0.983,0.991,0.395,0.41,0.981,0.993,0.758,0.934,0.949,0.131,0.862,0.162,0.522,0.841,0.883,0.98,0.095,0.721,0.819,0.419,0.955,0.46,0.909,0.901,0.996,0.926,0.307,0.995,0.34,0.844,0.205,0.262,0.91,0.926,0.986,0.837,0.968,0.927,0.125,0.303,0.272,0.04,0.874,0.203,0.118,0.892,0.811,0.628,0.909,0.997,0.273,0.137,0.961,0.132,0.826,0.281,0.972,0.427,0.983,0.076,0.501,0.405,0.565,0.96,0.545,0.118,0.266,0.354,0.716,0.941,0.838,0.854,0.983,0.495,0.985,0.882,0.144,0.311,0.122,0.994,0.962,0.963,0.96,0.967,0.743,0.965,0.981,0.922,0.171,0.968,0.954,0.915,0.966,0.148,0.219,0.994,0.971,0.274,0.893,0.569,0.973,0.979,0.983,0.903,0.864,0.994,0.914,0.954,0.857,0.818,0.363,0.397,0.82,0.799,0.913,0.139,0.937,0.979,0.122,0.439,0.86,0.543,0.961,0.872,0.947,0.657,0.994,0.5,0.204,0.986,0.913,0.259,0.939,0.029,0.466,0.665,0.549,0.732,0.976,0.92,0.664,0.976,0.988,0.942,0.816,0.911,0.213,0.746,0.944,0.912,0.941,0.9,0.789,0.947,0.666,0.817,0.92,0.765,0.809,0.401,0.126,0.274,0.09,0.54,0.983,0.953,0.043,0.876,0.697,0.077,0.934,0.769,0.573,0.309,0.103,0.339,0.618,0.976,0.894,0.136,0.962,0.69,0.583,0.364,0.588,0.207,0.334,0.876,0.583,0.444,0.949,0.994,0.214,0.496,0.626,0.925,0.195,0.696,0.642,0.795,0.903,0.201,0.411,0.395,0.715,0.969,0.9,0.188,0.216,0.872,0.948,0.934,0.107,0.792,0.793,0.882,0.898,0.954,0.987,0.639,0.879,0.947,0.735,0.91,0.554,0.848,0.718,0.928,0.961,0.293,0.121,0.202,0.387,0.972,0.503,0.082,0.858,0.447,0.752,0.352,0.418,0.864,0.186,0.135,0.678,0.578,0.732,0.578,0.531,0.561,0.335,0.163,0.855,0.639,0.578,0.474,0.714,0.718,0.66,0.79,0.587,0.741,0.726,0.157,0.606,0.66,0.889,0.237,0.59,0.398,0.936,0.592,0.535,0.082,0.628,0.578,0.871,0.674,0.505,0.811,0.622,0.735,0.092,0.835,0.575,0.83,0.405,0.638,0.434,0.414,0.373,0.511,0.699,0.824,0.745,0.5,0.622,0.345,0.139,0.872,0.845,0.616,0.678,0.484,0.837,0.522,0.277,0.227,0.186,0.726,0.319,0.082,0.679,0.857,0.837,0.973,0.885,0.854,0.831,0.691,0.685,0.857,0.843,0.929,0.958,0.817,0.817,0.802,0.819,0.963,0.232,0.766,0.722,0.82,0.609,0.349,0.699,0.739,0.29,0.962,0.896,0.967,0.295,0.818,0.938,0.997,0.941,0.895,0.757,0.651,0.149,0.746,0.277,0.331,0.919,0.413,0.485,0.341,0.863,0.651,0.336,0.832,0.349,0.814,0.887,0.556,0.585,0.616,0.479,0.799,0.51,0.777,0.899,0.722,0.385,0.304,0.81,0.409,0.89,0.302,0.323,0.77,0.571,0.918,0.463,0.249,0.782,0.872,0.503,0.654,0.922,0.317,0.882,0.98,0.859,0.658,0.704,0.953,0.907,0.949,0.122,0.76,0.76,0.534,0.481,0.532,0.194,0.436,0.54,0.344,0.418,0.213,0.088,0.939,0.943,0.391,0.614,0.145,0.316,0.489,0.241,0.154,0.459,0.748,0.132,0.662,0.959,0.613,0.363,0.258,0.397,0.872,0.328,0.926,0.446,0.522,0.735,0.854,0.684,0.769,0.602,0.407,0.575,0.778,0.572,0.346,0.636,0.152,0.766,0.841,0.887,0.834,0.822,0.609,0.846,0.803,0.911,0.906,0.409,0.485,0.514,0.398,0.346,0.787,0.91,0.571,0.462,0.46,0.452,0.328,0.876,0.66,0.515,0.56,0.247,0.406,0.576,0.523,0.732,0.944,0.6,0.665,0.918,0.497,0.565,0.687,0.783,0.746,0.818,0.7,0.31,0.387,0.836,0.459,0.588,0.53,0.393,0.429,0.4,0.478,0.698,0.694,0.409,0.937,0.904,0.371,0.894,0.896,0.909,0.641,0.176,0.496,0.618,0.49,0.724,0.246,0.523,0.339,0.145,0.647,0.602,0.532,0.183,0.455,0.231,0.583,0.568,0.645,0.63,0.972,0.581,0.517,0.876,0.525,0.936,0.46,0.097,0.637,0.629,0.852,0.759,0.347,0.383,0.951,0.781,0.576,0.551,0.542,0.298,0.317,0.664,0.38,0.258,0.75,0.541,0.875,0.215,0.28,0.092,0.755,0.806,0.939,0.265,0.849,0.367,0.934,0.894,0.822,0.912,0.219,0.702,0.198,0.583,0.422,0.642,0.526,0.468,0.615,0.698,0.596,0.512,0.927,0.807,0.645,0.5,0.481,0.426,0.305,0.298,0.879,0.523,0.628,0.823,0.64,0.82,0.931,0.417,0.37,0.134,0.539,0.945,0.679,0.27,0.698,0.327,0.934,0.647,0.515,0.451,0.195,0.331,0.314,0.498,0.673,0.609,0.324,0.531,0.453,0.587,0.835,0.578,0.851,0.866,0.489,0.483,0.508,0.444,0.681,0.275,0.674,0.352,0.61,0.674,0.316,0.559,0.541,0.613,0.31,0.67,0.385,0.647,0.677,0.691,0.643,0.688,0.57,0.179,0.226,0.456,0.763,0.586,0.512,0.515,0.946,0.208,0.877,0.537,0.861,0.411,0.724,0.56,0.719,0.896,0.835,0.455,0.909,0.53,0.537,0.443,0.453,0.431,0.721,0.547,0.424,0.583,0.234,0.224,0.35,0.761,0.834,0.379,0.253,0.341,0.469,0.537,0.366,0.338,0.504,0.501,0.808,0.884,0.576,0.319,0.569,0.551,0.589,0.696,0.745,0.681,0.58,0.832,0.576,0.374,0.72,0.615,0.43,0.5,0.484,0.443,0.878,0.305,0.226,0.227,0.924,0.595,0.601,0.715,0.679,0.296,0.518,0.798,0.394,0.72,0.749,0.804,0.527,0.557,0.565,0.709,0.452,0.559,0.531,0.689,0.446,0.709,0.687,0.213,0.913,0.401,0.454,0.489,0.628,0.636,0.651,0.653,0.336,0.607,0.569,0.464,0.584,0.55,0.829,0.592,0.825,0.897,0.379,0.505,0.813,0.171,0.545,0.304,0.43,0.615,0.667,0.745,0.588,0.909,0.58,0.366,0.537,0.505,0.443,0.422,0.432,0.527,0.439,0.535,0.355,0.216,0.506,0.654,0.504,0.645,0.727,0.399,0.542,0.511,0.481,0.578,0.508,0.457,0.384,0.588,0.595,0.515,0.621,0.363,0.716,0.509,0.943,0.457,0.554,0.557,0.549,0.445,0.363,0.38,0.688,0.093,0.726,0.392,0.464,0.429,0.472'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub24 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub24['pred'] = sub24['pred'].apply(lambda x: round(x,3))\n",
    "sub24_txt = ''\n",
    "for prob in list(sub24['pred'].values):\n",
    "    sub24_txt = sub24_txt+','+str(prob)\n",
    "sub24_txt = sub24_txt[1:]\n",
    "\n",
    "sub24_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fce1e",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "abed8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_roc = cc.get_coarse_class_table()[['variable','roc']].drop_duplicates().sort_values(by = 'roc',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3190991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_roc['variable'] = uni_roc['variable'].apply(lambda x: 'Z_'+x)\n",
    "uni_roc.set_index('variable',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c873a9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_mean_Oysho</th>\n",
       "      <td>0.827269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_Oysho</th>\n",
       "      <td>0.826220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_KadÄ±n</th>\n",
       "      <td>0.788881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_Pantolon</th>\n",
       "      <td>0.788565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_529.0</th>\n",
       "      <td>0.788565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_daily_order_secs_btw_consecutives_min</th>\n",
       "      <td>0.529611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_hourly_order_secs_btw_first_last</th>\n",
       "      <td>0.527055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_hourly_order_secs_btw_consecutives_min</th>\n",
       "      <td>0.524916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_max_Terapi Men</th>\n",
       "      <td>0.519928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_Terapi Men</th>\n",
       "      <td>0.518116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               roc\n",
       "variable                                          \n",
       "Z_sellingprice_mean_Oysho                 0.827269\n",
       "Z_sellingprice_median_Oysho               0.826220\n",
       "Z_sellingprice_count_KadÄ±n                0.788881\n",
       "Z_sellingprice_median_Pantolon            0.788565\n",
       "Z_sellingprice_median_529.0               0.788565\n",
       "...                                            ...\n",
       "Z_daily_order_secs_btw_consecutives_min   0.529611\n",
       "Z_hourly_order_secs_btw_first_last        0.527055\n",
       "Z_hourly_order_secs_btw_consecutives_min  0.524916\n",
       "Z_sellingprice_max_Terapi Men             0.519928\n",
       "Z_sellingprice_count_Terapi Men           0.518116\n",
       "\n",
       "[1539 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fcda5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = X_train_cc.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f410d39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_sellingprice_count_basket</th>\n",
       "      <th>Z_sellingprice_count_favorite</th>\n",
       "      <th>Z_sellingprice_count_order</th>\n",
       "      <th>Z_sellingprice_count_search</th>\n",
       "      <th>Z_sellingprice_count_visit</th>\n",
       "      <th>Z_sellingprice_sum_basket</th>\n",
       "      <th>Z_sellingprice_sum_favorite</th>\n",
       "      <th>Z_sellingprice_sum_order</th>\n",
       "      <th>Z_sellingprice_sum_search</th>\n",
       "      <th>Z_sellingprice_sum_visit</th>\n",
       "      <th>...</th>\n",
       "      <th>Z_favorite_cnt_before_order_12hour</th>\n",
       "      <th>Z_favorite_cnt_before_order_24hour</th>\n",
       "      <th>Z_time_btw_basketand_order_mean</th>\n",
       "      <th>Z_time_btw_basketand_order_max</th>\n",
       "      <th>Z_time_btw_basketand_order_min</th>\n",
       "      <th>Z_time_btw_two_orders_mean</th>\n",
       "      <th>Z_time_btw_two_orders_max</th>\n",
       "      <th>Z_time_btw_two_orders_min</th>\n",
       "      <th>Z_transaction_count</th>\n",
       "      <th>Z_experience_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_basket</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.270760</td>\n",
       "      <td>0.489683</td>\n",
       "      <td>0.415717</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.270760</td>\n",
       "      <td>0.489683</td>\n",
       "      <td>0.415717</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234207</td>\n",
       "      <td>-0.249585</td>\n",
       "      <td>0.573644</td>\n",
       "      <td>0.573644</td>\n",
       "      <td>0.573644</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_favorite</th>\n",
       "      <td>-0.270760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.238396</td>\n",
       "      <td>-0.214980</td>\n",
       "      <td>-0.099177</td>\n",
       "      <td>-0.270760</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.238396</td>\n",
       "      <td>-0.214980</td>\n",
       "      <td>-0.099177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376103</td>\n",
       "      <td>0.399822</td>\n",
       "      <td>-0.283978</td>\n",
       "      <td>-0.283978</td>\n",
       "      <td>-0.283978</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_order</th>\n",
       "      <td>0.489683</td>\n",
       "      <td>-0.238396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194701</td>\n",
       "      <td>0.105027</td>\n",
       "      <td>0.489683</td>\n",
       "      <td>0.238396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194701</td>\n",
       "      <td>0.105027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.388591</td>\n",
       "      <td>-0.413097</td>\n",
       "      <td>0.928729</td>\n",
       "      <td>0.928729</td>\n",
       "      <td>0.928729</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_search</th>\n",
       "      <td>0.415717</td>\n",
       "      <td>-0.214980</td>\n",
       "      <td>0.194701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364051</td>\n",
       "      <td>0.415717</td>\n",
       "      <td>0.214980</td>\n",
       "      <td>0.194701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133130</td>\n",
       "      <td>-0.141967</td>\n",
       "      <td>0.276398</td>\n",
       "      <td>0.276398</td>\n",
       "      <td>0.276398</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_count_visit</th>\n",
       "      <td>0.294884</td>\n",
       "      <td>-0.099177</td>\n",
       "      <td>0.105027</td>\n",
       "      <td>0.364051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.099177</td>\n",
       "      <td>0.105027</td>\n",
       "      <td>0.364051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107404</td>\n",
       "      <td>-0.114177</td>\n",
       "      <td>0.205723</td>\n",
       "      <td>0.205723</td>\n",
       "      <td>0.205723</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_two_orders_mean</th>\n",
       "      <td>0.440678</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446959</td>\n",
       "      <td>-0.472197</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_two_orders_max</th>\n",
       "      <td>0.440678</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446959</td>\n",
       "      <td>-0.472197</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_two_orders_min</th>\n",
       "      <td>0.440678</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.300478</td>\n",
       "      <td>0.794102</td>\n",
       "      <td>0.214505</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446959</td>\n",
       "      <td>-0.472197</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_transaction_count</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_experience_flag</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows Ã— 1539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Z_sellingprice_count_basket  \\\n",
       "Z_sellingprice_count_basket                       1.000000   \n",
       "Z_sellingprice_count_favorite                    -0.270760   \n",
       "Z_sellingprice_count_order                        0.489683   \n",
       "Z_sellingprice_count_search                       0.415717   \n",
       "Z_sellingprice_count_visit                        0.294884   \n",
       "...                                                    ...   \n",
       "Z_time_btw_two_orders_mean                        0.440678   \n",
       "Z_time_btw_two_orders_max                         0.440678   \n",
       "Z_time_btw_two_orders_min                         0.440678   \n",
       "Z_transaction_count                                    NaN   \n",
       "Z_experience_flag                                      NaN   \n",
       "\n",
       "                               Z_sellingprice_count_favorite  \\\n",
       "Z_sellingprice_count_basket                        -0.270760   \n",
       "Z_sellingprice_count_favorite                       1.000000   \n",
       "Z_sellingprice_count_order                         -0.238396   \n",
       "Z_sellingprice_count_search                        -0.214980   \n",
       "Z_sellingprice_count_visit                         -0.099177   \n",
       "...                                                      ...   \n",
       "Z_time_btw_two_orders_mean                         -0.300478   \n",
       "Z_time_btw_two_orders_max                          -0.300478   \n",
       "Z_time_btw_two_orders_min                          -0.300478   \n",
       "Z_transaction_count                                      NaN   \n",
       "Z_experience_flag                                        NaN   \n",
       "\n",
       "                               Z_sellingprice_count_order  \\\n",
       "Z_sellingprice_count_basket                      0.489683   \n",
       "Z_sellingprice_count_favorite                   -0.238396   \n",
       "Z_sellingprice_count_order                       1.000000   \n",
       "Z_sellingprice_count_search                      0.194701   \n",
       "Z_sellingprice_count_visit                       0.105027   \n",
       "...                                                   ...   \n",
       "Z_time_btw_two_orders_mean                       0.794102   \n",
       "Z_time_btw_two_orders_max                        0.794102   \n",
       "Z_time_btw_two_orders_min                        0.794102   \n",
       "Z_transaction_count                                   NaN   \n",
       "Z_experience_flag                                     NaN   \n",
       "\n",
       "                               Z_sellingprice_count_search  \\\n",
       "Z_sellingprice_count_basket                       0.415717   \n",
       "Z_sellingprice_count_favorite                    -0.214980   \n",
       "Z_sellingprice_count_order                        0.194701   \n",
       "Z_sellingprice_count_search                       1.000000   \n",
       "Z_sellingprice_count_visit                        0.364051   \n",
       "...                                                    ...   \n",
       "Z_time_btw_two_orders_mean                        0.214505   \n",
       "Z_time_btw_two_orders_max                         0.214505   \n",
       "Z_time_btw_two_orders_min                         0.214505   \n",
       "Z_transaction_count                                    NaN   \n",
       "Z_experience_flag                                      NaN   \n",
       "\n",
       "                               Z_sellingprice_count_visit  \\\n",
       "Z_sellingprice_count_basket                      0.294884   \n",
       "Z_sellingprice_count_favorite                   -0.099177   \n",
       "Z_sellingprice_count_order                       0.105027   \n",
       "Z_sellingprice_count_search                      0.364051   \n",
       "Z_sellingprice_count_visit                       1.000000   \n",
       "...                                                   ...   \n",
       "Z_time_btw_two_orders_mean                       0.149270   \n",
       "Z_time_btw_two_orders_max                        0.149270   \n",
       "Z_time_btw_two_orders_min                        0.149270   \n",
       "Z_transaction_count                                   NaN   \n",
       "Z_experience_flag                                     NaN   \n",
       "\n",
       "                               Z_sellingprice_sum_basket  \\\n",
       "Z_sellingprice_count_basket                     1.000000   \n",
       "Z_sellingprice_count_favorite                  -0.270760   \n",
       "Z_sellingprice_count_order                      0.489683   \n",
       "Z_sellingprice_count_search                     0.415717   \n",
       "Z_sellingprice_count_visit                      0.294884   \n",
       "...                                                  ...   \n",
       "Z_time_btw_two_orders_mean                      0.440678   \n",
       "Z_time_btw_two_orders_max                       0.440678   \n",
       "Z_time_btw_two_orders_min                       0.440678   \n",
       "Z_transaction_count                                  NaN   \n",
       "Z_experience_flag                                    NaN   \n",
       "\n",
       "                               Z_sellingprice_sum_favorite  \\\n",
       "Z_sellingprice_count_basket                       0.270760   \n",
       "Z_sellingprice_count_favorite                    -1.000000   \n",
       "Z_sellingprice_count_order                        0.238396   \n",
       "Z_sellingprice_count_search                       0.214980   \n",
       "Z_sellingprice_count_visit                        0.099177   \n",
       "...                                                    ...   \n",
       "Z_time_btw_two_orders_mean                        0.300478   \n",
       "Z_time_btw_two_orders_max                         0.300478   \n",
       "Z_time_btw_two_orders_min                         0.300478   \n",
       "Z_transaction_count                                    NaN   \n",
       "Z_experience_flag                                      NaN   \n",
       "\n",
       "                               Z_sellingprice_sum_order  \\\n",
       "Z_sellingprice_count_basket                    0.489683   \n",
       "Z_sellingprice_count_favorite                 -0.238396   \n",
       "Z_sellingprice_count_order                     1.000000   \n",
       "Z_sellingprice_count_search                    0.194701   \n",
       "Z_sellingprice_count_visit                     0.105027   \n",
       "...                                                 ...   \n",
       "Z_time_btw_two_orders_mean                     0.794102   \n",
       "Z_time_btw_two_orders_max                      0.794102   \n",
       "Z_time_btw_two_orders_min                      0.794102   \n",
       "Z_transaction_count                                 NaN   \n",
       "Z_experience_flag                                   NaN   \n",
       "\n",
       "                               Z_sellingprice_sum_search  \\\n",
       "Z_sellingprice_count_basket                     0.415717   \n",
       "Z_sellingprice_count_favorite                  -0.214980   \n",
       "Z_sellingprice_count_order                      0.194701   \n",
       "Z_sellingprice_count_search                     1.000000   \n",
       "Z_sellingprice_count_visit                      0.364051   \n",
       "...                                                  ...   \n",
       "Z_time_btw_two_orders_mean                      0.214505   \n",
       "Z_time_btw_two_orders_max                       0.214505   \n",
       "Z_time_btw_two_orders_min                       0.214505   \n",
       "Z_transaction_count                                  NaN   \n",
       "Z_experience_flag                                    NaN   \n",
       "\n",
       "                               Z_sellingprice_sum_visit  ...  \\\n",
       "Z_sellingprice_count_basket                    0.294884  ...   \n",
       "Z_sellingprice_count_favorite                 -0.099177  ...   \n",
       "Z_sellingprice_count_order                     0.105027  ...   \n",
       "Z_sellingprice_count_search                    0.364051  ...   \n",
       "Z_sellingprice_count_visit                     1.000000  ...   \n",
       "...                                                 ...  ...   \n",
       "Z_time_btw_two_orders_mean                     0.149270  ...   \n",
       "Z_time_btw_two_orders_max                      0.149270  ...   \n",
       "Z_time_btw_two_orders_min                      0.149270  ...   \n",
       "Z_transaction_count                                 NaN  ...   \n",
       "Z_experience_flag                                   NaN  ...   \n",
       "\n",
       "                               Z_favorite_cnt_before_order_12hour  \\\n",
       "Z_sellingprice_count_basket                             -0.234207   \n",
       "Z_sellingprice_count_favorite                            0.376103   \n",
       "Z_sellingprice_count_order                              -0.388591   \n",
       "Z_sellingprice_count_search                             -0.133130   \n",
       "Z_sellingprice_count_visit                              -0.107404   \n",
       "...                                                           ...   \n",
       "Z_time_btw_two_orders_mean                              -0.446959   \n",
       "Z_time_btw_two_orders_max                               -0.446959   \n",
       "Z_time_btw_two_orders_min                               -0.446959   \n",
       "Z_transaction_count                                           NaN   \n",
       "Z_experience_flag                                             NaN   \n",
       "\n",
       "                               Z_favorite_cnt_before_order_24hour  \\\n",
       "Z_sellingprice_count_basket                             -0.249585   \n",
       "Z_sellingprice_count_favorite                            0.399822   \n",
       "Z_sellingprice_count_order                              -0.413097   \n",
       "Z_sellingprice_count_search                             -0.141967   \n",
       "Z_sellingprice_count_visit                              -0.114177   \n",
       "...                                                           ...   \n",
       "Z_time_btw_two_orders_mean                              -0.472197   \n",
       "Z_time_btw_two_orders_max                               -0.472197   \n",
       "Z_time_btw_two_orders_min                               -0.472197   \n",
       "Z_transaction_count                                           NaN   \n",
       "Z_experience_flag                                             NaN   \n",
       "\n",
       "                               Z_time_btw_basketand_order_mean  \\\n",
       "Z_sellingprice_count_basket                           0.573644   \n",
       "Z_sellingprice_count_favorite                        -0.283978   \n",
       "Z_sellingprice_count_order                            0.928729   \n",
       "Z_sellingprice_count_search                           0.276398   \n",
       "Z_sellingprice_count_visit                            0.205723   \n",
       "...                                                        ...   \n",
       "Z_time_btw_two_orders_mean                            0.792685   \n",
       "Z_time_btw_two_orders_max                             0.792685   \n",
       "Z_time_btw_two_orders_min                             0.792685   \n",
       "Z_transaction_count                                        NaN   \n",
       "Z_experience_flag                                          NaN   \n",
       "\n",
       "                               Z_time_btw_basketand_order_max  \\\n",
       "Z_sellingprice_count_basket                          0.573644   \n",
       "Z_sellingprice_count_favorite                       -0.283978   \n",
       "Z_sellingprice_count_order                           0.928729   \n",
       "Z_sellingprice_count_search                          0.276398   \n",
       "Z_sellingprice_count_visit                           0.205723   \n",
       "...                                                       ...   \n",
       "Z_time_btw_two_orders_mean                           0.792685   \n",
       "Z_time_btw_two_orders_max                            0.792685   \n",
       "Z_time_btw_two_orders_min                            0.792685   \n",
       "Z_transaction_count                                       NaN   \n",
       "Z_experience_flag                                         NaN   \n",
       "\n",
       "                               Z_time_btw_basketand_order_min  \\\n",
       "Z_sellingprice_count_basket                          0.573644   \n",
       "Z_sellingprice_count_favorite                       -0.283978   \n",
       "Z_sellingprice_count_order                           0.928729   \n",
       "Z_sellingprice_count_search                          0.276398   \n",
       "Z_sellingprice_count_visit                           0.205723   \n",
       "...                                                       ...   \n",
       "Z_time_btw_two_orders_mean                           0.792685   \n",
       "Z_time_btw_two_orders_max                            0.792685   \n",
       "Z_time_btw_two_orders_min                            0.792685   \n",
       "Z_transaction_count                                       NaN   \n",
       "Z_experience_flag                                         NaN   \n",
       "\n",
       "                               Z_time_btw_two_orders_mean  \\\n",
       "Z_sellingprice_count_basket                      0.440678   \n",
       "Z_sellingprice_count_favorite                   -0.300478   \n",
       "Z_sellingprice_count_order                       0.794102   \n",
       "Z_sellingprice_count_search                      0.214505   \n",
       "Z_sellingprice_count_visit                       0.149270   \n",
       "...                                                   ...   \n",
       "Z_time_btw_two_orders_mean                       1.000000   \n",
       "Z_time_btw_two_orders_max                        1.000000   \n",
       "Z_time_btw_two_orders_min                        1.000000   \n",
       "Z_transaction_count                                   NaN   \n",
       "Z_experience_flag                                     NaN   \n",
       "\n",
       "                               Z_time_btw_two_orders_max  \\\n",
       "Z_sellingprice_count_basket                     0.440678   \n",
       "Z_sellingprice_count_favorite                  -0.300478   \n",
       "Z_sellingprice_count_order                      0.794102   \n",
       "Z_sellingprice_count_search                     0.214505   \n",
       "Z_sellingprice_count_visit                      0.149270   \n",
       "...                                                  ...   \n",
       "Z_time_btw_two_orders_mean                      1.000000   \n",
       "Z_time_btw_two_orders_max                       1.000000   \n",
       "Z_time_btw_two_orders_min                       1.000000   \n",
       "Z_transaction_count                                  NaN   \n",
       "Z_experience_flag                                    NaN   \n",
       "\n",
       "                               Z_time_btw_two_orders_min  Z_transaction_count  \\\n",
       "Z_sellingprice_count_basket                     0.440678                  NaN   \n",
       "Z_sellingprice_count_favorite                  -0.300478                  NaN   \n",
       "Z_sellingprice_count_order                      0.794102                  NaN   \n",
       "Z_sellingprice_count_search                     0.214505                  NaN   \n",
       "Z_sellingprice_count_visit                      0.149270                  NaN   \n",
       "...                                                  ...                  ...   \n",
       "Z_time_btw_two_orders_mean                      1.000000                  NaN   \n",
       "Z_time_btw_two_orders_max                       1.000000                  NaN   \n",
       "Z_time_btw_two_orders_min                       1.000000                  NaN   \n",
       "Z_transaction_count                                  NaN                  NaN   \n",
       "Z_experience_flag                                    NaN                  NaN   \n",
       "\n",
       "                               Z_experience_flag  \n",
       "Z_sellingprice_count_basket                  NaN  \n",
       "Z_sellingprice_count_favorite                NaN  \n",
       "Z_sellingprice_count_order                   NaN  \n",
       "Z_sellingprice_count_search                  NaN  \n",
       "Z_sellingprice_count_visit                   NaN  \n",
       "...                                          ...  \n",
       "Z_time_btw_two_orders_mean                   NaN  \n",
       "Z_time_btw_two_orders_max                    NaN  \n",
       "Z_time_btw_two_orders_min                    NaN  \n",
       "Z_transaction_count                          NaN  \n",
       "Z_experience_flag                            NaN  \n",
       "\n",
       "[1539 rows x 1539 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5cb06071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c4ebd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de0849d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in corr_df.columns:\n",
    "\n",
    "    tmp_df = corr_df[[col]]\n",
    "    tmp_df = tmp_df[(tmp_df[col]>0.9)]\n",
    "\n",
    "    if tmp_df.shape[0]>1:\n",
    "        tmp_df = pd.merge(tmp_df,uni_roc,how='left',left_index=True,right_index=True)\n",
    "        tmp_df = tmp_df.sort_values(by='roc',ascending = False)\n",
    "\n",
    "        keep_list = keep_list + list(tmp_df.head(1).index)\n",
    "    else:\n",
    "        keep_list = keep_list + [col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19cde7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "83ddab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list = list(pd.DataFrame(keep_list,columns=['variable_name']).drop_duplicates()['variable_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b38a5a",
   "metadata": {},
   "source": [
    "* LR trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "716ea323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8539578359085476\n",
      "0.5101728846956199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_PL Woman</th>\n",
       "      <td>3.897415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_Makyaj_x</th>\n",
       "      <td>3.508219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_time_btw_basketand_order_max</th>\n",
       "      <td>3.334961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_mean_SOHO</th>\n",
       "      <td>3.289930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_max_Saat</th>\n",
       "      <td>2.358966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_Ev &amp; Mobilya</th>\n",
       "      <td>-1.986683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_median_Apple</th>\n",
       "      <td>-2.211903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_mean_Elektrikli Ev Aletleri</th>\n",
       "      <td>-2.357573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_weekly_favorite_secs_btw_consecutives_max</th>\n",
       "      <td>-2.952093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_sellingprice_sum_KadÄ±n</th>\n",
       "      <td>-3.518718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             coefficient\n",
       "Z_sellingprice_sum_PL Woman                     3.897415\n",
       "Z_sellingprice_median_Makyaj_x                  3.508219\n",
       "Z_time_btw_basketand_order_max                  3.334961\n",
       "Z_sellingprice_mean_SOHO                        3.289930\n",
       "Z_sellingprice_max_Saat                         2.358966\n",
       "...                                                  ...\n",
       "Z_sellingprice_sum_Ev & Mobilya                -1.986683\n",
       "Z_sellingprice_median_Apple                    -2.211903\n",
       "Z_sellingprice_mean_Elektrikli Ev Aletleri     -2.357573\n",
       "Z_weekly_favorite_secs_btw_consecutives_max    -2.952093\n",
       "Z_sellingprice_sum_KadÄ±n                       -3.518718\n",
       "\n",
       "[286 rows x 1 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1',C=10,solver='saga')\n",
    "lr.fit(X_train_cc[variable_list],y_train)\n",
    "pred_train = pd.DataFrame(lr.predict_proba(X_train_cc[variable_list])[:,1],columns=['pred'],index=X_train_cc.index)\n",
    "pred_val = pd.DataFrame(lr.predict_proba(X_val_cc[variable_list])[:,1],columns=['pred'],index=X_val_cc.index)\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "pred_train['pred_binary'] = np.where(pred_train['pred']>0.5,1,0)\n",
    "pred_val['pred_binary'] = np.where(pred_val['pred']>0.5,1,0)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])\n",
    "\n",
    "err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "print(roc_val)\n",
    "print(err_rate_val)\n",
    "pd.DataFrame(lr.coef_.T,columns=['coefficient'],index=X_train_cc[variable_list].columns).sort_values(by='coefficient',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c3615f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.DataFrame(lr.predict_proba(X_test_cc[variable_list])[:,1],columns=['pred'],index=X_test_cc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fbec6771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.992,0.973,0.987,0.988,0.989,0.997,0.537,0.565,0.925,0.971,0.37,0.104,0.964,0.993,0.99,0.995,0.303,0.093,0.991,0.931,0.993,0.977,0.984,0.982,0.993,0.999,0.909,0.995,0.951,0.976,0.998,0.95,0.972,0.962,0.983,0.994,0.796,0.794,0.961,0.974,0.981,0.638,0.985,0.976,0.992,0.976,0.984,0.657,0.464,0.159,0.208,0.994,0.892,0.029,0.178,0.057,0.958,0.982,0.992,0.989,0.998,0.813,0.092,0.987,0.996,0.568,0.974,0.961,0.94,0.995,0.989,0.979,0.947,0.99,0.845,0.655,0.991,0.997,0.977,0.924,0.516,0.733,0.998,0.169,0.989,0.983,0.721,0.997,0.978,0.983,0.997,0.983,0.884,0.989,0.415,0.843,0.92,0.899,0.628,0.986,0.97,0.995,0.997,0.93,0.508,0.649,0.656,0.783,0.934,0.575,0.979,0.987,0.99,0.995,0.875,0.988,0.975,0.974,0.983,0.785,0.994,0.999,0.562,0.952,0.998,0.887,0.964,0.994,0.128,0.052,0.996,0.705,0.054,0.983,0.968,0.994,0.6,0.99,0.787,0.999,0.217,0.753,0.958,0.979,0.528,0.95,0.986,0.96,0.22,0.95,0.998,0.989,0.977,0.993,0.618,0.896,0.991,0.943,0.989,0.887,0.974,0.984,0.95,0.689,0.997,0.968,0.785,0.948,0.97,0.274,0.993,0.584,0.697,0.996,0.995,0.958,0.971,0.845,0.331,0.921,0.315,0.196,0.902,0.376,0.101,0.994,0.987,0.995,0.996,0.882,0.988,0.35,0.338,0.916,0.921,0.214,0.985,0.47,0.992,0.711,0.871,0.983,0.997,0.996,0.957,0.675,0.245,0.311,0.217,0.224,0.983,0.967,0.466,0.997,0.964,0.415,0.986,0.896,0.533,0.995,0.918,0.954,0.846,0.804,0.971,0.499,0.057,0.878,0.824,0.106,0.028,0.959,0.91,0.992,0.2,0.988,0.994,0.83,0.991,0.991,0.061,0.985,0.945,0.982,0.998,0.367,0.951,0.123,0.97,0.98,0.874,0.098,0.959,0.989,0.06,0.834,0.995,0.993,0.599,0.111,0.989,0.922,0.355,0.948,0.99,0.99,0.797,0.995,0.965,0.096,0.9,0.96,0.994,0.271,0.976,0.873,0.99,0.956,0.978,0.973,0.992,0.995,0.139,0.933,0.896,0.154,0.924,0.947,0.764,0.977,0.272,0.097,0.755,0.9,0.373,0.16,0.994,0.236,0.76,0.968,0.98,0.973,0.962,0.98,0.999,0.988,0.973,0.979,0.944,0.977,0.998,0.991,0.233,0.958,0.862,0.262,0.998,0.389,0.607,0.481,0.823,0.88,0.267,0.383,0.986,0.926,0.265,0.911,0.898,0.665,0.126,0.985,0.117,0.993,0.966,0.881,0.946,0.995,0.062,0.653,0.936,0.977,0.974,0.978,0.845,0.979,0.862,0.068,0.663,0.995,0.281,0.333,0.983,0.948,0.584,0.991,0.895,0.994,0.994,0.995,0.996,0.97,0.439,0.963,0.945,0.912,0.101,0.988,0.98,0.054,0.991,0.918,0.981,0.996,0.883,0.942,0.99,0.997,0.941,0.902,0.969,0.966,0.103,0.89,0.988,0.914,0.101,0.993,0.997,0.938,0.048,0.972,0.726,0.998,0.876,0.992,0.997,0.993,0.998,0.978,0.874,0.988,0.991,0.762,0.97,0.79,0.263,0.152,0.923,0.14,0.142,0.332,0.955,0.921,0.942,0.102,0.931,0.283,0.475,0.669,0.991,0.979,0.969,0.984,0.411,0.541,0.489,0.21,0.985,0.822,0.907,0.249,0.142,0.993,0.468,0.983,0.966,0.935,0.612,0.997,0.291,0.736,0.991,0.413,0.957,0.734,0.751,0.984,0.012,0.636,0.969,0.744,0.267,0.995,0.933,0.94,0.992,0.997,0.994,0.993,0.995,0.619,0.993,0.885,0.904,0.975,0.103,0.786,0.064,0.994,0.762,0.34,0.12,0.99,0.817,0.96,0.933,0.972,0.431,0.899,0.306,0.783,0.993,0.238,0.131,0.364,0.985,0.976,0.924,0.086,0.088,0.879,0.363,0.815,0.65,0.996,0.973,0.722,0.372,0.065,0.997,0.848,0.996,0.669,0.309,0.997,0.398,0.99,0.969,0.766,0.938,0.882,0.828,0.438,0.238,0.299,0.039,0.616,0.869,0.089,0.235,0.185,0.24,0.515,0.968,0.993,0.971,0.996,0.995,0.99,0.046,0.997,0.463,0.053,0.992,0.86,0.686,0.075,0.83,0.869,0.223,0.958,0.531,0.2,0.932,0.197,0.065,0.969,0.593,0.639,0.988,0.979,0.949,0.988,0.923,0.948,0.027,0.624,0.329,0.89,0.995,0.165,0.554,0.981,0.342,0.989,0.869,0.505,0.302,0.877,0.98,0.879,0.044,0.146,0.788,0.985,0.805,0.935,0.954,0.995,0.992,0.078,0.952,0.992,0.695,0.997,0.994,0.702,0.92,0.946,0.979,0.988,0.858,0.218,0.96,0.948,0.997,0.455,0.108,0.966,0.992,0.999,0.684,0.946,0.184,0.927,0.471,0.975,0.937,0.63,0.853,0.986,0.115,0.822,0.161,0.992,0.721,0.085,0.154,0.29,0.994,0.989,0.971,0.974,0.961,0.304,0.089,0.959,0.943,0.998,0.755,0.746,0.642,0.031,0.966,0.584,0.265,0.051,0.979,0.992,0.996,0.993,0.894,0.635,0.938,0.122,0.521,0.741,0.986,0.957,0.885,0.683,0.037,0.972,0.968,0.929,0.956,0.139,0.963,0.976,0.831,0.812,0.158,0.328,0.251,0.148,0.994,0.998,0.978,0.836,0.35,0.911,0.946,0.476,0.673,0.681,0.981,0.96,0.205,0.193,0.513,0.085,0.83,0.969,0.942,0.818,0.056,0.92,0.963,0.734,0.304,0.304,0.954,0.535,0.856,0.811,0.998,0.998,0.183,0.731,0.927,0.99,0.247,0.997,0.927,0.877,0.993,0.474,0.956,0.159,0.993,0.279,0.998,0.168,0.954,0.961,0.978,0.951,0.944,0.102,0.908,0.847,0.982,0.108,0.938,0.957,0.318,0.989,0.289,0.468,0.979,0.762,0.907,0.985,0.882,0.912,0.99,0.958,0.992,0.993,0.994,0.979,0.982,0.983,0.985,0.455,0.656,0.99,0.395,0.988,0.985,0.174,0.904,0.958,0.565,0.705,0.992,0.607,0.572,0.931,0.995,0.996,0.175,0.615,0.996,0.974,0.89,0.199,0.877,0.494,0.041,0.955,0.997,0.132,0.46,0.248,0.59,0.918,0.989,0.843,0.978,0.482,0.99,0.931,0.996,0.909,0.215,0.766,0.995,0.141,0.207,0.966,0.618,0.882,0.899,0.429,0.986,0.083,0.989,0.756,0.153,0.075,0.846,0.047,0.615,0.997,0.975,0.844,0.97,0.986,0.995,0.484,0.992,0.51,0.57,0.979,0.993,0.959,0.974,0.997,0.993,0.996,0.706,0.804,0.88,0.615,0.884,0.933,0.455,0.998,0.919,0.955,0.937,0.66,0.672,0.872,0.557,0.015,0.885,0.703,0.999,0.755,0.993,0.833,0.903,0.987,0.995,0.717,0.984,0.655,0.742,0.961,0.846,0.448,0.46,0.994,0.848,0.441,0.998,0.783,0.969,0.995,0.664,0.993,0.524,0.367,0.128,0.942,0.666,0.186,0.997,0.503,0.993,0.328,0.966,0.875,0.969,0.992,0.017,0.214,0.635,0.248,0.943,0.983,0.778,0.058,0.998,0.658,0.316,0.328,0.983,0.123,0.193,0.633,0.999,0.972,0.955,0.219,0.103,0.816,0.296,0.969,0.784,0.635,0.344,0.914,0.344,0.236,0.86,0.86,0.178,0.26,0.999,0.992,0.583,0.955,0.998,0.73,0.321,0.99,0.906,0.091,0.979,0.923,0.958,0.546,0.976,0.389,0.755,0.064,0.973,0.997,0.981,0.993,0.492,0.904,0.968,0.031,0.84,0.915,0.975,0.191,0.307,0.994,0.984,0.979,0.928,0.991,0.971,0.138,0.998,0.568,0.129,0.995,0.713,0.963,0.984,0.988,0.995,0.743,0.996,0.611,0.462,0.839,0.955,0.974,0.902,0.247,0.962,0.247,0.949,0.093,0.399,0.945,0.996,0.981,0.987,0.974,0.99,0.561,0.973,0.83,0.602,0.635,0.865,0.326,0.985,0.663,0.956,0.998,0.984,0.994,0.884,0.969,0.297,0.992,0.777,0.915,0.896,0.979,0.996,0.717,0.834,0.96,0.98,0.911,0.731,0.107,0.96,0.768,0.067,0.978,0.687,0.945,0.947,0.496,0.201,0.999,0.341,0.99,0.185,0.901,0.406,0.977,0.037,0.529,0.251,0.491,0.228,0.993,0.969,0.972,0.95,0.525,0.994,0.62,0.815,0.138,0.732,0.987,0.058,0.478,0.111,0.648,0.064,0.145,0.977,0.995,0.939,0.98,0.994,0.375,0.952,0.937,0.972,0.975,0.974,0.187,0.549,0.86,0.53,0.491,0.999,0.992,0.304,0.991,0.99,0.957,0.891,0.425,0.983,0.428,0.975,0.974,0.584,0.942,0.852,0.997,0.291,0.994,0.306,0.519,0.517,0.176,0.712,0.973,0.513,0.554,0.655,0.478,0.219,0.482,0.941,0.217,0.314,0.227,0.979,0.995,0.676,0.996,0.78,0.059,0.36,0.543,0.657,0.399,0.572,0.73,0.24,0.635,0.084,0.979,0.992,0.625,0.454,0.548,0.234,0.974,0.503,0.491,0.24,0.999,0.499,0.56,0.808,0.975,0.319,0.528,0.986,0.568,0.044,0.979,0.059,0.994,0.384,0.904,0.987,0.847,0.998,0.072,0.459,0.603,0.602,0.474,0.963,0.236,0.664,0.117,0.993,0.824,0.815,0.275,0.908,0.811,0.926,0.703,0.993,0.991,0.46,0.284,0.502,0.879,0.967,0.995,0.752,0.931,0.673,0.97,0.854,0.747,0.658,0.985,0.201,0.907,0.506,0.991,0.649,0.828,0.022,0.988,0.736,0.553,0.422,0.249,0.965,0.73,0.87,0.11,0.983,0.362,0.047,0.996,0.593,0.283,0.904,0.151,0.959,0.495,0.9,0.593,0.71,0.436,0.822,0.405,0.823,0.219,0.939,0.9,0.623,0.826,0.909,0.291,0.984,0.936,0.926,0.783,0.987,0.932,0.957,0.865,0.655,0.996,0.781,0.383,0.211,0.167,0.58,0.434,0.441,0.616,0.122,0.329,0.816,0.619,0.982,0.958,0.472,0.988,0.283,0.548,0.082,0.211,0.981,0.912,0.231,0.626,0.974,0.991,0.911,0.838,0.65,0.996,0.93,0.233,0.955,0.624,0.939,0.355,0.539,0.492,0.996,0.732,0.985,0.492,0.535,0.281,0.308,0.997,0.997,0.434,0.917,0.023,0.713,0.855,0.91,0.5,0.238,0.996,0.556,0.994,0.984,0.089,0.785,0.986,0.291,0.686,0.813,0.368,0.964,0.91,0.474,0.662,0.272,0.17,0.208,0.872,0.111,0.576,0.992,0.958,0.99,0.753,0.022,0.994,0.188,0.996,0.283,0.998,0.091,0.805,0.655,0.996,0.243,0.187,0.891,0.753,0.727,0.997,0.648,0.055,0.483,0.93,0.987,0.3,0.961,0.263,0.184,0.715,0.774,0.99,0.422,0.98,0.198,0.253,0.773,0.984,0.233,0.66,0.603,0.478,0.439,0.992,0.712,0.89,0.348,0.878,0.896,0.982,0.997,0.111,0.999,0.701,0.758,0.775,0.609,0.485,0.787,0.739,0.529,0.946,0.993,0.137,0.396,0.629,0.911,0.563,0.092,0.986,0.157,0.142,0.899,0.98,0.418,0.905,0.935,0.335,0.551,0.352,0.426,0.739,0.434,0.748,0.236,0.39,0.18,0.993,0.363,0.444,0.143,0.606,0.574,0.91,0.774,0.592,0.632,0.163,0.325,0.28,0.568,0.147,0.891,0.305,0.244,0.931,0.741,0.44,0.69,0.451,0.512,0.997,0.333,0.734,0.724,0.342,0.366,0.379,0.829,0.759,0.552,0.763,0.212,0.466,0.401,0.798,0.863,0.25,0.977,0.944,0.445,0.399,0.106,0.537,0.56,0.874,0.984,0.831,0.686,0.282,0.382,0.315,0.617,0.931,0.514,0.35,0.568,0.974,0.451,0.08,0.191,0.782,0.954,0.24,0.118,0.514,0.919,0.126,0.273,0.956,0.322,0.992,0.948,0.378,0.352,0.949,0.082,0.612,0.332,0.988,0.663,0.548,0.237,0.918,0.783,0.934,0.994,0.683,0.411,0.966,0.079,0.657,0.964,0.97,0.972,0.828,0.797,0.692,0.986,0.109,0.723,0.605,0.797,0.253,0.975,0.909,0.593,0.297,0.98,0.99,0.656,0.949,0.455,0.977,0.995,0.948,0.604,0.974,0.792,0.917,0.902,0.956,0.716,0.75,0.2,0.961,0.226,0.891,0.497,0.964,0.657,0.858,0.501,0.401,0.992,0.987,0.697,0.968,0.997,0.868,0.415,0.92,0.129,0.946,0.982,0.021,0.918,0.104,0.16,0.906,0.309,0.455,0.985,0.685,0.918,0.907,0.08,0.074,0.098,0.818,0.406,0.296,0.202,0.96,0.087,0.661,0.994,0.267,0.995,0.958,0.984,0.949,0.873,0.475,0.993,0.07,0.814,0.967,0.98,0.988,0.397,0.412,0.98,0.994,0.771,0.909,0.95,0.163,0.875,0.191,0.537,0.859,0.843,0.985,0.076,0.749,0.782,0.371,0.941,0.461,0.902,0.901,0.996,0.93,0.298,0.996,0.356,0.847,0.225,0.228,0.868,0.912,0.984,0.844,0.969,0.91,0.151,0.301,0.273,0.037,0.844,0.202,0.127,0.903,0.763,0.631,0.916,0.997,0.299,0.12,0.949,0.172,0.835,0.288,0.976,0.495,0.983,0.077,0.473,0.432,0.563,0.965,0.48,0.127,0.28,0.377,0.728,0.923,0.82,0.793,0.981,0.506,0.983,0.841,0.182,0.306,0.145,0.994,0.967,0.968,0.963,0.966,0.723,0.964,0.988,0.938,0.221,0.97,0.956,0.905,0.964,0.1,0.181,0.993,0.963,0.305,0.891,0.637,0.976,0.979,0.973,0.883,0.865,0.994,0.916,0.968,0.864,0.839,0.329,0.41,0.774,0.75,0.914,0.131,0.944,0.982,0.11,0.499,0.862,0.571,0.975,0.865,0.936,0.637,0.995,0.525,0.218,0.977,0.924,0.275,0.932,0.035,0.456,0.671,0.543,0.748,0.981,0.924,0.624,0.977,0.987,0.931,0.818,0.904,0.155,0.737,0.938,0.878,0.944,0.861,0.804,0.931,0.695,0.818,0.897,0.688,0.775,0.358,0.127,0.328,0.091,0.568,0.982,0.961,0.03,0.859,0.714,0.072,0.93,0.782,0.573,0.353,0.164,0.309,0.627,0.98,0.89,0.163,0.957,0.641,0.596,0.348,0.604,0.192,0.27,0.882,0.559,0.439,0.946,0.994,0.191,0.455,0.627,0.936,0.22,0.669,0.63,0.807,0.884,0.217,0.425,0.384,0.763,0.976,0.914,0.15,0.209,0.873,0.935,0.937,0.12,0.768,0.705,0.877,0.895,0.95,0.984,0.695,0.888,0.952,0.735,0.904,0.581,0.887,0.688,0.933,0.956,0.286,0.101,0.221,0.364,0.973,0.501,0.108,0.859,0.478,0.765,0.423,0.418,0.899,0.15,0.155,0.593,0.561,0.752,0.561,0.556,0.591,0.389,0.23,0.846,0.636,0.568,0.486,0.769,0.677,0.595,0.812,0.585,0.744,0.661,0.145,0.477,0.658,0.869,0.263,0.609,0.333,0.942,0.64,0.532,0.104,0.599,0.612,0.87,0.635,0.511,0.794,0.634,0.751,0.087,0.856,0.574,0.852,0.43,0.702,0.485,0.535,0.486,0.498,0.749,0.804,0.721,0.48,0.634,0.327,0.132,0.873,0.797,0.633,0.7,0.483,0.786,0.536,0.306,0.219,0.211,0.708,0.328,0.089,0.68,0.863,0.833,0.975,0.85,0.85,0.792,0.744,0.66,0.845,0.82,0.934,0.939,0.832,0.804,0.819,0.864,0.958,0.169,0.802,0.754,0.869,0.598,0.368,0.659,0.701,0.314,0.952,0.88,0.957,0.282,0.822,0.942,0.996,0.947,0.899,0.755,0.647,0.162,0.709,0.262,0.303,0.909,0.393,0.468,0.365,0.872,0.648,0.308,0.839,0.358,0.759,0.898,0.562,0.605,0.637,0.462,0.749,0.526,0.779,0.912,0.704,0.396,0.328,0.837,0.402,0.89,0.243,0.309,0.781,0.6,0.9,0.513,0.314,0.828,0.905,0.48,0.652,0.926,0.37,0.846,0.979,0.845,0.669,0.705,0.965,0.903,0.951,0.11,0.765,0.756,0.545,0.512,0.519,0.18,0.453,0.525,0.272,0.393,0.191,0.092,0.925,0.938,0.396,0.53,0.151,0.269,0.509,0.21,0.223,0.474,0.721,0.122,0.634,0.954,0.576,0.275,0.301,0.398,0.877,0.312,0.913,0.474,0.538,0.713,0.885,0.699,0.744,0.684,0.419,0.561,0.856,0.524,0.296,0.62,0.115,0.753,0.852,0.889,0.83,0.849,0.59,0.83,0.812,0.903,0.904,0.391,0.477,0.455,0.416,0.302,0.772,0.902,0.568,0.463,0.474,0.417,0.312,0.884,0.639,0.529,0.542,0.252,0.388,0.54,0.5,0.595,0.955,0.62,0.684,0.917,0.457,0.555,0.7,0.788,0.735,0.812,0.691,0.307,0.52,0.867,0.532,0.595,0.526,0.437,0.523,0.417,0.513,0.711,0.749,0.415,0.918,0.906,0.339,0.869,0.906,0.917,0.631,0.179,0.541,0.597,0.461,0.746,0.317,0.546,0.389,0.162,0.655,0.602,0.594,0.188,0.434,0.219,0.58,0.581,0.606,0.674,0.967,0.576,0.522,0.868,0.468,0.95,0.486,0.103,0.661,0.601,0.854,0.751,0.384,0.347,0.903,0.758,0.567,0.58,0.548,0.333,0.319,0.598,0.371,0.33,0.78,0.608,0.908,0.203,0.299,0.105,0.733,0.817,0.939,0.289,0.874,0.361,0.948,0.907,0.847,0.914,0.207,0.716,0.208,0.596,0.397,0.657,0.521,0.464,0.599,0.683,0.608,0.536,0.93,0.808,0.587,0.516,0.428,0.499,0.321,0.315,0.895,0.485,0.656,0.806,0.649,0.814,0.935,0.434,0.383,0.122,0.562,0.939,0.694,0.268,0.726,0.314,0.94,0.693,0.491,0.451,0.195,0.309,0.303,0.54,0.681,0.568,0.364,0.49,0.428,0.6,0.829,0.577,0.844,0.787,0.466,0.545,0.516,0.461,0.688,0.288,0.687,0.344,0.582,0.584,0.298,0.528,0.54,0.588,0.344,0.676,0.362,0.616,0.667,0.702,0.619,0.749,0.541,0.172,0.269,0.464,0.755,0.585,0.45,0.474,0.929,0.22,0.88,0.61,0.838,0.432,0.762,0.568,0.729,0.904,0.83,0.509,0.885,0.518,0.506,0.424,0.485,0.433,0.709,0.523,0.456,0.576,0.23,0.214,0.324,0.74,0.822,0.407,0.364,0.382,0.53,0.498,0.48,0.341,0.531,0.501,0.864,0.888,0.589,0.285,0.548,0.544,0.589,0.698,0.742,0.679,0.588,0.824,0.573,0.381,0.725,0.623,0.345,0.477,0.465,0.444,0.897,0.307,0.209,0.212,0.949,0.602,0.618,0.738,0.703,0.284,0.527,0.782,0.408,0.717,0.723,0.868,0.49,0.565,0.575,0.689,0.513,0.574,0.508,0.627,0.429,0.666,0.696,0.171,0.889,0.394,0.461,0.469,0.607,0.628,0.572,0.696,0.333,0.587,0.589,0.462,0.575,0.527,0.846,0.575,0.79,0.878,0.37,0.452,0.834,0.18,0.545,0.366,0.414,0.642,0.703,0.709,0.586,0.932,0.544,0.379,0.503,0.525,0.427,0.424,0.426,0.518,0.468,0.524,0.343,0.249,0.598,0.66,0.472,0.686,0.749,0.392,0.586,0.533,0.519,0.584,0.544,0.439,0.381,0.624,0.57,0.51,0.62,0.374,0.758,0.512,0.931,0.492,0.567,0.547,0.553,0.445,0.387,0.365,0.692,0.112,0.746,0.336,0.48,0.428,0.501'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub25 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub25['pred'] = sub25['pred'].apply(lambda x: round(x,3))\n",
    "sub25_txt = ''\n",
    "for prob in list(sub25['pred'].values):\n",
    "    sub25_txt = sub25_txt+','+str(prob)\n",
    "sub25_txt = sub25_txt[1:]\n",
    "\n",
    "sub25_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a69676",
   "metadata": {},
   "source": [
    "* Submission 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ccd052c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:46:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=100,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_train_cc[variable_list],y_train)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_cc[variable_list])[:,1],columns=['pred'],index=X_train_cc.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_cc[variable_list])[:,1],columns=['pred'],index=X_val_cc.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_cc[variable_list])[:,1],columns=['pred'],index=X_test_cc.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d7caccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.999999890494579\n",
      "Val ROC:  0.8507392565011447\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5a5a2609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0,0.998,0.985,1.0,0.994,1.0,0.918,0.345,0.949,0.997,0.039,0.041,0.985,1.0,0.996,0.999,0.57,0.006,1.0,0.999,1.0,0.995,0.998,0.923,0.998,1.0,0.99,1.0,0.99,0.986,1.0,0.982,0.998,0.998,1.0,1.0,0.908,0.952,0.999,0.998,0.994,0.951,0.999,1.0,0.999,0.976,1.0,0.542,0.24,0.035,0.157,1.0,0.999,0.001,0.044,0.007,0.993,0.999,1.0,0.998,1.0,0.867,0.025,0.998,0.999,0.938,0.999,0.989,0.954,1.0,0.999,0.997,0.986,0.993,0.965,0.861,0.997,1.0,1.0,0.967,0.544,0.833,1.0,0.1,0.995,0.997,0.841,0.998,0.999,0.982,1.0,0.998,0.762,0.999,0.032,0.946,0.992,0.997,0.068,1.0,0.999,1.0,1.0,0.998,0.584,0.325,0.884,0.943,0.996,0.145,0.995,0.999,1.0,0.998,0.918,1.0,0.918,0.999,0.991,0.985,0.998,1.0,0.074,0.963,1.0,0.946,0.998,1.0,0.004,0.109,0.998,0.768,0.008,0.999,0.931,1.0,0.689,0.999,0.996,0.997,0.009,0.745,0.999,0.999,0.572,0.994,0.999,0.978,0.096,0.973,1.0,0.997,0.999,1.0,0.326,0.996,0.997,0.948,0.999,0.929,0.999,0.998,0.999,0.935,1.0,0.998,0.899,0.988,0.999,0.821,1.0,0.771,0.78,1.0,1.0,0.998,0.997,0.999,0.109,0.969,0.116,0.073,0.992,0.059,0.045,0.999,1.0,1.0,1.0,0.983,0.995,0.08,0.187,0.991,0.998,0.424,1.0,0.817,0.999,0.937,0.312,0.994,1.0,1.0,0.997,0.995,0.069,0.094,0.014,0.01,0.999,1.0,0.459,1.0,0.989,0.518,0.997,0.998,0.923,0.999,0.993,0.999,0.945,0.96,0.999,0.52,0.012,0.805,0.992,0.021,0.01,0.998,0.856,1.0,0.008,0.999,1.0,0.751,1.0,0.999,0.005,0.999,0.996,0.999,0.999,0.047,0.998,0.004,0.999,0.998,0.748,0.008,0.988,0.999,0.004,0.914,1.0,0.999,0.497,0.012,0.996,0.823,0.335,0.999,0.999,0.999,0.865,1.0,0.996,0.031,0.993,0.99,1.0,0.336,0.998,0.996,0.993,0.93,0.999,0.991,0.998,0.999,0.006,0.981,0.989,0.048,0.997,0.995,0.99,1.0,0.03,0.001,0.545,0.998,0.2,0.021,0.999,0.035,0.96,0.998,0.999,1.0,0.956,0.981,1.0,0.996,0.998,0.998,0.994,0.999,0.999,1.0,0.132,0.997,0.879,0.01,1.0,0.759,0.946,0.578,0.689,0.965,0.425,0.367,0.998,0.989,0.004,0.94,0.966,0.905,0.007,1.0,0.063,0.999,0.999,0.997,0.989,1.0,0.02,0.529,0.96,0.998,0.914,0.999,0.995,0.996,0.976,0.002,0.958,1.0,0.107,0.162,1.0,0.997,0.235,0.999,0.917,1.0,1.0,1.0,0.999,0.995,0.401,0.998,1.0,0.989,0.015,1.0,1.0,0.003,0.998,0.999,0.995,1.0,0.961,0.991,0.983,1.0,0.995,0.973,0.999,1.0,0.009,0.997,0.998,0.996,0.013,1.0,1.0,0.997,0.04,0.999,0.821,0.999,0.99,0.999,1.0,1.0,0.995,0.999,0.984,1.0,1.0,0.992,0.999,0.998,0.026,0.063,0.99,0.013,0.105,0.191,0.999,0.997,0.998,0.004,0.999,0.307,0.441,0.993,0.999,0.996,0.992,1.0,0.204,0.482,0.438,0.046,0.999,0.928,0.993,0.01,0.071,1.0,0.276,0.998,0.999,0.998,0.121,1.0,0.359,0.901,1.0,0.119,0.758,0.956,0.792,0.999,0.005,0.773,1.0,0.975,0.054,1.0,0.918,0.993,1.0,1.0,1.0,1.0,1.0,0.942,1.0,0.998,0.996,0.999,0.013,0.729,0.114,0.999,0.985,0.486,0.055,1.0,0.947,0.998,0.988,1.0,0.242,0.953,0.05,0.984,1.0,0.123,0.011,0.123,0.999,0.999,0.986,0.026,0.046,0.898,0.492,0.991,0.098,1.0,0.998,0.975,0.062,0.094,1.0,0.975,1.0,0.364,0.191,1.0,0.239,1.0,0.998,0.882,0.996,0.97,0.993,0.165,0.039,0.02,0.012,0.014,0.994,0.024,0.091,0.032,0.066,0.834,0.986,1.0,0.997,0.999,1.0,1.0,0.006,1.0,0.616,0.064,0.998,0.992,0.769,0.006,0.974,0.94,0.013,0.995,0.866,0.021,0.996,0.013,0.019,1.0,0.974,0.957,1.0,0.994,0.999,0.999,0.946,0.99,0.016,0.8,0.065,0.996,1.0,0.495,0.601,1.0,0.138,1.0,0.965,0.63,0.073,0.93,0.999,0.69,0.001,0.004,0.943,0.999,0.89,0.994,0.998,0.999,1.0,0.253,0.99,0.999,0.328,1.0,1.0,0.714,0.999,0.996,0.989,0.999,0.974,0.369,0.994,0.999,1.0,0.096,0.002,1.0,1.0,0.997,0.219,0.973,0.022,0.92,0.641,0.999,0.993,0.858,0.996,0.998,0.009,0.978,0.013,0.999,0.971,0.025,0.286,0.118,0.999,0.999,0.989,0.996,0.997,0.565,0.05,1.0,0.995,1.0,0.947,0.978,0.8,0.002,0.997,0.861,0.19,0.077,0.993,1.0,1.0,1.0,0.996,0.372,0.998,0.161,0.516,0.991,0.999,0.998,0.922,0.895,0.001,0.997,0.996,0.996,0.997,0.039,0.996,1.0,0.963,0.817,0.06,0.249,0.316,0.06,0.997,1.0,1.0,0.991,0.174,0.996,0.975,0.019,0.944,0.714,0.999,0.973,0.042,0.03,0.829,0.006,0.757,0.994,0.999,0.998,0.111,0.986,0.971,0.989,0.145,0.082,1.0,0.851,0.997,0.935,1.0,1.0,0.036,0.587,0.99,0.995,0.054,1.0,0.993,0.988,1.0,0.3,0.998,0.02,1.0,0.807,1.0,0.021,0.996,0.999,0.999,0.999,0.958,0.033,0.859,0.985,0.997,0.098,0.999,0.996,0.398,1.0,0.138,0.094,0.998,0.975,0.987,0.995,0.973,0.982,1.0,0.999,1.0,1.0,1.0,0.988,0.979,1.0,1.0,0.06,0.951,1.0,0.908,1.0,0.997,0.028,0.963,0.998,0.457,0.965,0.999,0.501,0.879,0.996,0.997,1.0,0.059,0.956,1.0,0.994,0.96,0.014,0.995,0.85,0.008,1.0,1.0,0.005,0.625,0.262,0.169,0.938,1.0,0.98,0.998,0.239,0.997,0.969,1.0,0.994,0.033,0.993,1.0,0.019,0.419,0.999,0.182,0.919,0.931,0.845,1.0,0.014,0.999,0.172,0.118,0.003,0.989,0.006,0.962,1.0,0.994,0.958,1.0,1.0,1.0,0.085,0.999,0.881,0.858,0.989,1.0,0.998,1.0,1.0,0.998,1.0,0.886,0.996,0.934,0.655,0.9,0.993,0.222,1.0,0.975,0.983,0.984,0.128,0.48,0.959,0.683,0.0,0.976,0.662,1.0,0.96,0.999,0.997,0.87,0.994,0.999,0.883,1.0,0.935,0.831,0.999,0.985,0.066,0.251,1.0,0.991,0.107,1.0,0.997,0.993,1.0,0.138,1.0,0.694,0.145,0.078,0.99,0.755,0.052,1.0,0.719,0.999,0.066,0.974,0.979,1.0,0.999,0.006,0.147,0.846,0.009,0.985,0.999,0.985,0.006,0.999,0.581,0.211,0.248,1.0,0.01,0.042,0.526,1.0,0.999,0.995,0.095,0.057,0.924,0.142,0.999,0.98,0.695,0.295,0.994,0.017,0.055,0.986,0.962,0.051,0.023,1.0,1.0,0.008,0.862,1.0,0.939,0.929,0.999,0.984,0.033,0.997,0.999,0.994,0.197,0.998,0.331,0.97,0.047,0.997,1.0,1.0,0.998,0.247,0.992,0.999,0.004,0.911,0.991,0.994,0.169,0.058,0.999,0.999,0.996,0.975,1.0,1.0,0.081,1.0,0.947,0.006,1.0,0.768,1.0,1.0,1.0,0.999,0.763,0.999,0.778,0.064,0.976,0.994,0.999,0.996,0.292,0.994,0.512,0.997,0.034,0.051,0.999,0.992,0.998,1.0,0.999,1.0,0.695,0.999,0.916,0.965,0.823,0.935,0.301,1.0,0.943,0.982,1.0,0.999,1.0,0.989,0.997,0.086,0.998,0.979,0.98,0.992,1.0,1.0,0.93,0.79,1.0,1.0,0.737,0.748,0.083,1.0,0.874,0.016,1.0,0.576,0.995,1.0,0.131,0.011,1.0,0.318,0.998,0.177,0.977,0.273,0.996,0.004,0.323,0.016,0.182,0.002,0.997,1.0,0.999,0.996,0.45,1.0,0.635,0.756,0.033,0.979,0.992,0.023,0.233,0.002,0.822,0.019,0.006,0.998,1.0,0.98,0.989,1.0,0.196,0.999,0.999,0.999,0.999,0.999,0.376,0.989,0.99,0.203,0.683,1.0,0.999,0.444,1.0,0.999,0.983,0.998,0.281,0.999,0.315,0.985,0.999,0.557,0.992,0.833,1.0,0.334,0.999,0.044,0.369,0.855,0.409,0.967,0.997,0.06,0.306,0.8,0.173,0.204,0.275,0.999,0.004,0.006,0.419,0.999,0.999,0.831,1.0,0.945,0.029,0.145,0.97,0.07,0.107,0.931,0.987,0.275,0.638,0.003,0.999,0.999,0.921,0.239,0.631,0.081,0.999,0.255,0.53,0.387,1.0,0.551,0.517,0.955,1.0,0.057,0.254,1.0,0.906,0.181,0.995,0.009,0.996,0.365,0.959,0.971,0.973,1.0,0.04,0.335,0.95,0.84,0.383,0.999,0.005,0.294,0.597,0.999,0.996,0.792,0.651,0.995,0.992,0.979,0.303,0.999,0.999,0.815,0.122,0.296,0.986,1.0,0.998,0.947,0.996,0.786,0.999,0.991,0.841,0.973,1.0,0.021,0.999,0.789,0.999,0.913,0.969,0.001,0.999,0.99,0.033,0.517,0.066,1.0,0.909,0.938,0.042,1.0,0.388,0.002,1.0,0.144,0.071,0.96,0.281,1.0,0.623,0.989,0.307,0.992,0.898,0.978,0.463,0.869,0.103,0.996,0.999,0.545,0.998,0.992,0.045,0.997,0.981,0.999,0.946,1.0,0.99,0.998,0.994,0.964,1.0,0.798,0.657,0.094,0.175,0.757,0.766,0.287,0.928,0.044,0.036,0.999,0.886,1.0,1.0,0.262,0.999,0.043,0.328,0.032,0.039,1.0,0.983,0.122,0.991,0.998,1.0,0.994,0.756,0.919,1.0,0.998,0.121,0.998,0.361,0.997,0.051,0.924,0.421,1.0,0.935,0.999,0.479,0.885,0.252,0.324,1.0,1.0,0.066,1.0,0.007,0.921,0.996,0.951,0.716,0.551,0.99,0.382,0.996,0.997,0.003,0.988,0.999,0.134,0.551,0.953,0.506,0.961,0.999,0.259,0.615,0.13,0.05,0.006,0.961,0.046,0.229,0.992,0.996,0.999,0.823,0.004,1.0,0.047,1.0,0.112,1.0,0.103,0.895,0.816,0.999,0.024,0.02,0.919,0.923,0.938,1.0,0.887,0.002,0.241,0.998,1.0,0.155,1.0,0.003,0.119,0.725,0.992,1.0,0.03,0.996,0.103,0.076,0.775,1.0,0.168,0.802,0.248,0.07,0.042,1.0,0.731,0.996,0.086,0.988,0.992,1.0,0.999,0.01,1.0,0.685,0.652,0.764,0.865,0.651,0.286,0.889,0.024,0.995,0.999,0.042,0.332,0.503,0.993,0.912,0.004,0.993,0.032,0.144,0.914,0.998,0.46,0.993,0.981,0.027,0.409,0.269,0.115,0.977,0.059,0.948,0.23,0.259,0.602,1.0,0.515,0.544,0.005,0.181,0.18,0.993,0.977,0.969,0.661,0.16,0.645,0.41,0.991,0.022,0.993,0.129,0.115,0.999,0.193,0.914,0.539,0.95,0.762,1.0,0.126,0.991,0.431,0.052,0.197,0.051,0.934,0.991,0.439,0.984,0.09,0.054,0.022,0.989,0.996,0.032,1.0,0.864,0.106,0.618,0.03,0.679,0.508,0.981,1.0,0.866,0.942,0.429,0.646,0.642,0.81,0.98,0.515,0.346,0.944,0.99,0.793,0.273,0.1,0.858,0.996,0.583,0.363,0.659,0.992,0.324,0.042,0.992,0.218,1.0,0.983,0.438,0.025,0.998,0.054,0.733,0.147,1.0,0.725,0.826,0.079,0.964,0.892,0.998,0.999,0.735,0.275,0.999,0.222,0.909,1.0,0.998,0.996,0.934,0.553,0.791,1.0,0.02,0.959,0.638,0.992,0.056,1.0,0.799,0.41,0.355,0.999,0.999,0.786,0.999,0.14,1.0,1.0,0.999,0.934,1.0,0.997,0.995,0.985,0.995,0.8,0.977,0.081,1.0,0.846,0.906,0.303,0.93,0.722,0.975,0.938,0.375,0.999,0.999,0.795,0.974,1.0,0.998,0.204,0.984,0.01,0.998,1.0,0.001,0.997,0.007,0.007,0.863,0.177,0.68,1.0,0.924,0.994,1.0,0.01,0.002,0.006,0.985,0.027,0.334,0.125,0.999,0.021,0.682,1.0,0.073,0.997,0.998,1.0,0.999,0.915,0.17,1.0,0.048,0.965,0.996,1.0,0.999,0.771,0.785,0.996,0.999,0.876,0.998,0.998,0.201,0.994,0.173,0.305,0.997,0.937,0.995,0.265,0.987,0.95,0.223,0.998,0.097,0.979,0.997,1.0,0.934,0.294,0.999,0.196,0.984,0.035,0.495,0.971,0.981,0.999,0.869,0.998,0.972,0.011,0.18,0.052,0.004,0.997,0.07,0.115,0.998,0.949,0.866,0.937,1.0,0.009,0.011,0.999,0.045,0.996,0.117,0.997,0.261,0.999,0.539,0.074,0.353,0.494,0.987,0.182,0.012,0.118,0.382,0.96,0.985,0.964,0.854,1.0,0.039,0.995,0.992,0.146,0.349,0.01,1.0,0.995,0.999,0.999,0.996,0.301,1.0,1.0,0.984,0.002,0.989,0.971,0.977,0.999,0.012,0.181,0.999,0.999,0.043,0.998,0.413,0.998,0.996,1.0,0.673,0.954,0.999,0.97,0.997,0.771,0.993,0.675,0.169,0.872,0.409,0.996,0.039,1.0,1.0,0.049,0.071,0.977,0.097,0.994,0.997,0.938,0.985,1.0,0.141,0.006,1.0,0.973,0.1,0.993,0.002,0.091,0.972,0.043,0.908,0.999,0.988,0.038,0.998,0.999,0.999,0.973,0.987,0.108,0.964,0.985,0.997,0.999,0.991,0.731,0.97,0.91,0.738,0.991,0.954,0.937,0.033,0.028,0.01,0.005,0.957,1.0,0.998,0.005,0.854,0.671,0.018,0.978,0.872,0.433,0.156,0.122,0.166,0.286,0.999,0.971,0.173,0.947,0.385,0.563,0.15,0.299,0.014,0.709,0.962,0.405,0.807,0.995,1.0,0.058,0.077,0.952,1.0,0.038,0.976,0.286,0.986,0.997,0.021,0.379,0.028,0.326,0.999,0.983,0.319,0.045,0.981,0.986,0.999,0.672,0.964,0.676,0.993,0.992,1.0,0.994,0.974,0.996,0.993,0.731,0.987,0.026,0.995,0.86,0.99,0.976,0.026,0.015,0.127,0.133,0.999,0.866,0.475,0.969,0.353,0.828,0.086,0.453,0.974,0.028,0.008,0.247,0.938,0.984,0.938,0.277,0.2,0.009,0.118,0.984,0.546,0.771,0.633,0.968,0.639,0.368,0.948,0.525,0.588,0.84,0.02,0.86,0.515,0.99,0.517,0.299,0.054,1.0,0.37,0.766,0.011,0.785,0.722,0.957,0.567,0.889,0.954,0.8,0.975,0.211,0.962,0.388,0.908,0.16,0.988,0.117,0.647,0.104,0.78,0.627,0.869,0.601,0.069,0.154,0.353,0.016,0.987,0.959,0.683,0.97,0.309,0.746,0.657,0.086,0.491,0.073,0.996,0.484,0.067,0.702,0.988,0.997,0.999,0.977,0.963,0.988,0.974,0.836,0.991,0.974,0.998,0.997,0.988,0.97,0.977,0.934,0.998,0.093,0.903,0.807,0.913,0.957,0.709,0.919,0.939,0.197,0.999,0.99,0.982,0.359,0.962,1.0,1.0,1.0,0.987,0.971,0.979,0.008,0.835,0.048,0.259,0.899,0.281,0.476,0.621,0.769,0.886,0.134,0.998,0.02,0.914,0.947,0.449,0.302,0.136,0.201,0.892,0.023,0.837,0.998,0.614,0.304,0.084,0.975,0.138,0.974,0.524,0.054,0.985,0.561,0.989,0.572,0.169,0.996,0.998,0.139,0.574,0.984,0.042,0.92,1.0,0.99,0.905,0.513,0.99,0.986,0.997,0.035,0.992,0.934,0.5,0.64,0.602,0.041,0.408,0.735,0.557,0.325,0.007,0.041,0.986,0.999,0.177,0.285,0.555,0.025,0.117,0.571,0.052,0.285,0.998,0.538,0.837,0.999,0.684,0.193,0.043,0.562,0.465,0.478,0.993,0.285,0.199,0.865,0.992,0.886,0.886,0.983,0.143,0.426,0.664,0.501,0.588,0.827,0.046,0.857,0.982,0.991,0.97,0.804,0.93,0.988,0.956,0.995,0.997,0.704,0.779,0.364,0.19,0.273,0.898,0.998,0.956,0.225,0.393,0.041,0.789,0.99,0.847,0.59,0.282,0.19,0.099,0.923,0.342,0.431,0.994,0.272,0.976,0.99,0.957,0.889,0.982,0.983,0.941,1.0,0.692,0.009,0.136,0.976,0.817,0.699,0.912,0.931,0.195,0.049,0.978,0.424,0.911,0.62,0.996,0.989,0.086,0.934,0.998,0.997,0.745,0.536,0.33,0.84,0.506,0.785,0.026,0.957,0.995,0.028,0.941,0.286,0.571,0.025,0.061,0.772,0.649,0.794,0.434,0.961,0.914,0.824,0.351,0.98,0.772,0.997,0.345,0.275,0.358,0.449,0.982,0.977,0.633,0.68,0.94,0.982,0.939,0.344,0.419,0.109,0.292,0.655,0.803,0.386,0.736,0.264,0.995,0.072,0.057,0.155,0.283,0.943,0.993,0.048,0.991,0.02,1.0,0.972,0.997,0.999,0.119,0.006,0.026,0.563,0.757,0.419,0.973,0.518,0.932,0.563,0.527,0.6,0.986,0.987,0.624,0.053,0.315,0.357,0.835,0.034,0.927,0.085,0.711,0.819,0.889,0.952,0.981,0.022,0.275,0.01,0.131,0.998,0.997,0.634,0.996,0.283,0.98,0.847,0.328,0.019,0.805,0.573,0.116,0.561,0.879,0.916,0.808,0.058,0.857,0.382,0.986,0.007,0.987,0.697,0.759,0.14,0.12,0.547,0.956,0.008,0.986,0.384,0.702,0.972,0.354,0.551,0.744,0.785,0.126,0.892,0.853,0.81,0.154,0.84,0.645,0.876,0.286,0.051,0.058,0.016,0.959,0.898,0.866,0.89,0.997,0.196,0.984,0.221,0.997,0.246,0.931,0.02,0.86,0.963,0.999,0.012,0.957,0.573,0.335,0.569,0.855,0.797,0.687,0.413,0.119,0.232,0.659,0.106,0.649,0.93,0.949,0.18,0.363,0.01,0.01,0.21,0.14,0.625,0.326,0.272,0.965,0.888,0.854,0.406,0.069,0.887,0.889,0.975,0.678,0.766,0.007,0.915,0.257,0.15,0.825,0.714,0.452,0.802,0.553,0.077,0.994,0.096,0.035,0.796,0.993,0.269,0.959,0.979,0.859,0.751,0.318,0.86,0.662,0.994,0.946,0.993,0.701,0.178,0.662,0.988,0.366,0.92,0.418,0.373,0.892,0.513,0.303,0.172,0.991,0.091,0.039,0.083,0.132,0.795,0.86,0.028,0.384,0.304,0.813,0.661,0.618,0.902,0.625,0.17,0.964,0.998,0.01,0.134,0.992,0.006,0.045,0.756,0.04,0.751,0.743,0.762,0.719,0.987,0.095,0.338,0.509,0.983,0.464,0.053,0.688,0.409,0.153,0.984,0.189,0.045,0.685,0.985,0.537,0.917,0.993,0.162,0.853,0.854,0.553,0.915,0.856,0.644,0.004,0.837,0.867,0.855,0.944,0.823,0.83,0.463,0.999,0.001,0.028,0.007,0.006,0.706,0.19,0.864,0.964,0.006,0.23,0.846,0.523,0.859,0.062'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub26 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub26['pred'] = sub26['pred'].apply(lambda x: round(x,3))\n",
    "sub26_txt = ''\n",
    "for prob in list(sub26['pred'].values):\n",
    "    sub26_txt = sub26_txt+','+str(prob)\n",
    "sub26_txt = sub26_txt[1:]\n",
    "\n",
    "sub26_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f7124956",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(variable_list).to_csv('variable_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "009f37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cc.to_csv('X_train_cc.csv')\n",
    "X_val_cc.to_csv('X_val_cc.csv')\n",
    "X_test_cc.to_csv('X_test_cc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
