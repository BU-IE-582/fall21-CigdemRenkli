{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a739384",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16020846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances,pairwise_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from metric_learn import NCA\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8832d8",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed626f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_unique.csv')\n",
    "X_val = pd.read_csv('X_val_unique.csv')\n",
    "X_test = pd.read_csv('X_test_unique.csv')\n",
    "\n",
    "y_train = pd.read_csv('y_train_unique.csv')\n",
    "y_val = pd.read_csv('y_val_unique.csv')\n",
    "\n",
    "X_train.set_index('unique_id',inplace=True)\n",
    "X_val.set_index('unique_id',inplace=True)\n",
    "X_test.set_index('unique_id',inplace=True)\n",
    "y_train.set_index('unique_id',inplace=True)\n",
    "y_val.set_index('unique_id',inplace=True)\n",
    "\n",
    "X_train_experienced = X_train[X_train['experience_flag']==1]\n",
    "y_train_experienced = y_train[y_train.index.isin(list(X_train_experienced.index))]\n",
    "X_val_experienced = X_val[X_val['experience_flag']==1]\n",
    "y_val_experienced = y_val[y_val.index.isin(list(X_val_experienced.index))]\n",
    "X_test_experienced = X_test[X_test['experience_flag']==1]\n",
    "\n",
    "X_train_cold = X_train[X_train['experience_flag']==0]\n",
    "y_train_cold = y_train[y_train.index.isin(list(X_train_cold.index))]\n",
    "X_val_cold = X_val[X_val['experience_flag']==0]\n",
    "y_val_cold = y_val[y_val.index.isin(list(X_val_cold.index))]\n",
    "X_test_cold = X_test[X_test['experience_flag']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a911312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2063, 1539)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_experienced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fe4143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 1539)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_experienced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f062b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under = pd.read_csv('X_train_under.csv')\n",
    "y_train_under = pd.read_csv('y_train_under.csv')\n",
    "\n",
    "X_train_under.set_index('unique_id',inplace=True)\n",
    "y_train_under.set_index('unique_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f1ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_experienced_under = pd.read_csv('X_train_experienced_under.csv')\n",
    "y_train_experienced_under = pd.read_csv('y_train_experienced_under.csv')\n",
    "\n",
    "X_train_experienced_under.set_index('unique_id',inplace=True)\n",
    "y_train_experienced_under.set_index('unique_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbd5387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3104, 1539)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_experienced_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869b760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cold_under = pd.read_csv('X_train_cold_under.csv')\n",
    "y_train_cold_under = pd.read_csv('y_train_cold_under.csv')\n",
    "\n",
    "X_train_cold_under.set_index('unique_id',inplace=True)\n",
    "y_train_cold_under.set_index('unique_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8337015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>7982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>7990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>7993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>7998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id\n",
       "0             9\n",
       "1            18\n",
       "2            21\n",
       "3            25\n",
       "4            31\n",
       "...         ...\n",
       "2375       7982\n",
       "2376       7990\n",
       "2377       7993\n",
       "2378       7994\n",
       "2379       7998\n",
       "\n",
       "[2380 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_test_df = pd.read_csv('test_ids_in_prediction.csv')\n",
    "id_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e353eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd504cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sort_index(inplace=True)\n",
    "X_val.sort_index(inplace=True)\n",
    "X_test.sort_index(inplace=True)\n",
    "y_train.sort_index(inplace=True)\n",
    "y_val.sort_index(inplace=True)\n",
    "\n",
    "X_train_under.sort_index(inplace=True)\n",
    "y_train_under.sort_index(inplace=True)\n",
    "X_train_experienced_under.sort_index(inplace=True)\n",
    "y_train_experienced_under.sort_index(inplace=True)\n",
    "X_train_cold_under.sort_index(inplace=True)\n",
    "X_train_cold_under.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c168bb",
   "metadata": {},
   "source": [
    "### Submission 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e759a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(max_depth=2,learning_rate=0.01,n_estimators=100,min_child_samples=100,colsample_bytree=0.5)\n",
    "lgbm.fit(X_train_cold_under,y_train_cold_under)\n",
    "\n",
    "pred_train = pd.DataFrame(lgbm.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(lgbm.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(lgbm.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6666e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.8514203072108882\n",
      "Val ROC:  0.8444668833422739\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828e45b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.359959</td>\n",
       "      <td>0.104381</td>\n",
       "      <td>0.464341</td>\n",
       "      <td>0.38806</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.473331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.359959         0.104381        0.464341        0.38806   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.085271      0.473331  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "369d8ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.675,0.603,0.56,0.756,0.699,0.763,0.534,0.305,0.614,0.644,0.483,0.362,0.768,0.768,0.532,0.673,0.461,0.303,0.551,0.75,0.756,0.551,0.683,0.621,0.718,0.632,0.739,0.712,0.573,0.557,0.698,0.724,0.547,0.715,0.768,0.753,0.489,0.554,0.768,0.609,0.565,0.756,0.59,0.661,0.765,0.711,0.761,0.388,0.582,0.756,0.308,0.763,0.732,0.301,0.318,0.298,0.768,0.746,0.768,0.678,0.745,0.634,0.417,0.677,0.763,0.64,0.768,0.613,0.761,0.768,0.756,0.606,0.751,0.58,0.61,0.673,0.604,0.756,0.693,0.732,0.315,0.547,0.765,0.327,0.742,0.627,0.735,0.712,0.617,0.596,0.761,0.587,0.506,0.763,0.511,0.766,0.768,0.694,0.456,0.634,0.723,0.637,0.715,0.768,0.549,0.427,0.485,0.329,0.567,0.462,0.563,0.768,0.755,0.729,0.312,0.759,0.595,0.76,0.715,0.768,0.735,0.768,0.329,0.58,0.621,0.768,0.761,0.666,0.305,0.297,0.742,0.561,0.294,0.688,0.638,0.755,0.573,0.768,0.489,0.648,0.393,0.636,0.577,0.524,0.692,0.768,0.547,0.596,0.339,0.761,0.763,0.721,0.768,0.761,0.534,0.675,0.657,0.573,0.761,0.537,0.743,0.595,0.768,0.544,0.761,0.722,0.61,0.74,0.579,0.315,0.765,0.562,0.637,0.752,0.761,0.553,0.768,0.742,0.43,0.754,0.361,0.305,0.641,0.485,0.421,0.713,0.764,0.767,0.768,0.752,0.759,0.486,0.506,0.735,0.768,0.302,0.75,0.354,0.725,0.714,0.643,0.724,0.767,0.759,0.446,0.547,0.401,0.315,0.54,0.296,0.733,0.633,0.386,0.768,0.761,0.552,0.768,0.717,0.605,0.641,0.584,0.61,0.485,0.759,0.767,0.438,0.295,0.761,0.403,0.321,0.298,0.601,0.575,0.768,0.405,0.768,0.768,0.604,0.768,0.743,0.329,0.768,0.641,0.663,0.687,0.395,0.561,0.294,0.748,0.576,0.381,0.295,0.739,0.76,0.305,0.486,0.768,0.754,0.44,0.296,0.729,0.468,0.323,0.768,0.768,0.768,0.547,0.768,0.634,0.295,0.561,0.689,0.743,0.35,0.616,0.586,0.714,0.729,0.738,0.755,0.729,0.768,0.307,0.738,0.537,0.295,0.723,0.583,0.485,0.764,0.308,0.294,0.356,0.711,0.316,0.418,0.638,0.296,0.416,0.728,0.768,0.694,0.67,0.761,0.768,0.706,0.552,0.709,0.768,0.742,0.768,0.701,0.294,0.675,0.734,0.309,0.768,0.485,0.583,0.498,0.731,0.745,0.316,0.475,0.768,0.732,0.485,0.538,0.614,0.485,0.336,0.738,0.296,0.755,0.596,0.485,0.7,0.768,0.305,0.692,0.643,0.763,0.608,0.731,0.653,0.759,0.561,0.302,0.728,0.756,0.319,0.317,0.768,0.708,0.378,0.768,0.711,0.735,0.768,0.768,0.763,0.641,0.316,0.563,0.516,0.729,0.302,0.768,0.742,0.404,0.724,0.766,0.615,0.768,0.545,0.642,0.547,0.685,0.589,0.55,0.729,0.732,0.325,0.489,0.728,0.694,0.376,0.761,0.768,0.768,0.332,0.747,0.52,0.768,0.638,0.765,0.768,0.761,0.632,0.546,0.489,0.768,0.768,0.509,0.768,0.601,0.309,0.296,0.604,0.363,0.383,0.409,0.683,0.502,0.64,0.299,0.693,0.296,0.313,0.463,0.761,0.585,0.603,0.768,0.407,0.32,0.485,0.299,0.766,0.729,0.547,0.309,0.296,0.768,0.471,0.726,0.746,0.529,0.296,0.622,0.351,0.731,0.721,0.498,0.671,0.492,0.365,0.767,0.407,0.427,0.759,0.768,0.324,0.756,0.502,0.759,0.768,0.768,0.739,0.751,0.768,0.32,0.759,0.625,0.555,0.768,0.302,0.549,0.389,0.614,0.484,0.479,0.397,0.755,0.47,0.757,0.72,0.753,0.485,0.559,0.3,0.767,0.768,0.302,0.294,0.393,0.739,0.699,0.768,0.313,0.296,0.542,0.485,0.763,0.641,0.763,0.761,0.419,0.417,0.308,0.735,0.756,0.756,0.485,0.294,0.729,0.38,0.768,0.743,0.485,0.681,0.768,0.665,0.429,0.307,0.294,0.295,0.485,0.489,0.306,0.494,0.327,0.295,0.318,0.737,0.767,0.712,0.75,0.768,0.608,0.294,0.737,0.555,0.314,0.768,0.594,0.595,0.296,0.668,0.523,0.301,0.736,0.485,0.428,0.758,0.485,0.296,0.763,0.47,0.485,0.763,0.721,0.605,0.729,0.63,0.696,0.295,0.699,0.329,0.489,0.759,0.335,0.631,0.75,0.317,0.768,0.715,0.512,0.326,0.694,0.768,0.41,0.303,0.316,0.737,0.704,0.537,0.609,0.723,0.743,0.745,0.294,0.761,0.76,0.534,0.768,0.757,0.333,0.751,0.757,0.579,0.748,0.586,0.295,0.605,0.527,0.768,0.296,0.328,0.685,0.735,0.729,0.559,0.768,0.317,0.71,0.485,0.743,0.696,0.757,0.761,0.641,0.295,0.499,0.397,0.765,0.559,0.307,0.311,0.308,0.682,0.766,0.746,0.751,0.734,0.403,0.294,0.768,0.755,0.768,0.706,0.763,0.485,0.294,0.759,0.485,0.334,0.294,0.626,0.761,0.761,0.761,0.705,0.506,0.756,0.294,0.485,0.761,0.768,0.573,0.714,0.485,0.337,0.555,0.701,0.761,0.489,0.362,0.761,0.768,0.486,0.731,0.396,0.485,0.309,0.345,0.767,0.761,0.768,0.711,0.309,0.761,0.734,0.485,0.504,0.435,0.768,0.765,0.352,0.322,0.494,0.346,0.537,0.756,0.613,0.745,0.307,0.416,0.448,0.366,0.296,0.427,0.486,0.746,0.678,0.605,0.745,0.768,0.314,0.715,0.768,0.761,0.301,0.768,0.489,0.641,0.73,0.432,0.768,0.305,0.761,0.329,0.615,0.323,0.723,0.727,0.746,0.597,0.748,0.308,0.699,0.489,0.729,0.294,0.768,0.58,0.311,0.768,0.448,0.308,0.766,0.524,0.714,0.765,0.659,0.623,0.768,0.765,0.768,0.768,0.577,0.766,0.601,0.768,0.768,0.485,0.515,0.753,0.446,0.768,0.749,0.41,0.658,0.743,0.485,0.582,0.768,0.308,0.485,0.766,0.768,0.764,0.295,0.491,0.768,0.547,0.606,0.413,0.768,0.718,0.379,0.585,0.768,0.324,0.588,0.324,0.323,0.387,0.755,0.658,0.768,0.485,0.726,0.75,0.761,0.434,0.38,0.735,0.761,0.296,0.308,0.766,0.423,0.466,0.532,0.356,0.768,0.309,0.768,0.637,0.401,0.295,0.768,0.295,0.628,0.768,0.653,0.642,0.768,0.756,0.756,0.485,0.582,0.417,0.509,0.54,0.768,0.746,0.768,0.768,0.749,0.761,0.545,0.723,0.59,0.462,0.703,0.561,0.485,0.768,0.766,0.761,0.689,0.397,0.326,0.64,0.369,0.362,0.639,0.485,0.768,0.48,0.751,0.734,0.698,0.641,0.714,0.607,0.589,0.616,0.486,0.75,0.505,0.354,0.485,0.767,0.658,0.319,0.768,0.698,0.744,0.768,0.485,0.763,0.35,0.354,0.49,0.743,0.745,0.299,0.768,0.562,0.768,0.426,0.562,0.768,0.682,0.768,0.296,0.337,0.517,0.37,0.704,0.47,0.426,0.412,0.768,0.485,0.417,0.337,0.768,0.33,0.324,0.331,0.768,0.768,0.745,0.442,0.337,0.486,0.312,0.629,0.334,0.317,0.353,0.687,0.309,0.302,0.534,0.514,0.364,0.531,0.768,0.768,0.486,0.713,0.768,0.745,0.43,0.768,0.643,0.374,0.766,0.564,0.686,0.303,0.765,0.485,0.575,0.3,0.712,0.768,0.768,0.72,0.32,0.768,0.725,0.294,0.737,0.508,0.761,0.353,0.57,0.768,0.737,0.768,0.626,0.768,0.768,0.296,0.767,0.555,0.294,0.768,0.503,0.768,0.629,0.755,0.766,0.32,0.768,0.508,0.43,0.53,0.728,0.731,0.766,0.308,0.768,0.428,0.423,0.324,0.305,0.768,0.729,0.632,0.653,0.759,0.728,0.32,0.768,0.329,0.729,0.325,0.616,0.31,0.768,0.485,0.64,0.761,0.765,0.768,0.601,0.768,0.317,0.709,0.733,0.639,0.601,0.583,0.768,0.485,0.466,0.761,0.765,0.629,0.42,0.329,0.73,0.639,0.361,0.755,0.674,0.759,0.706,0.3,0.355,0.765,0.465,0.768,0.41,0.756,0.485,0.739,0.294,0.485,0.485,0.354,0.485,0.759,0.766,0.768,0.722,0.485,0.698,0.485,0.448,0.294,0.525,0.546,0.304,0.326,0.345,0.418,0.37,0.311,0.763,0.768,0.485,0.768,0.755,0.333,0.59,0.638,0.628,0.767,0.748,0.39,0.494,0.608,0.392,0.485,0.768,0.768,0.603,0.768,0.679,0.71,0.763,0.425,0.596,0.322,0.699,0.765,0.407,0.768,0.557,0.768,0.48,0.766,0.349,0.485,0.485,0.46,0.485,0.568,0.485,0.673,0.313,0.456,0.477,0.324,0.58,0.294,0.485,0.556,0.637,0.763,0.537,0.768,0.599,0.301,0.316,0.489,0.692,0.3,0.466,0.442,0.326,0.301,0.337,0.677,0.756,0.489,0.462,0.474,0.307,0.73,0.533,0.548,0.298,0.768,0.378,0.465,0.489,0.59,0.328,0.456,0.758,0.32,0.317,0.687,0.294,0.748,0.485,0.489,0.7,0.485,0.768,0.294,0.485,0.734,0.485,0.485,0.744,0.324,0.485,0.294,0.604,0.603,0.489,0.3,0.572,0.719,0.761,0.524,0.698,0.767,0.319,0.302,0.323,0.489,0.768,0.73,0.666,0.687,0.486,0.765,0.546,0.454,0.486,0.768,0.294,0.686,0.369,0.766,0.485,0.489,0.294,0.727,0.512,0.469,0.334,0.302,0.642,0.403,0.766,0.308,0.736,0.294,0.295,0.763,0.295,0.294,0.421,0.326,0.53,0.474,0.479,0.486,0.486,0.502,0.595,0.328,0.745,0.316,0.489,0.712,0.485,0.768,0.73,0.485,0.768,0.614,0.747,0.5,0.729,0.523,0.538,0.485,0.554,0.768,0.602,0.319,0.301,0.309,0.48,0.365,0.577,0.485,0.383,0.308,0.696,0.46,0.634,0.761,0.485,0.768,0.485,0.485,0.295,0.336,0.743,0.556,0.3,0.587,0.718,0.722,0.768,0.699,0.489,0.768,0.766,0.308,0.758,0.49,0.761,0.485,0.486,0.485,0.766,0.486,0.759,0.415,0.315,0.317,0.441,0.768,0.766,0.328,0.585,0.294,0.318,0.755,0.744,0.559,0.316,0.667,0.485,0.768,0.768,0.32,0.645,0.677,0.324,0.582,0.588,0.329,0.638,0.489,0.485,0.485,0.337,0.323,0.296,0.486,0.404,0.583,0.723,0.741,0.768,0.329,0.299,0.615,0.361,0.768,0.327,0.758,0.308,0.528,0.341,0.72,0.324,0.301,0.629,0.485,0.636,0.765,0.311,0.303,0.484,0.675,0.768,0.431,0.748,0.326,0.32,0.485,0.485,0.756,0.322,0.756,0.621,0.485,0.337,0.765,0.329,0.328,0.485,0.321,0.296,0.768,0.42,0.486,0.317,0.486,0.59,0.768,0.736,0.332,0.722,0.485,0.348,0.486,0.442,0.333,0.427,0.646,0.479,0.609,0.768,0.485,0.392,0.485,0.667,0.373,0.294,0.755,0.295,0.343,0.572,0.592,0.485,0.543,0.638,0.485,0.485,0.324,0.485,0.485,0.33,0.443,0.34,0.473,0.485,0.765,0.337,0.485,0.294,0.485,0.348,0.766,0.329,0.485,0.485,0.368,0.485,0.427,0.485,0.318,0.486,0.485,0.372,0.763,0.402,0.435,0.354,0.485,0.485,0.746,0.345,0.486,0.333,0.485,0.485,0.3,0.452,0.301,0.485,0.679,0.294,0.485,0.339,0.64,0.636,0.485,0.638,0.543,0.485,0.318,0.368,0.485,0.318,0.526,0.768,0.486,0.485,0.478,0.328,0.328,0.659,0.53,0.479,0.317,0.485,0.766,0.311,0.295,0.314,0.485,0.588,0.324,0.399,0.489,0.559,0.333,0.462,0.583,0.322,0.738,0.634,0.485,0.334,0.763,0.301,0.497,0.324,0.767,0.631,0.496,0.485,0.547,0.485,0.58,0.736,0.495,0.497,0.766,0.294,0.339,0.632,0.755,0.768,0.757,0.445,0.679,0.761,0.467,0.756,0.366,0.489,0.296,0.72,0.722,0.434,0.367,0.569,0.768,0.421,0.557,0.442,0.558,0.768,0.768,0.486,0.768,0.47,0.689,0.543,0.632,0.46,0.451,0.31,0.753,0.316,0.489,0.3,0.703,0.341,0.502,0.606,0.379,0.768,0.625,0.485,0.768,0.765,0.514,0.364,0.749,0.344,0.591,0.766,0.295,0.751,0.3,0.294,0.723,0.314,0.69,0.768,0.5,0.58,0.717,0.294,0.303,0.295,0.552,0.302,0.294,0.3,0.732,0.305,0.36,0.768,0.308,0.59,0.688,0.768,0.738,0.317,0.302,0.758,0.294,0.489,0.53,0.768,0.615,0.313,0.381,0.621,0.758,0.698,0.491,0.768,0.294,0.636,0.295,0.353,0.761,0.408,0.765,0.3,0.359,0.766,0.294,0.673,0.316,0.693,0.736,0.766,0.607,0.294,0.768,0.294,0.755,0.55,0.443,0.627,0.752,0.768,0.497,0.489,0.725,0.308,0.301,0.323,0.294,0.458,0.324,0.315,0.768,0.706,0.302,0.69,0.759,0.325,0.324,0.59,0.314,0.669,0.428,0.589,0.328,0.767,0.296,0.438,0.324,0.404,0.495,0.313,0.306,0.324,0.308,0.748,0.685,0.739,0.697,0.768,0.316,0.489,0.753,0.324,0.312,0.309,0.765,0.687,0.675,0.574,0.766,0.569,0.634,0.768,0.579,0.309,0.675,0.722,0.486,0.693,0.294,0.305,0.75,0.583,0.378,0.669,0.413,0.768,0.768,0.768,0.585,0.638,0.766,0.678,0.768,0.486,0.588,0.431,0.372,0.577,0.374,0.586,0.295,0.76,0.766,0.321,0.309,0.638,0.316,0.765,0.588,0.638,0.463,0.636,0.336,0.294,0.723,0.759,0.37,0.766,0.296,0.309,0.621,0.326,0.485,0.763,0.638,0.354,0.722,0.719,0.65,0.486,0.763,0.294,0.53,0.675,0.598,0.764,0.583,0.486,0.499,0.766,0.59,0.489,0.42,0.397,0.332,0.336,0.296,0.308,0.388,0.766,0.59,0.294,0.751,0.505,0.294,0.611,0.588,0.437,0.323,0.294,0.294,0.389,0.675,0.761,0.301,0.76,0.377,0.429,0.351,0.373,0.296,0.353,0.675,0.44,0.389,0.59,0.723,0.294,0.348,0.335,0.59,0.31,0.329,0.541,0.629,0.575,0.308,0.311,0.309,0.317,0.764,0.669,0.332,0.329,0.638,0.638,0.656,0.307,0.486,0.441,0.638,0.588,0.72,0.59,0.505,0.486,0.53,0.621,0.566,0.485,0.588,0.64,0.486,0.638,0.332,0.297,0.308,0.308,0.629,0.396,0.317,0.475,0.353,0.646,0.332,0.326,0.487,0.3,0.307,0.577,0.363,0.562,0.363,0.429,0.485,0.323,0.327,0.486,0.485,0.485,0.326,0.485,0.485,0.433,0.577,0.485,0.485,0.485,0.31,0.379,0.485,0.48,0.308,0.485,0.394,0.625,0.485,0.486,0.301,0.489,0.485,0.596,0.485,0.485,0.485,0.485,0.486,0.294,0.485,0.486,0.489,0.326,0.485,0.349,0.42,0.485,0.485,0.485,0.485,0.433,0.379,0.376,0.336,0.324,0.538,0.5,0.485,0.485,0.485,0.485,0.485,0.316,0.379,0.324,0.489,0.339,0.308,0.486,0.486,0.489,0.588,0.72,0.486,0.485,0.506,0.485,0.489,0.485,0.483,0.572,0.489,0.489,0.486,0.486,0.563,0.38,0.489,0.486,0.405,0.362,0.371,0.485,0.485,0.485,0.53,0.5,0.458,0.326,0.588,0.59,0.708,0.64,0.489,0.642,0.53,0.328,0.485,0.441,0.378,0.486,0.485,0.477,0.485,0.551,0.489,0.306,0.486,0.34,0.489,0.486,0.485,0.351,0.6,0.327,0.485,0.326,0.485,0.503,0.329,0.316,0.3,0.489,0.316,0.503,0.326,0.316,0.485,0.32,0.638,0.324,0.327,0.489,0.36,0.301,0.47,0.486,0.328,0.486,0.603,0.486,0.536,0.638,0.638,0.638,0.486,0.317,0.486,0.51,0.485,0.316,0.316,0.317,0.353,0.508,0.294,0.337,0.329,0.317,0.489,0.489,0.353,0.379,0.347,0.554,0.485,0.378,0.331,0.485,0.485,0.401,0.485,0.603,0.485,0.362,0.485,0.485,0.485,0.485,0.53,0.486,0.469,0.485,0.51,0.47,0.487,0.485,0.485,0.485,0.486,0.485,0.312,0.485,0.485,0.485,0.485,0.489,0.59,0.486,0.485,0.489,0.489,0.486,0.489,0.485,0.485,0.485,0.353,0.387,0.489,0.487,0.485,0.485,0.333,0.485,0.485,0.554,0.536,0.485,0.485,0.33,0.474,0.485,0.485,0.486,0.502,0.489,0.485,0.486,0.397,0.321,0.485,0.486,0.485,0.485,0.485,0.485,0.485,0.486,0.485,0.485,0.485,0.485,0.485,0.353,0.336,0.485,0.489,0.485,0.489,0.486,0.324,0.471,0.489,0.489,0.485,0.328,0.485,0.485,0.485,0.486,0.316,0.485,0.485,0.328,0.485,0.485,0.485,0.332,0.485,0.361,0.485,0.606,0.485,0.485,0.59,0.485,0.485,0.593,0.485,0.588,0.315,0.31,0.485,0.363,0.563,0.486,0.353,0.324,0.569,0.486,0.485,0.485,0.506,0.337,0.474,0.485,0.485,0.378,0.486,0.337,0.485,0.302,0.294,0.309,0.342,0.485,0.59,0.301,0.487,0.338,0.638,0.315,0.638,0.489,0.324,0.486,0.328,0.429,0.437,0.486,0.485,0.378,0.485,0.485,0.485,0.485,0.486,0.485,0.485,0.354,0.442,0.485,0.379,0.339,0.622,0.412,0.485,0.486,0.485,0.503,0.549,0.327,0.441,0.3,0.415,0.638,0.485,0.362,0.485,0.329,0.638,0.486,0.485,0.485,0.415,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.44,0.486,0.489,0.485,0.486,0.569,0.415,0.485,0.485,0.485,0.486,0.43,0.538,0.485,0.485,0.569,0.485,0.485,0.485,0.485,0.326,0.485,0.485,0.485,0.485,0.485,0.485,0.486,0.438,0.309,0.353,0.485,0.486,0.485,0.485,0.485,0.596,0.324,0.485,0.365,0.588,0.422,0.486,0.486,0.486,0.524,0.58,0.486,0.588,0.363,0.405,0.363,0.485,0.485,0.485,0.485,0.485,0.485,0.483,0.311,0.441,0.486,0.485,0.334,0.337,0.485,0.485,0.328,0.485,0.451,0.485,0.485,0.485,0.489,0.486,0.317,0.485,0.485,0.485,0.485,0.485,0.485,0.486,0.486,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.489,0.378,0.436,0.391,0.505,0.485,0.485,0.485,0.379,0.451,0.485,0.485,0.485,0.486,0.485,0.588,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.324,0.309,0.563,0.405,0.325,0.485,0.485,0.485,0.485,0.486,0.324,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.588,0.337,0.391,0.588,0.474,0.485,0.485,0.353,0.486,0.486,0.485,0.485,0.486,0.485,0.485,0.485,0.485,0.485,0.362,0.337,0.339,0.474,0.485,0.437,0.339,0.486,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.337,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.58,0.485,0.498,0.485,0.485,0.485,0.415,0.485,0.486,0.407,0.486,0.337,0.485,0.485,0.485'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub11 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub11['pred'] = sub11['pred'].apply(lambda x: round(x,3))\n",
    "sub11_txt = ''\n",
    "for prob in list(sub11['pred'].values):\n",
    "    sub11_txt = sub11_txt+','+str(prob)\n",
    "sub11_txt = sub11_txt[1:]\n",
    "\n",
    "sub11_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a03b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub11.to_csv('sub11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a10d87",
   "metadata": {},
   "source": [
    "### Submission 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79a6d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:03:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:03:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=2,learning_rate=0.01,n_estimators=100,min_child_samples=20,colsample_bytree=1)\n",
    "xgb.fit(X_train_cold_under,y_train_cold_under)\n",
    "\n",
    "pred_train = pd.DataFrame(xgb.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(xgb.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(xgb.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982bfc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.8570074927989235\n",
      "Val ROC:  0.8496138055318896\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3b55e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.359959</td>\n",
       "      <td>0.097938</td>\n",
       "      <td>0.457897</td>\n",
       "      <td>0.382632</td>\n",
       "      <td>0.080103</td>\n",
       "      <td>0.462736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.359959         0.097938        0.457897       0.382632   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.080103      0.462736  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a5c4251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.677,0.618,0.581,0.736,0.711,0.765,0.515,0.3,0.645,0.613,0.459,0.354,0.763,0.765,0.577,0.688,0.487,0.3,0.595,0.748,0.758,0.549,0.686,0.636,0.738,0.628,0.733,0.728,0.58,0.581,0.714,0.731,0.577,0.73,0.765,0.744,0.494,0.581,0.765,0.609,0.577,0.76,0.59,0.692,0.75,0.685,0.758,0.419,0.586,0.742,0.306,0.765,0.724,0.308,0.308,0.291,0.765,0.746,0.765,0.688,0.74,0.628,0.408,0.684,0.765,0.634,0.763,0.628,0.762,0.765,0.762,0.609,0.744,0.558,0.618,0.675,0.626,0.763,0.68,0.731,0.311,0.577,0.752,0.325,0.746,0.645,0.741,0.722,0.611,0.576,0.765,0.566,0.503,0.763,0.515,0.765,0.763,0.707,0.461,0.628,0.689,0.626,0.711,0.765,0.587,0.433,0.483,0.341,0.57,0.449,0.587,0.765,0.756,0.72,0.323,0.754,0.602,0.762,0.712,0.765,0.726,0.765,0.339,0.582,0.598,0.765,0.763,0.686,0.3,0.293,0.735,0.574,0.29,0.677,0.618,0.754,0.585,0.765,0.494,0.61,0.375,0.631,0.556,0.515,0.702,0.765,0.526,0.561,0.347,0.758,0.752,0.726,0.765,0.756,0.532,0.675,0.649,0.614,0.756,0.515,0.733,0.607,0.763,0.552,0.756,0.74,0.638,0.747,0.546,0.325,0.76,0.579,0.671,0.754,0.758,0.572,0.765,0.742,0.46,0.741,0.388,0.3,0.636,0.478,0.451,0.73,0.763,0.765,0.765,0.756,0.765,0.504,0.459,0.741,0.765,0.309,0.735,0.371,0.704,0.681,0.663,0.733,0.765,0.758,0.462,0.587,0.395,0.316,0.505,0.294,0.713,0.647,0.393,0.765,0.756,0.548,0.765,0.714,0.622,0.64,0.591,0.625,0.478,0.758,0.763,0.44,0.291,0.758,0.404,0.327,0.291,0.619,0.591,0.765,0.4,0.765,0.763,0.625,0.765,0.74,0.324,0.765,0.628,0.639,0.697,0.38,0.597,0.29,0.733,0.56,0.387,0.295,0.738,0.758,0.3,0.482,0.765,0.756,0.413,0.294,0.74,0.436,0.327,0.765,0.763,0.765,0.553,0.765,0.627,0.291,0.584,0.684,0.72,0.361,0.636,0.59,0.724,0.738,0.737,0.756,0.74,0.765,0.3,0.737,0.567,0.291,0.74,0.605,0.483,0.765,0.304,0.29,0.334,0.71,0.325,0.434,0.647,0.294,0.414,0.688,0.765,0.681,0.679,0.756,0.765,0.707,0.579,0.677,0.765,0.7,0.765,0.71,0.29,0.666,0.731,0.31,0.765,0.478,0.624,0.499,0.731,0.739,0.325,0.431,0.765,0.725,0.478,0.515,0.617,0.483,0.327,0.733,0.299,0.758,0.579,0.483,0.713,0.765,0.3,0.703,0.608,0.765,0.611,0.698,0.667,0.752,0.58,0.293,0.724,0.758,0.324,0.329,0.765,0.721,0.386,0.765,0.722,0.718,0.765,0.765,0.763,0.64,0.327,0.596,0.551,0.74,0.293,0.765,0.747,0.38,0.739,0.76,0.599,0.765,0.548,0.647,0.577,0.686,0.602,0.561,0.738,0.746,0.326,0.494,0.74,0.691,0.365,0.756,0.765,0.765,0.327,0.748,0.521,0.763,0.633,0.763,0.765,0.763,0.628,0.577,0.494,0.765,0.765,0.535,0.765,0.597,0.315,0.294,0.61,0.361,0.372,0.407,0.686,0.504,0.64,0.299,0.675,0.294,0.311,0.457,0.762,0.528,0.592,0.765,0.398,0.335,0.478,0.291,0.76,0.696,0.536,0.306,0.294,0.765,0.469,0.73,0.748,0.556,0.294,0.628,0.351,0.736,0.721,0.47,0.635,0.516,0.358,0.765,0.438,0.439,0.756,0.765,0.327,0.756,0.536,0.754,0.765,0.765,0.733,0.758,0.765,0.337,0.765,0.621,0.526,0.765,0.293,0.526,0.388,0.621,0.486,0.468,0.377,0.752,0.488,0.76,0.706,0.748,0.478,0.536,0.308,0.765,0.765,0.313,0.29,0.381,0.727,0.707,0.765,0.316,0.294,0.526,0.478,0.763,0.648,0.765,0.763,0.406,0.404,0.309,0.747,0.756,0.762,0.485,0.29,0.74,0.372,0.765,0.72,0.483,0.69,0.765,0.661,0.44,0.3,0.291,0.291,0.485,0.494,0.308,0.491,0.326,0.29,0.313,0.731,0.765,0.712,0.75,0.765,0.626,0.29,0.74,0.536,0.319,0.763,0.598,0.592,0.294,0.696,0.503,0.309,0.738,0.478,0.459,0.763,0.478,0.294,0.746,0.483,0.478,0.765,0.739,0.618,0.74,0.662,0.647,0.291,0.703,0.327,0.494,0.758,0.327,0.63,0.75,0.325,0.765,0.706,0.556,0.328,0.709,0.763,0.392,0.3,0.319,0.74,0.68,0.515,0.634,0.74,0.739,0.74,0.291,0.758,0.763,0.525,0.765,0.758,0.341,0.75,0.758,0.618,0.754,0.611,0.294,0.615,0.501,0.765,0.294,0.327,0.71,0.732,0.738,0.595,0.765,0.309,0.694,0.483,0.74,0.706,0.758,0.758,0.628,0.29,0.506,0.395,0.752,0.581,0.3,0.314,0.308,0.653,0.76,0.748,0.749,0.726,0.397,0.295,0.765,0.748,0.763,0.703,0.742,0.478,0.29,0.758,0.483,0.331,0.29,0.635,0.756,0.763,0.756,0.715,0.492,0.758,0.291,0.478,0.758,0.765,0.587,0.733,0.478,0.347,0.526,0.685,0.758,0.494,0.354,0.76,0.765,0.478,0.741,0.392,0.478,0.304,0.35,0.752,0.763,0.765,0.71,0.31,0.758,0.719,0.478,0.506,0.477,0.765,0.752,0.381,0.316,0.492,0.347,0.501,0.756,0.628,0.75,0.3,0.429,0.471,0.37,0.294,0.403,0.478,0.754,0.7,0.614,0.727,0.765,0.329,0.718,0.765,0.756,0.308,0.765,0.494,0.647,0.716,0.424,0.765,0.299,0.752,0.327,0.628,0.327,0.703,0.715,0.754,0.595,0.752,0.304,0.714,0.494,0.74,0.29,0.765,0.6,0.311,0.765,0.451,0.306,0.76,0.549,0.679,0.756,0.669,0.617,0.765,0.752,0.765,0.765,0.598,0.76,0.619,0.765,0.765,0.478,0.497,0.748,0.427,0.765,0.756,0.385,0.659,0.74,0.478,0.598,0.765,0.3,0.478,0.76,0.765,0.765,0.291,0.513,0.765,0.577,0.631,0.422,0.761,0.672,0.377,0.597,0.765,0.327,0.596,0.327,0.323,0.391,0.756,0.599,0.765,0.478,0.726,0.718,0.762,0.427,0.383,0.716,0.762,0.294,0.308,0.76,0.419,0.471,0.506,0.341,0.763,0.304,0.765,0.643,0.403,0.291,0.765,0.294,0.623,0.765,0.628,0.633,0.761,0.758,0.758,0.478,0.594,0.427,0.519,0.577,0.765,0.756,0.765,0.765,0.75,0.756,0.592,0.703,0.59,0.464,0.678,0.543,0.485,0.765,0.76,0.756,0.685,0.41,0.327,0.64,0.389,0.354,0.616,0.483,0.765,0.474,0.743,0.737,0.669,0.647,0.729,0.618,0.602,0.661,0.482,0.748,0.504,0.374,0.478,0.765,0.634,0.33,0.765,0.686,0.754,0.765,0.485,0.765,0.372,0.351,0.479,0.74,0.739,0.291,0.765,0.56,0.765,0.423,0.548,0.765,0.687,0.765,0.294,0.327,0.485,0.376,0.684,0.469,0.425,0.377,0.765,0.48,0.426,0.321,0.765,0.326,0.327,0.334,0.765,0.765,0.726,0.439,0.327,0.5,0.309,0.645,0.346,0.332,0.366,0.684,0.306,0.308,0.512,0.561,0.367,0.495,0.765,0.765,0.485,0.708,0.765,0.74,0.439,0.765,0.663,0.375,0.76,0.591,0.681,0.317,0.765,0.483,0.567,0.308,0.708,0.765,0.765,0.688,0.337,0.763,0.685,0.291,0.734,0.527,0.756,0.356,0.571,0.761,0.74,0.765,0.617,0.765,0.765,0.294,0.765,0.526,0.29,0.765,0.52,0.765,0.631,0.758,0.76,0.335,0.765,0.504,0.448,0.538,0.724,0.737,0.756,0.306,0.763,0.464,0.398,0.331,0.299,0.765,0.696,0.64,0.644,0.752,0.724,0.335,0.765,0.341,0.725,0.337,0.604,0.314,0.765,0.48,0.64,0.756,0.752,0.76,0.594,0.765,0.325,0.647,0.732,0.583,0.605,0.588,0.763,0.485,0.456,0.758,0.756,0.64,0.411,0.321,0.73,0.636,0.345,0.756,0.679,0.765,0.709,0.312,0.356,0.752,0.448,0.765,0.4,0.762,0.483,0.733,0.29,0.483,0.483,0.374,0.478,0.746,0.76,0.763,0.723,0.483,0.686,0.483,0.488,0.29,0.539,0.515,0.297,0.331,0.346,0.418,0.365,0.32,0.763,0.765,0.487,0.765,0.758,0.326,0.588,0.628,0.616,0.765,0.754,0.386,0.504,0.608,0.427,0.478,0.765,0.765,0.575,0.765,0.659,0.68,0.76,0.425,0.616,0.319,0.707,0.752,0.4,0.765,0.569,0.765,0.456,0.76,0.368,0.483,0.478,0.448,0.485,0.569,0.478,0.657,0.311,0.457,0.478,0.335,0.571,0.29,0.478,0.543,0.628,0.765,0.567,0.765,0.619,0.308,0.329,0.494,0.684,0.304,0.452,0.455,0.327,0.313,0.331,0.67,0.756,0.494,0.425,0.483,0.3,0.73,0.552,0.584,0.301,0.765,0.399,0.448,0.494,0.59,0.331,0.462,0.76,0.353,0.314,0.684,0.29,0.754,0.478,0.494,0.721,0.483,0.765,0.29,0.478,0.727,0.483,0.483,0.731,0.328,0.478,0.291,0.626,0.6,0.494,0.313,0.583,0.747,0.762,0.517,0.696,0.765,0.335,0.309,0.327,0.494,0.765,0.738,0.664,0.684,0.478,0.765,0.555,0.445,0.478,0.765,0.295,0.686,0.389,0.76,0.478,0.494,0.29,0.717,0.54,0.483,0.346,0.293,0.647,0.421,0.76,0.304,0.747,0.291,0.291,0.76,0.29,0.293,0.423,0.346,0.538,0.478,0.469,0.478,0.478,0.504,0.596,0.336,0.739,0.325,0.494,0.71,0.483,0.765,0.73,0.483,0.765,0.626,0.724,0.504,0.74,0.52,0.515,0.482,0.558,0.763,0.596,0.33,0.308,0.302,0.472,0.365,0.569,0.485,0.386,0.304,0.703,0.462,0.641,0.76,0.478,0.765,0.483,0.478,0.29,0.335,0.75,0.536,0.308,0.581,0.714,0.737,0.765,0.658,0.494,0.765,0.76,0.315,0.746,0.47,0.756,0.478,0.482,0.48,0.76,0.493,0.758,0.405,0.324,0.308,0.452,0.765,0.76,0.331,0.566,0.291,0.329,0.758,0.756,0.561,0.323,0.675,0.483,0.765,0.765,0.308,0.684,0.675,0.331,0.586,0.578,0.348,0.635,0.494,0.478,0.478,0.331,0.327,0.294,0.482,0.389,0.608,0.717,0.738,0.765,0.327,0.299,0.616,0.376,0.765,0.325,0.75,0.304,0.556,0.341,0.738,0.327,0.29,0.644,0.478,0.647,0.765,0.315,0.312,0.464,0.684,0.765,0.458,0.73,0.327,0.335,0.478,0.483,0.758,0.312,0.758,0.635,0.478,0.334,0.76,0.341,0.336,0.478,0.317,0.294,0.765,0.438,0.483,0.323,0.483,0.59,0.765,0.719,0.344,0.727,0.48,0.352,0.483,0.462,0.327,0.448,0.647,0.458,0.595,0.765,0.48,0.413,0.478,0.661,0.402,0.291,0.756,0.29,0.352,0.583,0.545,0.483,0.549,0.635,0.48,0.483,0.33,0.485,0.483,0.335,0.45,0.362,0.438,0.478,0.76,0.327,0.483,0.291,0.485,0.356,0.758,0.341,0.478,0.483,0.356,0.483,0.439,0.482,0.308,0.483,0.478,0.374,0.765,0.396,0.439,0.381,0.48,0.483,0.754,0.346,0.487,0.341,0.478,0.48,0.303,0.466,0.308,0.48,0.711,0.295,0.478,0.327,0.64,0.635,0.478,0.635,0.549,0.478,0.308,0.371,0.478,0.321,0.541,0.763,0.478,0.485,0.47,0.331,0.336,0.643,0.538,0.487,0.323,0.478,0.756,0.315,0.295,0.307,0.478,0.578,0.331,0.405,0.494,0.583,0.329,0.446,0.587,0.316,0.737,0.647,0.478,0.34,0.746,0.308,0.506,0.326,0.765,0.632,0.479,0.478,0.565,0.483,0.579,0.727,0.496,0.519,0.76,0.295,0.337,0.64,0.742,0.763,0.75,0.456,0.679,0.758,0.437,0.76,0.381,0.494,0.294,0.717,0.724,0.437,0.352,0.554,0.765,0.459,0.56,0.429,0.59,0.765,0.765,0.478,0.765,0.494,0.684,0.549,0.64,0.452,0.453,0.309,0.75,0.325,0.494,0.3,0.708,0.341,0.504,0.629,0.398,0.765,0.599,0.483,0.765,0.752,0.523,0.38,0.754,0.352,0.605,0.76,0.291,0.75,0.308,0.293,0.708,0.327,0.664,0.765,0.504,0.6,0.717,0.291,0.323,0.294,0.548,0.313,0.293,0.308,0.737,0.3,0.354,0.765,0.306,0.585,0.668,0.765,0.737,0.329,0.297,0.75,0.29,0.504,0.538,0.765,0.628,0.326,0.376,0.598,0.763,0.658,0.518,0.765,0.293,0.628,0.293,0.334,0.756,0.405,0.752,0.312,0.359,0.76,0.29,0.657,0.329,0.684,0.731,0.76,0.605,0.295,0.765,0.291,0.758,0.576,0.44,0.619,0.741,0.765,0.506,0.494,0.729,0.308,0.312,0.331,0.291,0.463,0.327,0.325,0.765,0.703,0.313,0.693,0.758,0.351,0.316,0.597,0.327,0.682,0.422,0.602,0.327,0.763,0.294,0.444,0.327,0.401,0.496,0.318,0.304,0.331,0.308,0.732,0.684,0.745,0.704,0.765,0.327,0.494,0.748,0.331,0.319,0.309,0.752,0.684,0.67,0.583,0.756,0.59,0.638,0.765,0.575,0.309,0.675,0.726,0.487,0.684,0.29,0.299,0.75,0.587,0.376,0.682,0.439,0.765,0.763,0.765,0.614,0.635,0.76,0.678,0.765,0.487,0.579,0.409,0.372,0.579,0.38,0.587,0.29,0.76,0.76,0.331,0.31,0.635,0.327,0.756,0.579,0.635,0.472,0.64,0.335,0.293,0.717,0.752,0.36,0.76,0.294,0.306,0.617,0.331,0.478,0.76,0.635,0.354,0.721,0.703,0.644,0.482,0.76,0.293,0.538,0.675,0.587,0.754,0.588,0.487,0.505,0.76,0.59,0.494,0.427,0.396,0.342,0.342,0.294,0.304,0.381,0.76,0.59,0.29,0.739,0.504,0.29,0.626,0.579,0.423,0.328,0.293,0.293,0.374,0.684,0.76,0.294,0.756,0.405,0.424,0.365,0.373,0.294,0.356,0.675,0.428,0.366,0.59,0.717,0.29,0.356,0.328,0.588,0.315,0.34,0.544,0.633,0.585,0.308,0.313,0.31,0.313,0.754,0.678,0.347,0.341,0.635,0.635,0.643,0.303,0.483,0.452,0.635,0.583,0.717,0.59,0.504,0.478,0.538,0.635,0.572,0.485,0.583,0.626,0.487,0.635,0.348,0.299,0.304,0.308,0.632,0.368,0.327,0.494,0.366,0.676,0.312,0.331,0.494,0.308,0.306,0.586,0.362,0.548,0.362,0.424,0.483,0.331,0.33,0.483,0.485,0.485,0.327,0.483,0.485,0.452,0.587,0.483,0.485,0.485,0.297,0.389,0.485,0.472,0.304,0.48,0.38,0.632,0.478,0.482,0.308,0.494,0.483,0.584,0.483,0.485,0.483,0.485,0.478,0.29,0.478,0.483,0.494,0.331,0.483,0.37,0.403,0.478,0.483,0.483,0.483,0.471,0.391,0.385,0.331,0.327,0.527,0.504,0.485,0.483,0.483,0.485,0.485,0.327,0.391,0.331,0.494,0.335,0.315,0.482,0.487,0.494,0.578,0.724,0.487,0.485,0.498,0.483,0.494,0.483,0.469,0.583,0.494,0.494,0.482,0.483,0.561,0.395,0.494,0.483,0.406,0.381,0.366,0.483,0.478,0.478,0.538,0.504,0.478,0.331,0.579,0.588,0.721,0.64,0.494,0.647,0.538,0.327,0.485,0.45,0.391,0.487,0.478,0.483,0.485,0.549,0.494,0.297,0.485,0.364,0.494,0.487,0.478,0.376,0.611,0.337,0.483,0.331,0.483,0.504,0.339,0.327,0.312,0.494,0.327,0.504,0.327,0.323,0.483,0.312,0.64,0.328,0.316,0.494,0.356,0.312,0.458,0.483,0.331,0.483,0.6,0.487,0.532,0.635,0.635,0.635,0.485,0.308,0.478,0.504,0.483,0.329,0.327,0.312,0.356,0.504,0.293,0.327,0.327,0.328,0.494,0.494,0.37,0.389,0.356,0.572,0.483,0.391,0.331,0.483,0.483,0.401,0.478,0.6,0.487,0.361,0.483,0.483,0.483,0.478,0.538,0.478,0.483,0.485,0.504,0.464,0.494,0.483,0.478,0.483,0.48,0.478,0.306,0.483,0.478,0.48,0.487,0.494,0.59,0.487,0.483,0.494,0.494,0.483,0.494,0.483,0.478,0.478,0.366,0.399,0.494,0.494,0.483,0.478,0.331,0.478,0.478,0.56,0.542,0.48,0.478,0.335,0.478,0.478,0.478,0.482,0.504,0.494,0.485,0.487,0.388,0.315,0.485,0.483,0.478,0.48,0.478,0.478,0.478,0.478,0.483,0.478,0.478,0.478,0.478,0.366,0.327,0.478,0.494,0.48,0.494,0.487,0.327,0.476,0.494,0.494,0.483,0.341,0.478,0.483,0.478,0.478,0.321,0.483,0.48,0.327,0.483,0.483,0.483,0.331,0.483,0.376,0.483,0.59,0.478,0.478,0.591,0.48,0.478,0.585,0.485,0.579,0.323,0.32,0.478,0.38,0.56,0.485,0.366,0.327,0.574,0.482,0.483,0.485,0.492,0.331,0.478,0.48,0.478,0.378,0.487,0.331,0.483,0.297,0.293,0.315,0.346,0.483,0.59,0.312,0.494,0.354,0.635,0.321,0.635,0.494,0.328,0.485,0.335,0.424,0.445,0.483,0.485,0.38,0.485,0.485,0.485,0.485,0.483,0.483,0.483,0.372,0.429,0.483,0.385,0.331,0.629,0.422,0.483,0.487,0.482,0.504,0.546,0.332,0.452,0.312,0.427,0.635,0.485,0.349,0.482,0.327,0.635,0.478,0.483,0.478,0.427,0.485,0.478,0.478,0.483,0.485,0.483,0.483,0.444,0.487,0.494,0.485,0.483,0.572,0.428,0.478,0.48,0.483,0.478,0.444,0.543,0.478,0.48,0.572,0.478,0.48,0.48,0.483,0.327,0.485,0.478,0.483,0.485,0.485,0.483,0.483,0.439,0.311,0.37,0.485,0.483,0.485,0.48,0.483,0.593,0.331,0.487,0.381,0.581,0.411,0.485,0.485,0.478,0.524,0.575,0.485,0.579,0.362,0.382,0.362,0.485,0.485,0.485,0.483,0.483,0.485,0.478,0.315,0.452,0.487,0.487,0.337,0.331,0.483,0.485,0.331,0.478,0.452,0.483,0.483,0.487,0.494,0.483,0.313,0.485,0.485,0.485,0.478,0.483,0.485,0.485,0.483,0.483,0.485,0.483,0.48,0.478,0.485,0.485,0.48,0.494,0.391,0.45,0.406,0.504,0.485,0.483,0.485,0.389,0.452,0.485,0.487,0.485,0.487,0.483,0.579,0.48,0.483,0.478,0.485,0.485,0.485,0.483,0.483,0.478,0.483,0.331,0.311,0.562,0.382,0.32,0.483,0.485,0.485,0.483,0.485,0.331,0.485,0.485,0.485,0.478,0.483,0.483,0.478,0.485,0.579,0.337,0.408,0.581,0.485,0.485,0.478,0.37,0.487,0.487,0.485,0.483,0.487,0.478,0.485,0.478,0.48,0.485,0.355,0.332,0.331,0.485,0.485,0.444,0.337,0.483,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.483,0.485,0.485,0.331,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.48,0.577,0.485,0.495,0.485,0.485,0.485,0.427,0.48,0.485,0.407,0.485,0.332,0.485,0.48,0.485'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub12 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub12['pred'] = sub12['pred'].apply(lambda x: round(x,3))\n",
    "sub12_txt = ''\n",
    "for prob in list(sub12['pred'].values):\n",
    "    sub12_txt = sub12_txt+','+str(prob)\n",
    "sub12_txt = sub12_txt[1:]\n",
    "\n",
    "sub12_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7da2d",
   "metadata": {},
   "source": [
    "### Submission 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7001f",
   "metadata": {},
   "source": [
    "* Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b111c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.676,0.611,0.571,0.746,0.705,0.764,0.524,0.302,0.629,0.629,0.471,0.358,0.766,0.766,0.554,0.68,0.474,0.301,0.573,0.749,0.757,0.55,0.685,0.629,0.728,0.63,0.736,0.72,0.577,0.569,0.706,0.728,0.562,0.722,0.766,0.748,0.491,0.568,0.766,0.609,0.571,0.758,0.59,0.676,0.758,0.698,0.76,0.403,0.584,0.749,0.307,0.764,0.728,0.304,0.313,0.294,0.766,0.746,0.766,0.683,0.742,0.631,0.412,0.681,0.764,0.637,0.766,0.621,0.762,0.766,0.759,0.607,0.748,0.569,0.614,0.674,0.615,0.76,0.686,0.732,0.313,0.562,0.758,0.326,0.744,0.636,0.738,0.717,0.614,0.586,0.763,0.577,0.504,0.763,0.513,0.766,0.766,0.7,0.459,0.631,0.706,0.631,0.713,0.766,0.568,0.43,0.484,0.335,0.569,0.456,0.575,0.766,0.756,0.724,0.318,0.756,0.599,0.761,0.714,0.766,0.73,0.766,0.334,0.581,0.609,0.766,0.762,0.676,0.302,0.295,0.738,0.568,0.292,0.682,0.628,0.754,0.579,0.766,0.491,0.629,0.384,0.633,0.567,0.52,0.697,0.766,0.536,0.579,0.343,0.76,0.758,0.724,0.766,0.758,0.533,0.675,0.653,0.593,0.758,0.526,0.738,0.601,0.766,0.548,0.758,0.731,0.624,0.744,0.562,0.32,0.762,0.571,0.654,0.753,0.76,0.562,0.766,0.742,0.445,0.748,0.374,0.302,0.639,0.481,0.436,0.722,0.764,0.766,0.766,0.754,0.762,0.495,0.483,0.738,0.766,0.305,0.742,0.362,0.714,0.698,0.653,0.728,0.766,0.758,0.454,0.567,0.398,0.316,0.522,0.295,0.723,0.64,0.39,0.766,0.758,0.55,0.766,0.716,0.613,0.641,0.587,0.617,0.481,0.758,0.765,0.439,0.293,0.76,0.404,0.324,0.294,0.61,0.583,0.766,0.403,0.766,0.766,0.615,0.766,0.742,0.327,0.766,0.635,0.651,0.692,0.388,0.579,0.292,0.74,0.568,0.384,0.295,0.738,0.759,0.302,0.484,0.766,0.755,0.426,0.295,0.734,0.452,0.325,0.766,0.766,0.766,0.55,0.766,0.631,0.293,0.573,0.686,0.732,0.355,0.626,0.588,0.719,0.734,0.738,0.756,0.734,0.766,0.303,0.738,0.552,0.293,0.732,0.594,0.484,0.764,0.306,0.292,0.345,0.71,0.321,0.426,0.643,0.295,0.415,0.708,0.766,0.688,0.675,0.758,0.766,0.706,0.566,0.693,0.766,0.721,0.766,0.706,0.292,0.671,0.732,0.309,0.766,0.481,0.603,0.498,0.731,0.742,0.321,0.453,0.766,0.728,0.481,0.526,0.615,0.484,0.332,0.736,0.297,0.756,0.587,0.484,0.706,0.766,0.302,0.698,0.625,0.764,0.609,0.714,0.66,0.756,0.571,0.297,0.726,0.757,0.322,0.323,0.766,0.714,0.382,0.766,0.716,0.726,0.766,0.766,0.763,0.641,0.322,0.579,0.534,0.734,0.297,0.766,0.744,0.392,0.732,0.763,0.607,0.766,0.546,0.645,0.562,0.685,0.595,0.556,0.734,0.739,0.326,0.491,0.734,0.692,0.37,0.758,0.766,0.766,0.33,0.748,0.52,0.766,0.635,0.764,0.766,0.762,0.63,0.561,0.491,0.766,0.766,0.522,0.766,0.599,0.312,0.295,0.607,0.362,0.378,0.408,0.685,0.503,0.64,0.299,0.684,0.295,0.312,0.46,0.762,0.556,0.597,0.766,0.402,0.328,0.481,0.295,0.763,0.712,0.542,0.307,0.295,0.766,0.47,0.728,0.747,0.542,0.295,0.625,0.351,0.734,0.721,0.484,0.653,0.504,0.361,0.766,0.422,0.433,0.758,0.766,0.326,0.756,0.519,0.756,0.766,0.766,0.736,0.754,0.766,0.329,0.762,0.623,0.54,0.766,0.297,0.538,0.389,0.617,0.485,0.474,0.387,0.754,0.479,0.758,0.713,0.75,0.481,0.548,0.304,0.766,0.766,0.307,0.292,0.387,0.733,0.703,0.766,0.315,0.295,0.534,0.481,0.763,0.645,0.764,0.762,0.412,0.41,0.308,0.741,0.756,0.759,0.485,0.292,0.734,0.376,0.766,0.732,0.484,0.685,0.766,0.663,0.434,0.303,0.292,0.293,0.485,0.491,0.307,0.492,0.327,0.292,0.316,0.734,0.766,0.712,0.75,0.766,0.617,0.292,0.738,0.546,0.317,0.766,0.596,0.593,0.295,0.682,0.513,0.305,0.737,0.481,0.444,0.76,0.481,0.295,0.754,0.476,0.481,0.764,0.73,0.611,0.734,0.646,0.671,0.293,0.701,0.328,0.491,0.758,0.331,0.631,0.75,0.321,0.766,0.71,0.534,0.327,0.702,0.766,0.401,0.301,0.318,0.738,0.692,0.526,0.621,0.732,0.741,0.742,0.292,0.76,0.762,0.53,0.766,0.758,0.337,0.75,0.758,0.599,0.751,0.599,0.294,0.61,0.514,0.766,0.295,0.328,0.698,0.734,0.734,0.577,0.766,0.313,0.702,0.484,0.742,0.701,0.758,0.76,0.635,0.292,0.502,0.396,0.758,0.57,0.303,0.312,0.308,0.667,0.763,0.747,0.75,0.73,0.4,0.294,0.766,0.752,0.766,0.704,0.752,0.481,0.292,0.758,0.484,0.333,0.292,0.631,0.758,0.762,0.758,0.71,0.499,0.757,0.292,0.481,0.76,0.766,0.58,0.724,0.481,0.342,0.54,0.693,0.76,0.491,0.358,0.76,0.766,0.482,0.736,0.394,0.481,0.306,0.347,0.76,0.762,0.766,0.71,0.309,0.76,0.726,0.481,0.505,0.456,0.766,0.758,0.366,0.319,0.493,0.346,0.519,0.756,0.621,0.748,0.303,0.422,0.46,0.368,0.295,0.415,0.482,0.75,0.689,0.609,0.736,0.766,0.322,0.716,0.766,0.758,0.304,0.766,0.491,0.644,0.723,0.428,0.766,0.302,0.756,0.328,0.621,0.325,0.713,0.721,0.75,0.596,0.75,0.306,0.706,0.491,0.734,0.292,0.766,0.59,0.311,0.766,0.45,0.307,0.763,0.536,0.697,0.76,0.664,0.62,0.766,0.758,0.766,0.766,0.587,0.763,0.61,0.766,0.766,0.481,0.506,0.75,0.436,0.766,0.752,0.397,0.659,0.742,0.481,0.59,0.766,0.304,0.481,0.763,0.766,0.764,0.293,0.502,0.766,0.562,0.619,0.417,0.764,0.695,0.378,0.591,0.766,0.326,0.592,0.326,0.323,0.389,0.756,0.629,0.766,0.481,0.726,0.734,0.762,0.43,0.382,0.726,0.762,0.295,0.308,0.763,0.421,0.469,0.519,0.349,0.766,0.306,0.766,0.64,0.402,0.293,0.766,0.294,0.625,0.766,0.641,0.637,0.764,0.757,0.757,0.481,0.588,0.422,0.514,0.558,0.766,0.751,0.766,0.766,0.75,0.758,0.569,0.713,0.59,0.463,0.691,0.552,0.485,0.766,0.763,0.758,0.687,0.403,0.327,0.64,0.379,0.358,0.627,0.484,0.766,0.477,0.747,0.736,0.683,0.644,0.722,0.613,0.595,0.639,0.484,0.749,0.504,0.364,0.481,0.766,0.646,0.325,0.766,0.692,0.749,0.766,0.485,0.764,0.361,0.352,0.484,0.742,0.742,0.295,0.766,0.561,0.766,0.424,0.555,0.766,0.685,0.766,0.295,0.332,0.501,0.373,0.694,0.469,0.425,0.394,0.766,0.482,0.421,0.329,0.766,0.328,0.326,0.333,0.766,0.766,0.736,0.441,0.332,0.493,0.31,0.637,0.34,0.325,0.359,0.685,0.307,0.305,0.523,0.538,0.365,0.513,0.766,0.766,0.485,0.71,0.766,0.742,0.434,0.766,0.653,0.374,0.763,0.577,0.683,0.31,0.765,0.484,0.571,0.304,0.71,0.766,0.766,0.704,0.329,0.766,0.705,0.292,0.736,0.518,0.758,0.354,0.571,0.764,0.738,0.766,0.621,0.766,0.766,0.295,0.766,0.54,0.292,0.766,0.512,0.766,0.63,0.756,0.763,0.328,0.766,0.506,0.439,0.534,0.726,0.734,0.761,0.307,0.766,0.446,0.41,0.328,0.302,0.766,0.712,0.636,0.649,0.756,0.726,0.328,0.766,0.335,0.727,0.331,0.61,0.312,0.766,0.482,0.64,0.758,0.758,0.764,0.597,0.766,0.321,0.678,0.732,0.611,0.603,0.585,0.766,0.485,0.461,0.76,0.76,0.635,0.415,0.325,0.73,0.637,0.353,0.756,0.677,0.762,0.708,0.306,0.355,0.758,0.457,0.766,0.405,0.759,0.484,0.736,0.292,0.484,0.484,0.364,0.481,0.752,0.763,0.766,0.722,0.484,0.692,0.484,0.468,0.292,0.532,0.53,0.3,0.329,0.345,0.418,0.367,0.316,0.763,0.766,0.486,0.766,0.756,0.33,0.589,0.633,0.622,0.766,0.751,0.388,0.499,0.608,0.409,0.481,0.766,0.766,0.589,0.766,0.669,0.695,0.762,0.425,0.606,0.321,0.703,0.758,0.403,0.766,0.563,0.766,0.468,0.763,0.358,0.484,0.481,0.454,0.485,0.569,0.481,0.665,0.312,0.457,0.477,0.33,0.575,0.292,0.481,0.55,0.633,0.764,0.552,0.766,0.609,0.304,0.323,0.491,0.688,0.302,0.459,0.449,0.327,0.307,0.334,0.673,0.756,0.491,0.444,0.478,0.303,0.73,0.542,0.566,0.299,0.766,0.389,0.457,0.491,0.59,0.33,0.459,0.759,0.337,0.316,0.685,0.292,0.751,0.481,0.491,0.71,0.484,0.766,0.292,0.481,0.73,0.484,0.484,0.738,0.326,0.481,0.292,0.615,0.601,0.491,0.306,0.577,0.733,0.762,0.52,0.697,0.766,0.327,0.305,0.325,0.491,0.766,0.734,0.665,0.685,0.482,0.765,0.55,0.45,0.482,0.766,0.294,0.686,0.379,0.763,0.481,0.491,0.292,0.722,0.526,0.476,0.34,0.297,0.645,0.412,0.763,0.306,0.742,0.292,0.293,0.762,0.292,0.293,0.422,0.336,0.534,0.476,0.474,0.482,0.482,0.503,0.595,0.332,0.742,0.321,0.491,0.711,0.484,0.766,0.73,0.484,0.766,0.62,0.736,0.502,0.734,0.522,0.526,0.483,0.556,0.766,0.599,0.325,0.304,0.305,0.476,0.365,0.573,0.485,0.385,0.306,0.7,0.461,0.637,0.76,0.481,0.766,0.484,0.481,0.292,0.336,0.746,0.546,0.304,0.584,0.716,0.73,0.766,0.678,0.491,0.766,0.763,0.311,0.752,0.48,0.758,0.481,0.484,0.482,0.763,0.489,0.758,0.41,0.32,0.312,0.447,0.766,0.763,0.33,0.575,0.292,0.324,0.756,0.75,0.56,0.32,0.671,0.484,0.766,0.766,0.314,0.665,0.676,0.328,0.584,0.583,0.339,0.637,0.491,0.481,0.481,0.334,0.325,0.295,0.484,0.397,0.595,0.72,0.74,0.766,0.328,0.299,0.615,0.368,0.766,0.326,0.754,0.306,0.542,0.341,0.729,0.326,0.295,0.637,0.481,0.641,0.765,0.313,0.307,0.474,0.679,0.766,0.445,0.739,0.327,0.328,0.481,0.484,0.757,0.317,0.757,0.628,0.481,0.336,0.762,0.335,0.332,0.481,0.319,0.295,0.766,0.429,0.484,0.32,0.484,0.59,0.766,0.728,0.338,0.724,0.482,0.35,0.484,0.452,0.33,0.438,0.647,0.469,0.602,0.766,0.482,0.402,0.481,0.664,0.388,0.292,0.756,0.292,0.348,0.577,0.569,0.484,0.546,0.637,0.482,0.484,0.327,0.485,0.484,0.333,0.447,0.351,0.456,0.481,0.762,0.332,0.484,0.292,0.485,0.352,0.762,0.335,0.481,0.484,0.362,0.484,0.433,0.483,0.313,0.484,0.481,0.373,0.764,0.399,0.437,0.367,0.482,0.484,0.75,0.345,0.486,0.337,0.481,0.482,0.301,0.459,0.304,0.482,0.695,0.294,0.481,0.333,0.64,0.635,0.481,0.637,0.546,0.481,0.313,0.369,0.481,0.32,0.534,0.766,0.482,0.485,0.474,0.33,0.332,0.651,0.534,0.483,0.32,0.481,0.761,0.313,0.295,0.31,0.481,0.583,0.328,0.402,0.491,0.571,0.331,0.454,0.585,0.319,0.738,0.641,0.481,0.337,0.754,0.304,0.502,0.325,0.766,0.631,0.487,0.481,0.556,0.484,0.579,0.732,0.495,0.508,0.763,0.294,0.338,0.636,0.748,0.766,0.754,0.451,0.679,0.76,0.452,0.758,0.373,0.491,0.295,0.718,0.723,0.435,0.359,0.561,0.766,0.44,0.558,0.435,0.574,0.766,0.766,0.482,0.766,0.482,0.686,0.546,0.636,0.456,0.452,0.309,0.752,0.321,0.491,0.3,0.706,0.341,0.503,0.617,0.389,0.766,0.612,0.484,0.766,0.758,0.518,0.372,0.752,0.348,0.598,0.763,0.293,0.75,0.304,0.293,0.716,0.321,0.677,0.766,0.502,0.59,0.717,0.292,0.313,0.294,0.55,0.307,0.293,0.304,0.734,0.302,0.357,0.766,0.307,0.587,0.678,0.766,0.738,0.323,0.299,0.754,0.292,0.496,0.534,0.766,0.621,0.32,0.379,0.609,0.76,0.678,0.504,0.766,0.293,0.632,0.294,0.344,0.758,0.406,0.758,0.306,0.359,0.763,0.292,0.665,0.323,0.689,0.734,0.763,0.606,0.294,0.766,0.292,0.756,0.563,0.442,0.623,0.746,0.766,0.502,0.491,0.727,0.308,0.306,0.327,0.292,0.461,0.326,0.32,0.766,0.704,0.307,0.692,0.758,0.338,0.32,0.593,0.321,0.675,0.425,0.595,0.328,0.765,0.295,0.441,0.326,0.403,0.495,0.316,0.305,0.328,0.308,0.74,0.685,0.742,0.7,0.766,0.322,0.491,0.75,0.328,0.316,0.309,0.758,0.685,0.673,0.579,0.761,0.579,0.636,0.766,0.577,0.309,0.675,0.724,0.486,0.689,0.292,0.302,0.75,0.585,0.377,0.675,0.426,0.766,0.766,0.766,0.599,0.637,0.763,0.678,0.766,0.486,0.583,0.42,0.372,0.578,0.377,0.587,0.292,0.76,0.763,0.326,0.309,0.637,0.322,0.76,0.583,0.637,0.468,0.638,0.336,0.293,0.72,0.756,0.365,0.763,0.295,0.307,0.619,0.329,0.481,0.762,0.637,0.354,0.722,0.711,0.647,0.484,0.762,0.293,0.534,0.675,0.593,0.759,0.585,0.486,0.502,0.763,0.59,0.491,0.423,0.397,0.337,0.339,0.295,0.306,0.385,0.763,0.59,0.292,0.745,0.504,0.292,0.619,0.583,0.43,0.326,0.293,0.293,0.382,0.679,0.76,0.297,0.758,0.391,0.426,0.358,0.373,0.295,0.354,0.675,0.434,0.378,0.59,0.72,0.292,0.352,0.332,0.589,0.312,0.335,0.542,0.631,0.58,0.308,0.312,0.309,0.315,0.759,0.673,0.34,0.335,0.637,0.637,0.649,0.305,0.484,0.447,0.637,0.585,0.718,0.59,0.504,0.482,0.534,0.628,0.569,0.485,0.585,0.633,0.486,0.637,0.34,0.298,0.306,0.308,0.631,0.382,0.322,0.484,0.359,0.661,0.322,0.329,0.49,0.304,0.306,0.581,0.362,0.555,0.362,0.426,0.484,0.327,0.329,0.484,0.485,0.485,0.327,0.484,0.485,0.443,0.582,0.484,0.485,0.485,0.303,0.384,0.485,0.476,0.306,0.482,0.387,0.629,0.481,0.484,0.304,0.491,0.484,0.59,0.484,0.485,0.484,0.485,0.482,0.292,0.481,0.484,0.491,0.329,0.484,0.359,0.411,0.481,0.484,0.484,0.484,0.452,0.385,0.381,0.334,0.326,0.532,0.502,0.485,0.484,0.484,0.485,0.485,0.322,0.385,0.328,0.491,0.337,0.311,0.484,0.486,0.491,0.583,0.722,0.486,0.485,0.502,0.484,0.491,0.484,0.476,0.577,0.491,0.491,0.484,0.484,0.562,0.388,0.491,0.484,0.406,0.371,0.368,0.484,0.481,0.481,0.534,0.502,0.468,0.329,0.583,0.589,0.714,0.64,0.491,0.645,0.534,0.328,0.485,0.446,0.385,0.486,0.481,0.48,0.485,0.55,0.491,0.301,0.485,0.352,0.491,0.486,0.481,0.363,0.605,0.332,0.484,0.329,0.484,0.504,0.334,0.322,0.306,0.491,0.322,0.504,0.327,0.32,0.484,0.316,0.639,0.326,0.322,0.491,0.358,0.306,0.464,0.484,0.33,0.484,0.601,0.486,0.534,0.637,0.637,0.637,0.485,0.312,0.482,0.507,0.484,0.323,0.322,0.315,0.354,0.506,0.293,0.332,0.328,0.323,0.491,0.491,0.361,0.384,0.351,0.563,0.484,0.385,0.331,0.484,0.484,0.401,0.481,0.601,0.486,0.361,0.484,0.484,0.484,0.481,0.534,0.482,0.476,0.485,0.507,0.467,0.49,0.484,0.481,0.484,0.483,0.481,0.309,0.484,0.481,0.482,0.486,0.491,0.59,0.486,0.484,0.491,0.491,0.484,0.491,0.484,0.481,0.481,0.359,0.393,0.491,0.49,0.484,0.481,0.332,0.481,0.481,0.557,0.539,0.482,0.481,0.333,0.476,0.481,0.481,0.484,0.503,0.491,0.485,0.486,0.393,0.318,0.485,0.484,0.481,0.482,0.481,0.481,0.481,0.482,0.484,0.481,0.481,0.481,0.481,0.359,0.332,0.481,0.491,0.482,0.491,0.486,0.326,0.473,0.491,0.491,0.484,0.335,0.481,0.484,0.481,0.482,0.319,0.484,0.482,0.328,0.484,0.484,0.484,0.332,0.484,0.368,0.484,0.598,0.481,0.481,0.591,0.482,0.481,0.589,0.485,0.583,0.319,0.315,0.481,0.371,0.561,0.485,0.359,0.326,0.571,0.484,0.484,0.485,0.499,0.334,0.476,0.482,0.481,0.378,0.486,0.334,0.484,0.299,0.293,0.312,0.344,0.484,0.59,0.306,0.49,0.346,0.637,0.318,0.637,0.491,0.326,0.485,0.332,0.426,0.441,0.484,0.485,0.379,0.485,0.485,0.485,0.485,0.484,0.484,0.484,0.363,0.435,0.484,0.382,0.335,0.625,0.417,0.484,0.486,0.483,0.504,0.548,0.33,0.447,0.306,0.421,0.637,0.485,0.355,0.483,0.328,0.637,0.482,0.484,0.481,0.421,0.485,0.481,0.481,0.484,0.485,0.484,0.484,0.442,0.486,0.491,0.485,0.484,0.571,0.421,0.481,0.482,0.484,0.482,0.437,0.54,0.481,0.482,0.571,0.481,0.482,0.482,0.484,0.327,0.485,0.481,0.484,0.485,0.485,0.484,0.484,0.439,0.31,0.361,0.485,0.484,0.485,0.482,0.484,0.595,0.328,0.486,0.373,0.585,0.416,0.485,0.485,0.482,0.524,0.577,0.485,0.583,0.362,0.394,0.362,0.485,0.485,0.485,0.484,0.484,0.485,0.48,0.313,0.447,0.486,0.486,0.336,0.334,0.484,0.485,0.33,0.481,0.452,0.484,0.484,0.486,0.491,0.484,0.315,0.485,0.485,0.485,0.481,0.484,0.485,0.485,0.484,0.484,0.485,0.484,0.482,0.481,0.485,0.485,0.482,0.491,0.385,0.443,0.399,0.504,0.485,0.484,0.485,0.384,0.452,0.485,0.486,0.485,0.486,0.484,0.583,0.482,0.484,0.481,0.485,0.485,0.485,0.484,0.484,0.481,0.484,0.328,0.31,0.562,0.394,0.323,0.484,0.485,0.485,0.484,0.485,0.328,0.485,0.485,0.485,0.481,0.484,0.484,0.481,0.485,0.583,0.337,0.399,0.585,0.479,0.485,0.481,0.361,0.486,0.486,0.485,0.484,0.486,0.481,0.485,0.481,0.482,0.485,0.358,0.335,0.335,0.479,0.485,0.441,0.338,0.484,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.484,0.485,0.485,0.334,0.485,0.485,0.485,0.485,0.485,0.485,0.485,0.482,0.579,0.485,0.496,0.485,0.485,0.485,0.421,0.482,0.485,0.407,0.485,0.335,0.485,0.482,0.485'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.5\n",
    "sub13 = pd.merge(sub11,sub12,how='left',on='unique_id')\n",
    "sub13['pred'] = th*sub13['pred_x']+(1-th)*sub13['pred_y']\n",
    "sub13['pred'] = sub13['pred'].apply(lambda x: round(x,3))\n",
    "sub13_txt = ''\n",
    "for prob in list(sub13['pred'].values):\n",
    "    sub13_txt = sub13_txt+','+str(prob)\n",
    "sub13_txt = sub13_txt[1:]\n",
    "\n",
    "sub13_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8752ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>pred_x</th>\n",
       "      <th>pred_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>7982</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>7990</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>7993</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>7994</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>7998</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id  pred_x  pred_y\n",
       "0             9   0.675   0.677\n",
       "1            18   0.603   0.618\n",
       "2            21   0.560   0.581\n",
       "3            25   0.756   0.736\n",
       "4            31   0.699   0.711\n",
       "...         ...     ...     ...\n",
       "2375       7982   0.486   0.485\n",
       "2376       7990   0.337   0.332\n",
       "2377       7993   0.485   0.485\n",
       "2378       7994   0.485   0.480\n",
       "2379       7998   0.485   0.485\n",
       "\n",
       "[2380 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub13_prep = pd.merge(sub11,sub12,how='left',on='unique_id')\n",
    "sub13_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3a4f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub13_prep['pred'] = th*sub13_prep['pred_x']+(1-th)*sub13_prep['pred_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1426832",
   "metadata": {},
   "source": [
    "### Submission 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e2295d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ce29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = pd.DataFrame(imp.transform(X_train),columns= X_train.columns, index=X_train.index)\n",
    "X_train_under_imputed = pd.DataFrame(imp.transform(X_train_under),columns= X_train_under.columns, index=X_train_under.index)\n",
    "X_train_experienced_imputed = pd.DataFrame(imp.transform(X_train_experienced),columns= X_train_experienced.columns, index=X_train_experienced.index)\n",
    "X_train_experienced_under_imputed = pd.DataFrame(imp.transform(X_train_experienced_under),columns= X_train_experienced_under.columns, index=X_train_experienced_under.index)\n",
    "X_train_cold_imputed = pd.DataFrame(imp.transform(X_train_cold),columns= X_train_cold.columns, index=X_train_cold.index)\n",
    "X_train_cold_under_imputed = pd.DataFrame(imp.transform(X_train_cold_under),columns= X_train_cold_under.columns, index=X_train_cold_under.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6179857",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_imputed = pd.DataFrame(imp.transform(X_val),columns= X_val.columns, index=X_val.index)\n",
    "X_val_experienced_imputed = pd.DataFrame(imp.transform(X_val_experienced),columns= X_val_experienced.columns, index=X_val_experienced.index)\n",
    "X_val_cold_imputed = pd.DataFrame(imp.transform(X_val_cold),columns= X_val_cold.columns, index=X_val_cold.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = pd.DataFrame(imp.transform(X_test),columns= X_test.columns, index=X_test.index)\n",
    "X_test_experienced_imputed = pd.DataFrame(imp.transform(X_test_experienced),columns= X_test_experienced.columns, index=X_test_experienced.index)\n",
    "X_test_cold_imputed = pd.DataFrame(imp.transform(X_test_cold),columns= X_test_cold.columns, index=X_test_cold.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5bd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:30:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:30:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=100,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_train_experienced_under_imputed,y_train_experienced_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b52969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9916020292668568\n",
      "Val ROC:  0.8513037350246653\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2ce2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126105</td>\n",
       "      <td>0.278155</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>0.412522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.126105              0.0        0.126105       0.278155   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.134367      0.412522  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0ea3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.997,0.965,0.994,0.998,0.997,1.0,0.49,0.011,0.958,0.994,0.055,0.014,0.999,1.0,0.959,0.998,0.011,0.002,0.985,0.996,1.0,0.992,0.999,0.999,0.999,0.997,0.966,0.999,0.892,0.925,0.998,0.998,0.995,0.976,1.0,0.997,0.546,0.964,0.991,0.974,0.913,0.988,0.998,0.904,0.999,0.994,0.999,0.311,0.56,0.849,0.007,0.999,0.996,0.002,0.009,0.009,0.997,0.999,0.997,0.998,1.0,0.02,0.041,0.998,1.0,0.759,0.999,0.995,0.999,0.999,1.0,0.998,0.999,0.999,0.887,0.99,0.985,1.0,0.997,0.995,0.019,0.962,0.999,0.08,0.999,0.906,0.718,1.0,0.997,0.969,1.0,0.985,0.441,0.999,0.012,0.977,0.996,0.98,0.052,1.0,0.989,0.999,0.999,0.999,0.859,0.348,0.775,0.32,0.998,0.845,0.982,1.0,0.998,0.997,0.931,0.998,0.928,0.998,0.992,0.999,0.996,1.0,0.519,0.453,0.959,0.983,0.997,1.0,0.008,0.001,0.983,0.312,0.002,0.997,0.97,0.999,0.243,0.998,0.993,0.998,0.068,0.208,0.998,0.965,0.929,0.999,0.935,0.999,0.021,0.983,1.0,0.995,0.999,0.999,0.66,0.974,0.991,0.988,0.999,0.601,1.0,0.747,0.996,0.971,1.0,0.994,0.794,0.994,0.996,0.212,0.999,0.478,0.548,0.999,0.998,0.85,0.999,0.998,0.113,0.968,0.036,0.126,0.953,0.215,0.569,0.999,0.994,1.0,0.999,0.985,0.998,0.04,0.501,0.997,0.987,0.002,0.997,0.031,0.999,0.865,0.99,0.999,0.998,0.994,0.971,0.118,0.242,0.58,0.244,0.006,1.0,0.998,0.033,1.0,0.999,0.806,0.995,0.999,0.536,0.999,0.964,0.992,0.038,0.99,0.999,0.048,0.005,0.98,0.157,0.016,0.002,0.96,0.916,0.998,0.033,1.0,0.999,0.936,1.0,1.0,0.001,0.991,0.949,0.999,0.999,0.029,0.751,0.002,0.996,0.978,0.923,0.004,0.998,0.999,0.005,0.647,1.0,1.0,0.031,0.013,1.0,0.792,0.024,0.999,1.0,0.999,0.743,1.0,0.953,0.038,0.99,0.938,1.0,0.48,0.999,0.99,0.999,1.0,0.996,0.987,0.996,0.992,0.001,0.998,0.749,0.003,0.999,0.984,0.614,0.999,0.003,0.007,0.02,0.999,0.968,0.003,0.999,0.007,0.085,1.0,0.997,0.995,0.997,0.994,1.0,0.999,0.983,0.999,0.999,0.997,1.0,0.999,0.023,1.0,0.998,0.002,1.0,0.096,0.842,0.143,0.928,0.975,0.001,0.515,1.0,0.983,0.266,0.708,0.606,0.904,0.017,0.994,0.194,0.999,0.876,0.804,0.998,0.997,0.001,0.704,0.981,0.976,0.618,0.997,0.926,1.0,0.939,0.008,0.991,1.0,0.115,0.022,0.998,0.996,0.053,1.0,0.782,0.994,1.0,0.999,1.0,0.995,0.006,0.929,0.904,0.999,0.011,1.0,0.998,0.099,0.993,0.997,0.996,0.999,0.998,0.852,0.999,1.0,0.998,0.707,0.974,0.987,0.035,0.973,0.999,0.985,0.002,1.0,1.0,0.998,0.05,0.999,0.278,1.0,0.986,0.999,1.0,1.0,0.999,0.998,0.994,0.999,1.0,0.928,1.0,0.91,0.018,0.005,0.913,0.019,0.007,0.007,0.959,0.463,0.96,0.006,0.971,0.002,0.074,0.749,1.0,0.912,0.971,0.998,0.024,0.062,0.072,0.081,0.999,0.937,0.982,0.004,0.007,1.0,0.284,0.999,0.993,0.991,0.004,1.0,0.011,0.962,0.999,0.172,0.999,0.543,0.075,0.999,0.027,0.088,0.994,0.766,0.014,0.998,0.931,1.0,1.0,1.0,0.995,1.0,0.999,0.114,0.969,0.996,0.995,0.998,0.005,0.982,0.012,0.897,0.347,0.586,0.008,0.888,0.987,0.991,0.986,0.994,0.005,0.972,0.197,0.983,1.0,0.004,0.005,0.003,0.994,0.997,1.0,0.007,0.002,0.973,0.049,0.996,0.787,1.0,1.0,0.429,0.019,0.006,1.0,0.99,1.0,0.961,0.003,1.0,0.008,1.0,1.0,0.449,0.999,0.88,0.995,0.272,0.034,0.002,0.007,0.015,0.968,0.009,0.578,0.033,0.001,0.036,0.999,0.999,0.996,1.0,1.0,0.969,0.002,0.999,0.363,0.012,0.998,0.949,0.653,0.012,0.996,0.859,0.237,0.996,0.082,0.145,0.981,0.017,0.002,0.999,0.027,0.129,1.0,0.999,0.996,0.996,0.988,0.996,0.006,0.541,0.017,1.0,1.0,0.013,0.957,0.989,0.006,1.0,0.993,0.005,0.051,0.956,1.0,0.836,0.001,0.007,0.996,0.997,0.666,0.999,1.0,1.0,1.0,0.089,0.999,0.998,0.021,1.0,0.993,0.787,0.997,0.741,0.987,1.0,0.987,0.033,0.969,0.963,1.0,0.005,0.002,0.993,0.944,1.0,0.619,0.999,0.017,0.962,0.06,1.0,0.998,0.819,1.0,0.997,0.006,0.535,0.018,0.999,0.985,0.046,0.066,0.002,0.998,0.995,0.999,0.999,0.999,0.11,0.012,1.0,0.992,1.0,0.847,0.992,0.064,0.004,0.999,0.612,0.026,0.004,0.991,1.0,1.0,0.997,0.978,0.24,1.0,0.075,0.036,0.921,1.0,0.989,0.999,0.139,0.004,0.871,0.989,0.977,0.995,0.002,0.997,0.997,0.911,0.965,0.028,0.055,0.065,0.076,0.997,1.0,0.999,0.988,0.119,0.996,0.977,0.022,0.721,0.902,0.999,0.994,0.056,0.03,0.507,0.03,0.706,0.998,0.979,0.985,0.003,0.941,0.919,0.928,0.003,0.018,0.941,0.995,0.991,0.945,0.999,1.0,0.009,0.996,0.996,0.996,0.36,1.0,0.996,0.996,1.0,0.017,0.999,0.021,1.0,0.043,0.997,0.005,0.99,0.84,0.978,0.996,0.975,0.03,0.821,0.478,1.0,0.034,0.997,0.701,0.031,0.998,0.004,0.004,1.0,0.902,0.883,0.998,0.831,0.919,0.999,0.999,0.998,0.998,0.999,0.997,0.982,1.0,1.0,0.078,0.339,0.996,0.308,0.998,0.985,0.057,0.617,0.998,0.156,0.944,1.0,0.006,0.128,0.998,1.0,0.993,0.002,0.874,1.0,0.985,0.973,0.025,0.992,0.633,0.001,0.999,1.0,0.005,0.142,0.078,0.07,0.888,0.998,0.921,0.99,0.191,0.991,0.961,1.0,0.252,0.02,0.979,1.0,0.003,0.017,1.0,0.311,0.337,0.644,0.044,0.999,0.003,0.998,0.643,0.002,0.005,0.935,0.001,0.696,1.0,0.85,0.992,1.0,0.999,0.999,0.007,0.964,0.892,0.367,0.892,0.999,0.999,0.987,0.999,1.0,0.999,0.665,0.872,0.922,0.066,0.981,0.703,0.009,1.0,0.988,0.997,0.999,0.018,0.031,0.986,0.21,0.001,0.987,0.008,1.0,0.727,1.0,0.993,0.998,0.998,0.999,0.609,0.999,0.96,0.783,0.993,0.996,0.029,0.013,1.0,0.97,0.016,1.0,0.996,1.0,1.0,0.04,1.0,0.029,0.706,0.007,0.893,0.926,0.178,1.0,0.463,1.0,0.536,0.954,0.998,0.998,1.0,0.003,0.028,0.764,0.075,1.0,0.825,0.818,0.021,1.0,0.73,0.772,0.015,1.0,0.04,0.004,0.91,1.0,0.999,0.948,0.03,0.031,0.12,0.039,0.976,0.417,0.056,0.005,0.997,0.028,0.016,0.58,0.992,0.179,0.042,1.0,0.998,0.032,0.909,1.0,0.996,0.652,0.999,0.993,0.005,0.995,0.909,0.879,0.105,0.999,0.045,0.104,0.012,0.996,0.999,0.999,1.0,0.097,0.999,1.0,0.001,0.994,0.791,0.999,0.048,0.186,1.0,1.0,0.999,0.969,0.997,0.995,0.001,0.999,0.62,0.001,0.999,0.932,1.0,1.0,0.999,1.0,0.442,1.0,0.145,0.04,0.996,0.999,0.997,0.991,0.018,1.0,0.01,0.312,0.001,0.021,1.0,1.0,0.994,0.999,0.997,0.997,0.178,0.998,0.612,0.897,0.297,0.97,0.069,0.993,0.798,0.994,1.0,0.993,1.0,0.975,0.983,0.024,0.999,0.305,0.996,0.993,0.999,0.999,0.971,0.075,0.998,0.999,0.986,0.819,0.02,0.999,0.978,0.221,1.0,0.697,0.985,0.99,0.015,0.015,1.0,0.094,1.0,0.026,0.998,0.069,1.0,0.001,0.443,0.007,0.051,0.038,0.963,0.996,0.994,0.999,0.38,1.0,0.115,0.953,0.011,0.916,0.928,0.004,0.062,0.028,0.112,0.002,0.004,1.0,1.0,0.19,0.994,1.0,0.13,0.893,0.95,0.984,0.995,0.999,0.003,0.163,0.771,0.232,0.245,1.0,0.998,0.448,1.0,0.995,0.954,0.965,0.462,0.998,0.016,0.988,0.998,0.891,0.989,0.987,1.0,0.242,0.999,0.138,0.639,0.028,0.116,0.069,0.982,0.086,0.757,0.802,0.042,0.032,0.183,0.974,0.002,0.041,0.432,0.692,0.994,0.525,1.0,0.262,0.005,0.008,0.67,0.931,0.072,0.592,0.755,0.032,0.011,0.121,1.0,1.0,0.823,0.034,0.195,0.011,0.988,0.066,0.43,0.012,1.0,0.095,0.018,0.949,0.989,0.141,0.265,0.989,0.215,0.02,0.993,0.013,1.0,0.013,0.969,0.996,0.916,1.0,0.016,0.063,0.804,0.283,0.17,0.999,0.038,0.89,0.006,0.991,0.953,0.349,0.016,0.986,0.998,0.999,0.367,0.997,0.999,0.007,0.036,0.117,0.871,0.902,0.997,0.933,0.964,0.942,0.997,0.994,0.257,0.691,1.0,0.004,0.992,0.062,1.0,0.238,0.9,0.008,1.0,0.993,0.002,0.01,0.052,0.995,0.869,0.994,0.11,1.0,0.008,0.002,0.999,0.026,0.136,0.448,0.001,0.973,0.553,0.971,0.11,0.956,0.859,0.882,0.065,0.991,0.005,0.704,0.965,0.853,0.956,0.998,0.024,0.998,0.973,0.998,0.65,1.0,0.956,0.751,0.971,0.988,1.0,0.85,0.353,0.013,0.013,0.244,0.05,0.917,0.956,0.002,0.26,0.971,0.604,0.994,0.999,0.048,0.999,0.033,0.342,0.014,0.004,0.999,0.998,0.01,0.719,0.999,1.0,1.0,0.834,0.584,0.999,0.992,0.038,0.995,0.007,0.999,0.014,0.578,0.485,1.0,0.514,0.997,0.272,0.009,0.002,0.009,0.999,1.0,0.026,0.987,0.005,0.098,0.997,0.996,0.954,0.024,0.998,0.186,1.0,0.999,0.139,0.961,0.998,0.002,0.163,0.741,0.227,0.969,0.999,0.817,0.082,0.127,0.001,0.002,0.895,0.014,0.246,0.988,0.962,0.999,0.394,0.0,0.975,0.009,1.0,0.004,0.999,0.003,0.927,0.771,0.99,0.004,0.02,0.967,0.015,0.97,0.999,0.384,0.011,0.637,0.972,1.0,0.066,0.991,0.088,0.012,0.172,0.309,1.0,0.048,0.944,0.221,0.013,0.5,0.996,0.09,0.414,0.406,0.1,0.004,1.0,0.024,0.813,0.035,0.951,0.997,0.999,0.999,0.036,0.999,0.469,0.047,0.809,0.139,0.052,0.637,0.994,0.122,0.957,1.0,0.01,0.045,0.059,0.903,0.13,0.19,0.999,0.003,0.459,0.966,0.999,0.899,0.999,0.787,0.041,0.07,0.008,0.829,0.21,0.114,0.033,0.062,0.02,0.041,0.995,0.025,0.059,0.077,0.027,0.376,0.995,0.405,0.36,0.371,0.183,0.09,0.016,0.727,0.041,0.875,0.033,0.007,0.997,0.102,0.739,0.073,0.649,0.075,0.999,0.28,0.979,0.764,0.115,0.039,0.017,0.725,0.021,0.813,0.77,0.008,0.185,0.067,0.912,0.875,0.015,0.978,0.992,0.072,0.557,0.004,0.372,0.189,0.976,1.0,0.938,0.382,0.113,0.152,0.092,0.948,0.989,0.104,0.038,0.034,0.999,0.117,0.004,0.007,0.397,0.975,0.17,0.009,0.363,0.947,0.008,0.066,0.992,0.132,0.997,0.844,0.006,0.003,0.939,0.367,0.561,0.06,0.999,0.95,0.011,0.032,0.885,0.032,0.399,1.0,0.061,0.036,0.999,0.034,0.091,0.996,1.0,0.994,0.949,0.613,0.965,0.997,0.076,0.984,0.273,0.997,0.013,0.982,0.996,0.261,0.28,0.994,0.998,0.089,0.496,0.265,0.968,1.0,1.0,0.579,1.0,0.843,0.999,0.988,0.932,0.35,0.402,0.005,0.984,0.021,0.996,0.149,0.996,0.111,0.783,0.077,0.231,0.998,0.953,0.444,1.0,1.0,0.784,0.008,0.998,0.17,0.969,0.999,0.001,1.0,0.05,0.016,0.899,0.189,0.983,1.0,0.832,0.977,0.996,0.001,0.005,0.006,0.783,0.343,0.008,0.202,0.989,0.131,0.103,1.0,0.041,0.992,0.966,1.0,0.996,0.344,0.015,0.999,0.001,0.119,0.975,1.0,0.959,0.006,0.104,0.888,0.998,0.863,0.995,0.964,0.001,0.927,0.004,0.009,0.999,0.102,0.976,0.015,0.705,0.995,0.016,0.97,0.133,0.945,0.996,1.0,0.993,0.877,0.997,0.1,0.997,0.33,0.019,0.997,0.976,0.999,0.808,0.978,0.964,0.001,0.021,0.008,0.038,0.698,0.051,0.001,0.996,0.989,0.023,0.975,0.999,0.035,0.001,0.997,0.014,0.945,0.315,0.997,0.0,0.999,0.001,0.006,0.004,0.008,0.904,0.644,0.007,0.168,0.104,0.77,0.992,0.921,0.992,0.999,0.245,0.917,0.995,0.003,0.073,0.006,0.99,0.997,0.979,0.999,0.998,0.986,0.999,0.996,0.958,0.132,0.961,0.966,0.882,0.997,0.002,0.019,0.999,0.999,0.079,0.97,0.95,0.999,0.995,1.0,0.897,0.985,0.994,0.994,0.999,0.614,0.985,0.446,0.412,0.275,0.391,0.898,0.003,0.995,0.987,0.022,0.156,0.988,0.034,0.993,0.961,0.989,0.255,0.997,0.122,0.108,0.998,0.993,0.04,0.978,0.001,0.134,0.123,0.054,0.018,0.998,0.994,0.083,0.999,0.997,0.665,0.64,0.987,0.139,0.956,0.988,0.776,0.999,0.998,0.847,0.557,0.992,0.803,0.95,0.862,0.626,0.091,0.028,0.004,0.014,0.897,1.0,0.99,0.003,0.847,0.72,0.01,0.858,0.973,0.035,0.004,0.001,0.099,0.448,0.999,0.978,0.025,0.99,0.212,0.039,0.096,0.121,0.43,0.173,0.984,0.748,0.088,0.985,1.0,0.018,0.014,0.229,0.752,0.021,0.283,0.313,0.994,0.631,0.048,0.007,0.124,0.024,0.999,0.65,0.148,0.051,0.964,0.917,0.803,0.017,0.198,0.271,0.991,0.962,0.998,0.999,0.067,0.509,0.982,0.21,0.952,0.026,0.995,0.858,0.977,0.991,0.107,0.001,0.0,0.113,0.999,0.079,0.031,0.978,0.067,0.993,0.033,0.003,0.183,0.009,0.006,0.756,0.989,0.253,0.989,0.019,0.475,0.041,0.416,0.92,0.938,0.938,0.38,0.073,0.619,0.813,0.96,0.157,0.815,0.876,0.055,0.631,0.638,0.88,0.005,0.896,0.079,0.934,0.24,0.848,0.001,0.49,0.217,0.883,0.032,0.579,0.298,0.462,0.92,0.001,0.971,0.696,0.935,0.019,0.107,0.059,0.044,0.039,0.397,0.702,0.104,0.751,0.435,0.782,0.011,0.021,0.94,0.769,0.982,0.843,0.086,0.974,0.71,0.003,0.877,0.011,0.991,0.054,0.022,0.884,0.991,0.996,0.968,0.998,0.647,0.971,0.569,0.386,0.846,0.651,0.976,0.966,0.991,0.346,0.318,0.969,0.995,0.194,0.975,0.013,0.574,0.828,0.372,0.121,0.547,0.363,0.992,0.83,0.983,0.029,0.933,0.985,0.999,0.998,0.992,0.962,0.905,0.002,0.962,0.005,0.065,0.99,0.328,0.081,0.022,0.966,0.73,0.003,0.969,0.217,0.995,0.888,0.049,0.33,0.599,0.004,0.305,0.003,0.725,0.798,0.078,0.034,0.086,0.992,0.004,0.962,0.936,0.011,0.804,0.011,0.735,0.039,0.008,0.984,0.96,0.02,0.765,0.669,0.017,0.648,0.99,0.404,0.621,0.915,0.985,0.991,0.994,0.008,0.626,0.907,0.015,0.029,0.047,0.001,0.753,0.117,0.089,0.01,0.009,0.112,0.814,0.99,0.002,0.035,0.047,0.133,0.047,0.189,0.002,0.095,0.47,0.065,0.249,0.997,0.1,0.079,0.128,0.088,0.602,0.237,0.992,0.294,0.406,0.208,0.268,0.722,0.161,0.64,0.029,0.666,0.912,0.05,0.042,0.026,0.048,0.175,0.687,0.998,0.906,0.218,0.444,0.996,0.985,0.156,0.995,0.029,0.284,0.182,0.035,0.363,0.983,0.689,0.593,0.021,0.004,0.052,0.018,0.724,0.736,0.25,0.005,0.229,0.099,0.406,0.041,0.717,0.541,0.621,0.819,0.944,0.762,0.027,0.862,0.98,0.399,0.965,0.277,0.04,0.017,0.561,0.225,0.04,0.014,0.034,0.109,0.009,0.044,0.481,0.913,0.745,0.999,0.992,0.034,0.089,0.39,0.869,0.483,0.021,0.115,0.487,0.142,0.673,0.629,0.645,0.387,0.009,0.771,0.314,0.464,0.014,0.087,0.039,0.025,0.85,0.012,0.271,0.996,0.05,0.036,0.602,0.385,0.991,0.692,0.001,0.104,0.14,0.898,0.428,0.015,0.017,0.962,0.85,0.114,0.085,0.015,0.005,0.041,0.028,0.029,0.163,0.827,0.27,0.449,0.002,0.015,0.005,0.061,0.831,0.995,0.023,0.826,0.006,0.971,0.966,0.998,0.982,0.237,0.018,0.001,0.039,0.085,0.544,0.153,0.329,0.428,0.849,0.209,0.144,0.931,0.691,0.382,0.177,0.019,0.603,0.169,0.016,0.978,0.052,0.647,0.841,0.808,0.827,0.89,0.003,0.135,0.002,0.005,0.993,0.887,0.008,0.795,0.018,0.974,0.415,0.515,0.019,0.037,0.548,0.023,0.008,0.064,0.87,0.767,0.272,0.687,0.732,0.966,0.004,0.43,0.912,0.365,0.137,0.026,0.125,0.176,0.021,0.78,0.037,0.173,0.895,0.011,0.167,0.123,0.817,0.042,0.832,0.148,0.131,0.154,0.745,0.025,0.846,0.068,0.007,0.007,0.056,0.901,0.39,0.181,0.082,0.994,0.019,0.938,0.328,0.995,0.068,0.302,0.013,0.504,0.97,0.973,0.026,0.992,0.914,0.665,0.721,0.923,0.918,0.942,0.637,0.008,0.422,0.06,0.009,0.099,0.863,0.748,0.252,0.053,0.016,0.002,0.081,0.047,0.005,0.048,0.13,0.292,0.919,0.902,0.036,0.121,0.594,0.467,0.391,0.04,0.268,0.048,0.85,0.196,0.035,0.584,0.366,0.12,0.895,0.031,0.085,0.994,0.04,0.002,0.108,0.362,0.091,0.152,0.871,0.021,0.945,0.338,0.532,0.038,0.982,0.556,0.967,0.843,0.353,0.103,0.81,0.461,0.09,0.071,0.533,0.111,0.025,0.019,0.003,0.989,0.013,0.052,0.014,0.79,0.229,0.15,0.043,0.033,0.297,0.948,0.237,0.02,0.078,0.011,0.657,0.468,0.993,0.005,0.206,0.996,0.001,0.07,0.007,0.004,0.986,0.936,0.017,0.128,0.983,0.1,0.148,0.12,0.502,0.847,0.073,0.73,0.007,0.062,0.949,0.006,0.02,0.495,0.758,0.144,0.218,0.996,0.872,0.454,0.07,0.131,0.773,0.13,0.004,0.002,0.401,0.732,0.931,0.836,0.055,0.099,0.004,0.995,0.003,0.07,0.046,0.008,0.741,0.023,0.427,0.946,0.001,0.078,0.423,0.381,0.88,0.123'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub14 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub14['pred'] = sub14['pred'].apply(lambda x: round(x,3))\n",
    "sub14_txt = ''\n",
    "for prob in list(sub14['pred'].values):\n",
    "    sub14_txt = sub14_txt+','+str(prob)\n",
    "sub14_txt = sub14_txt[1:]\n",
    "\n",
    "sub14_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "375bc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf1 = pred_train.copy()\n",
    "pred_val_rf1 = pred_val.copy()\n",
    "pred_test_rf1 = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174e7c0",
   "metadata": {},
   "source": [
    "### Submission 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc5b7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=200,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_train_under_imputed,y_train_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8b471e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9876854583809317\n",
      "Val ROC:  0.8522538821046284\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822a4f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.124405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124405</td>\n",
       "      <td>0.276798</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.431837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.124405              0.0        0.124405       0.276798   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.155039      0.431837  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "893e773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.939,0.994,1.0,1.0,0.999,0.361,0.005,0.981,0.997,0.005,0.001,0.999,1.0,0.974,1.0,0.02,0.007,0.982,0.997,1.0,0.998,1.0,1.0,1.0,0.986,0.997,1.0,0.981,0.997,1.0,0.994,0.994,0.982,0.992,1.0,0.864,0.96,0.997,0.963,0.982,0.993,0.994,1.0,1.0,0.994,1.0,0.293,0.029,0.966,0.019,1.0,0.99,0.004,0.023,0.003,0.999,1.0,0.998,1.0,1.0,0.435,0.009,0.997,1.0,0.975,1.0,0.998,0.991,1.0,1.0,0.99,0.999,0.998,0.829,0.904,0.999,1.0,0.995,0.987,0.002,0.714,0.999,0.086,1.0,0.981,0.849,1.0,0.994,0.997,1.0,0.999,0.933,0.998,0.482,0.998,0.997,0.932,0.714,1.0,0.949,0.997,1.0,0.999,0.886,0.463,0.312,0.113,0.983,0.336,0.996,1.0,0.996,1.0,0.501,0.999,0.984,0.998,0.994,0.999,1.0,1.0,0.581,0.859,0.995,0.964,1.0,1.0,0.002,0.0,0.999,0.842,0.008,0.998,0.994,1.0,0.758,0.999,0.91,0.999,0.105,0.557,0.993,0.981,0.907,1.0,0.803,0.995,0.019,0.997,1.0,0.996,0.998,1.0,0.401,0.999,0.695,0.996,0.999,0.9,1.0,0.968,0.997,0.899,1.0,0.995,0.85,0.997,0.999,0.074,1.0,0.03,0.749,1.0,0.994,0.838,0.999,0.967,0.215,0.936,0.067,0.002,0.997,0.006,0.111,1.0,0.999,1.0,1.0,0.996,1.0,0.001,0.34,0.954,0.935,0.0,0.999,0.053,1.0,0.628,0.979,1.0,1.0,0.998,0.866,0.523,0.093,0.578,0.075,0.001,1.0,0.998,0.095,1.0,0.997,0.129,1.0,1.0,0.654,0.999,0.99,0.998,0.051,0.999,1.0,0.08,0.009,0.998,0.041,0.192,0.004,0.965,0.793,1.0,0.041,1.0,0.999,0.992,1.0,1.0,0.004,0.999,0.971,1.0,1.0,0.014,0.894,0.006,0.996,0.972,0.907,0.002,0.999,1.0,0.012,0.95,1.0,1.0,0.06,0.016,1.0,0.894,0.012,1.0,1.0,1.0,0.943,1.0,0.995,0.002,0.99,0.995,1.0,0.046,0.992,0.458,1.0,0.999,1.0,0.981,0.997,0.988,0.002,1.0,0.464,0.001,0.999,0.997,0.581,0.999,0.032,0.017,0.011,0.998,0.924,0.026,0.998,0.003,0.96,1.0,1.0,0.994,0.997,0.998,1.0,1.0,0.987,1.0,1.0,1.0,1.0,0.999,0.004,1.0,0.97,0.0,1.0,0.034,0.757,0.212,0.991,0.998,0.011,0.615,0.999,0.959,0.021,0.835,0.966,0.735,0.04,0.999,0.052,1.0,0.711,0.959,1.0,0.999,0.0,0.937,0.996,0.99,0.588,0.999,0.996,0.998,0.961,0.002,0.744,0.999,0.002,0.011,0.992,0.999,0.027,1.0,0.841,0.999,1.0,1.0,1.0,0.999,0.001,0.994,0.914,0.995,0.003,1.0,1.0,0.034,1.0,0.993,0.996,1.0,0.993,0.993,0.994,1.0,0.999,0.912,0.895,0.999,0.014,0.646,0.998,0.999,0.0,1.0,1.0,0.99,0.088,0.999,0.169,1.0,0.997,1.0,1.0,1.0,0.998,0.997,0.985,1.0,0.999,0.7,1.0,0.941,0.002,0.001,0.977,0.023,0.008,0.029,0.983,0.323,0.997,0.002,0.999,0.0,0.263,0.949,1.0,0.877,0.526,1.0,0.023,0.024,0.466,0.097,0.999,0.967,0.935,0.003,0.004,1.0,0.903,0.999,0.999,0.922,0.004,0.999,0.016,0.998,1.0,0.685,1.0,0.828,0.208,1.0,0.004,0.083,0.991,0.988,0.003,0.998,0.869,0.997,1.0,1.0,0.998,1.0,1.0,0.023,0.967,0.95,0.955,0.999,0.003,0.762,0.003,0.994,0.444,0.477,0.006,0.999,0.872,0.973,0.999,0.998,0.003,0.768,0.14,0.993,1.0,0.006,0.001,0.001,0.998,0.987,0.994,0.004,0.001,0.888,0.201,0.911,0.22,1.0,1.0,0.452,0.204,0.001,1.0,0.985,1.0,0.986,0.0,1.0,0.467,1.0,1.0,0.05,0.994,0.994,0.987,0.827,0.035,0.002,0.006,0.004,0.997,0.001,0.487,0.138,0.002,0.057,0.998,0.997,0.996,1.0,1.0,0.997,0.001,1.0,0.632,0.001,1.0,0.695,0.591,0.001,0.972,0.759,0.004,0.999,0.014,0.007,0.999,0.103,0.001,1.0,0.086,0.765,1.0,1.0,0.999,0.998,0.999,0.999,0.001,0.993,0.003,0.994,0.999,0.003,0.971,0.999,0.001,1.0,0.926,0.059,0.519,0.962,1.0,0.994,0.001,0.01,0.999,0.998,0.95,0.985,0.999,1.0,1.0,0.023,0.999,0.999,0.555,1.0,0.999,0.178,0.997,0.957,0.96,1.0,0.973,0.028,0.998,0.941,1.0,0.002,0.001,0.998,0.985,1.0,0.916,0.999,0.048,0.84,0.121,1.0,1.0,0.97,1.0,0.999,0.0,0.564,0.007,0.999,0.988,0.009,0.002,0.0,1.0,0.976,0.997,1.0,0.998,0.052,0.003,1.0,0.999,1.0,0.328,0.985,0.486,0.001,0.991,0.048,0.008,0.005,0.998,1.0,1.0,0.996,0.998,0.442,1.0,0.003,0.033,0.882,1.0,0.954,0.998,0.4,0.003,0.896,1.0,0.998,0.999,0.037,0.999,0.999,0.949,0.896,0.004,0.045,0.002,0.004,0.997,1.0,1.0,0.971,0.227,0.989,0.96,0.339,0.847,0.941,1.0,0.992,0.08,0.001,0.18,0.037,0.712,0.998,0.917,0.999,0.001,0.909,0.966,0.795,0.006,0.008,0.988,0.982,0.955,0.785,1.0,1.0,0.002,0.999,0.98,0.998,0.012,1.0,0.994,0.998,1.0,0.023,1.0,0.126,1.0,0.034,0.996,0.0,0.998,0.796,0.991,1.0,0.887,0.045,0.918,0.81,1.0,0.019,0.999,0.878,0.016,1.0,0.001,0.034,1.0,0.959,0.994,0.999,0.682,0.996,1.0,0.996,1.0,1.0,1.0,0.998,0.996,1.0,1.0,0.054,0.678,0.999,0.06,0.996,1.0,0.007,0.383,0.998,0.229,0.866,1.0,0.005,0.232,0.997,0.999,0.999,0.004,0.836,1.0,0.994,0.639,0.018,0.995,0.121,0.003,0.996,1.0,0.006,0.071,0.008,0.008,0.958,1.0,0.957,0.999,0.064,0.973,0.862,1.0,0.062,0.043,0.676,1.0,0.01,0.056,0.999,0.255,0.778,0.809,0.032,0.997,0.03,1.0,0.962,0.001,0.003,0.997,0.0,0.414,1.0,0.948,0.983,1.0,1.0,1.0,0.018,0.971,0.598,0.044,0.972,1.0,1.0,0.965,1.0,1.0,1.0,0.457,0.466,0.962,0.288,0.994,0.871,0.008,0.999,0.996,0.998,0.996,0.064,0.032,0.951,0.134,0.0,0.981,0.0,1.0,0.978,1.0,0.999,0.989,0.997,1.0,0.293,0.999,0.957,0.841,0.994,0.984,0.008,0.004,1.0,0.752,0.027,1.0,0.994,0.999,1.0,0.02,1.0,0.017,0.77,0.005,0.99,0.97,0.107,0.999,0.893,0.999,0.463,0.866,0.992,1.0,1.0,0.007,0.1,0.387,0.033,1.0,0.996,0.74,0.079,1.0,0.355,0.229,0.194,1.0,0.164,0.01,0.984,1.0,0.998,0.874,0.101,0.001,0.175,0.041,0.957,0.835,0.111,0.103,0.997,0.035,0.027,0.769,0.232,0.088,0.02,1.0,1.0,0.002,0.968,1.0,0.999,0.195,0.999,0.992,0.048,0.999,0.598,0.925,0.003,0.998,0.014,0.106,0.003,0.984,0.999,0.999,1.0,0.014,0.999,0.992,0.001,0.998,0.673,1.0,0.005,0.128,1.0,1.0,1.0,0.97,1.0,0.998,0.0,1.0,0.296,0.0,1.0,0.526,1.0,1.0,1.0,0.999,0.439,1.0,0.765,0.038,0.996,0.997,0.999,0.999,0.291,1.0,0.024,0.962,0.003,0.001,0.999,1.0,0.99,0.999,0.993,0.98,0.009,1.0,0.948,0.981,0.013,0.995,0.044,0.997,0.959,0.998,1.0,0.999,1.0,0.986,0.997,0.004,0.999,0.559,0.984,0.977,0.999,1.0,0.998,0.062,1.0,0.997,0.925,0.874,0.018,0.992,0.998,0.137,1.0,0.857,0.96,0.987,0.034,0.102,1.0,0.009,1.0,0.007,0.956,0.05,0.999,0.001,0.963,0.001,0.008,0.009,0.998,0.999,0.999,1.0,0.233,0.999,0.052,0.884,0.003,0.747,0.944,0.004,0.021,0.013,0.642,0.002,0.0,1.0,1.0,0.696,0.999,1.0,0.033,0.967,0.766,0.965,0.997,1.0,0.0,0.474,0.884,0.209,0.109,1.0,0.999,0.575,1.0,0.993,0.995,0.99,0.11,0.948,0.029,0.998,0.996,0.89,0.999,0.912,1.0,0.04,1.0,0.018,0.25,0.295,0.275,0.095,0.987,0.085,0.92,0.451,0.003,0.015,0.222,0.895,0.001,0.008,0.679,0.872,0.998,0.872,1.0,0.425,0.014,0.05,0.202,0.953,0.033,0.305,0.723,0.006,0.282,0.052,0.997,0.999,0.742,0.014,0.454,0.006,0.996,0.013,0.575,0.002,1.0,0.275,0.007,0.977,0.997,0.001,0.339,0.998,0.481,0.001,0.992,0.002,1.0,0.016,0.995,0.966,0.333,1.0,0.003,0.013,0.971,0.347,0.205,1.0,0.023,0.717,0.017,0.991,0.991,0.122,0.005,0.978,1.0,1.0,0.035,0.993,1.0,0.053,0.002,0.006,0.945,0.998,0.987,0.971,0.999,0.995,1.0,0.996,0.347,0.228,1.0,0.001,0.967,0.002,1.0,0.032,0.201,0.003,1.0,0.994,0.008,0.0,0.02,1.0,0.904,0.949,0.001,1.0,0.044,0.0,0.999,0.149,0.01,0.702,0.002,0.981,0.299,0.738,0.069,0.99,0.89,0.962,0.051,0.997,0.002,0.755,0.989,0.938,0.986,0.998,0.001,0.997,0.999,0.998,0.473,1.0,0.923,0.729,0.952,0.999,1.0,0.966,0.16,0.004,0.025,0.604,0.088,0.968,0.997,0.001,0.164,0.999,0.178,1.0,1.0,0.145,1.0,0.0,0.096,0.0,0.001,1.0,0.998,0.002,0.815,1.0,1.0,0.998,0.998,0.952,1.0,0.997,0.107,0.995,0.011,0.999,0.014,0.836,0.09,0.999,0.949,1.0,0.024,0.013,0.001,0.008,1.0,1.0,0.008,0.996,0.004,0.197,0.99,0.999,0.924,0.001,0.998,0.063,1.0,0.999,0.018,0.997,1.0,0.006,0.782,0.96,0.111,0.987,0.981,0.803,0.117,0.181,0.0,0.0,0.981,0.001,0.429,0.972,0.97,1.0,0.545,0.0,0.994,0.071,0.999,0.159,1.0,0.0,0.913,0.788,0.999,0.035,0.007,0.393,0.051,0.859,1.0,0.132,0.004,0.136,0.998,1.0,0.055,0.995,0.058,0.003,0.008,0.076,0.999,0.001,0.996,0.387,0.01,0.091,0.999,0.004,0.829,0.93,0.017,0.002,1.0,0.004,0.598,0.028,0.912,0.995,1.0,0.999,0.025,0.999,0.269,0.017,0.251,0.807,0.003,0.339,0.487,0.129,0.997,1.0,0.012,0.03,0.043,0.993,0.219,0.088,1.0,0.003,0.665,0.998,0.999,0.085,1.0,0.919,0.033,0.129,0.012,0.257,0.026,0.418,0.472,0.033,0.02,0.406,0.999,0.183,0.013,0.018,0.1,0.653,0.999,0.066,0.224,0.057,0.018,0.037,0.031,0.783,0.155,0.928,0.246,0.051,0.997,0.209,0.059,0.413,0.419,0.07,1.0,0.181,0.9,0.245,0.03,0.093,0.002,0.715,0.013,0.817,0.935,0.036,0.767,0.031,0.988,0.979,0.106,0.998,0.913,0.019,0.044,0.003,0.561,0.008,0.963,1.0,0.98,0.064,0.174,0.313,0.109,0.991,0.958,0.192,0.039,0.001,1.0,0.948,0.007,0.0,0.622,0.998,0.206,0.016,0.218,0.86,0.023,0.131,0.999,0.009,0.995,0.883,0.111,0.034,0.996,0.006,0.179,0.291,0.999,0.919,0.015,0.602,0.343,0.171,0.869,1.0,0.136,0.002,1.0,0.009,0.519,0.984,1.0,0.998,0.989,0.145,0.998,0.998,0.011,0.89,0.095,0.933,0.021,0.66,0.998,0.379,0.013,0.978,0.999,0.462,0.751,0.084,0.996,1.0,1.0,0.391,1.0,0.913,0.998,0.84,0.992,0.077,0.257,0.0,0.987,0.001,0.99,0.062,0.994,0.823,0.709,0.539,0.028,1.0,0.996,0.238,1.0,1.0,0.987,0.001,1.0,0.088,0.924,0.993,0.001,1.0,0.031,0.0,0.976,0.012,0.943,1.0,0.696,0.992,0.999,0.043,0.008,0.011,0.542,0.024,0.0,0.099,0.992,0.119,0.01,1.0,0.049,0.991,0.923,1.0,0.989,0.401,0.015,0.999,0.0,0.488,0.985,1.0,0.996,0.018,0.699,0.962,0.999,0.982,0.969,0.987,0.0,0.98,0.001,0.144,0.988,0.141,0.953,0.016,0.868,0.999,0.001,0.979,0.034,0.863,0.998,0.999,0.848,0.778,1.0,0.014,0.988,0.015,0.004,0.973,1.0,0.999,0.273,0.962,0.988,0.001,0.099,0.025,0.002,0.892,0.027,0.0,0.997,0.97,0.033,0.98,1.0,0.004,0.02,0.928,0.059,0.929,0.32,0.999,0.091,1.0,0.002,0.006,0.002,0.194,0.979,0.099,0.001,0.009,0.035,0.252,0.987,0.932,0.99,0.999,0.187,0.879,0.991,0.001,0.022,0.003,0.993,1.0,0.917,0.996,0.997,0.939,0.999,0.993,0.979,0.006,0.997,0.932,0.969,0.986,0.002,0.0,0.999,0.998,0.022,0.915,0.524,0.999,0.996,0.999,0.613,0.959,0.993,0.996,0.999,0.677,0.999,0.188,0.02,0.766,0.649,0.94,0.002,0.996,0.993,0.002,0.015,1.0,0.002,1.0,0.996,0.995,0.768,0.997,0.058,0.002,1.0,0.946,0.002,0.97,0.001,0.03,0.066,0.015,0.315,1.0,0.998,0.096,1.0,0.995,0.995,0.752,0.975,0.004,0.952,0.998,0.893,1.0,0.999,0.951,0.854,0.99,0.832,0.989,0.241,0.806,0.056,0.005,0.003,0.01,0.082,1.0,0.998,0.0,0.972,0.777,0.017,0.466,0.951,0.076,0.003,0.002,0.005,0.368,1.0,0.986,0.004,0.994,0.45,0.755,0.055,0.008,0.006,0.002,0.985,0.067,0.165,0.993,1.0,0.008,0.012,0.01,0.851,0.005,0.162,0.043,0.999,0.888,0.003,0.017,0.009,0.007,0.986,0.958,0.017,0.051,0.973,0.987,0.96,0.046,0.746,0.67,0.991,0.986,0.999,0.997,0.082,0.607,0.996,0.364,0.929,0.008,0.95,0.996,0.99,0.78,0.082,0.0,0.0,0.063,0.987,0.009,0.007,0.679,0.129,0.949,0.043,0.003,0.144,0.012,0.002,0.886,0.411,0.041,0.411,0.375,0.003,0.031,0.349,0.891,0.596,0.418,0.184,0.675,0.794,0.658,0.703,0.028,0.543,0.997,0.001,0.52,0.501,0.584,0.006,0.772,0.365,0.986,0.273,0.287,0.004,0.053,0.718,0.951,0.047,0.618,0.436,0.112,0.936,0.001,0.963,0.668,0.606,0.0,0.103,0.0,0.6,0.051,0.56,0.13,0.481,0.805,0.013,0.74,0.028,0.083,0.56,0.83,0.995,0.925,0.221,0.898,0.055,0.005,0.089,0.001,0.608,0.054,0.002,0.699,0.949,0.987,0.997,1.0,0.963,0.99,0.281,0.44,0.561,0.887,0.994,0.998,0.93,0.033,0.958,0.903,0.933,0.043,0.905,0.015,0.051,0.8,0.059,0.234,0.172,0.587,0.993,0.939,0.996,0.039,0.902,0.834,0.998,0.992,0.919,0.983,0.504,0.02,0.376,0.063,0.013,0.979,0.221,0.774,0.048,0.949,0.889,0.022,0.986,0.212,0.82,0.949,0.021,0.22,0.681,0.22,0.222,0.004,0.694,0.713,0.227,0.034,0.025,0.971,0.007,0.985,0.802,0.024,0.978,0.002,0.944,0.111,0.004,0.862,0.949,0.048,0.432,0.759,0.022,0.954,0.997,0.668,0.134,0.846,0.993,0.992,0.999,0.003,0.993,0.89,0.011,0.016,0.077,0.0,0.931,0.127,0.011,0.126,0.001,0.03,0.965,0.982,0.003,0.178,0.023,0.024,0.554,0.273,0.004,0.004,0.884,0.389,0.367,0.992,0.578,0.207,0.013,0.016,0.488,0.069,0.995,0.397,0.219,0.206,0.804,0.693,0.059,0.472,0.017,0.823,0.776,0.078,0.007,0.07,0.011,0.901,0.52,0.86,0.991,0.188,0.066,0.977,0.24,0.344,0.999,0.007,0.46,0.187,0.013,0.063,0.763,0.163,0.226,0.077,0.005,0.024,0.031,0.745,0.466,0.414,0.006,0.03,0.622,0.653,0.037,0.993,0.493,0.731,0.296,0.983,0.274,0.024,0.964,0.972,0.601,0.992,0.415,0.025,0.079,0.578,0.014,0.45,0.053,0.326,0.274,0.113,0.042,0.466,0.787,0.895,0.996,0.979,0.01,0.356,0.717,0.91,0.122,0.041,0.106,0.279,0.018,0.704,0.013,0.803,0.034,0.143,0.065,0.557,0.027,0.026,0.02,0.006,0.003,0.855,0.02,0.126,0.996,0.068,0.021,0.863,0.679,0.998,0.579,0.0,0.086,0.251,0.916,0.532,0.009,0.378,0.981,0.98,0.313,0.947,0.003,0.001,0.028,0.014,0.62,0.294,0.244,0.156,0.48,0.001,0.021,0.012,0.258,0.303,0.996,0.004,0.656,0.09,0.954,0.992,0.994,0.985,0.329,0.003,0.001,0.755,0.106,0.915,0.048,0.061,0.994,0.612,0.208,0.43,0.982,0.739,0.374,0.015,0.095,0.99,0.075,0.039,0.998,0.231,0.878,0.524,0.111,0.302,0.823,0.097,0.706,0.001,0.093,0.987,0.977,0.081,0.958,0.001,0.995,0.927,0.015,0.021,0.063,0.147,0.041,0.348,0.094,0.861,0.084,0.528,0.305,0.985,0.893,0.035,0.76,0.868,0.372,0.128,0.168,0.127,0.551,0.085,0.711,0.151,0.18,0.801,0.036,0.091,0.144,0.314,0.618,0.394,0.039,0.18,0.631,0.271,0.033,0.899,0.019,0.041,0.001,0.017,0.943,0.882,0.293,0.691,0.993,0.007,0.957,0.932,0.999,0.013,0.589,0.002,0.798,0.963,0.826,0.004,0.997,0.401,0.928,0.32,0.253,0.04,0.773,0.495,0.011,0.02,0.005,0.054,0.003,0.985,0.212,0.181,0.082,0.135,0.001,0.144,0.021,0.027,0.002,0.003,0.244,0.939,0.971,0.625,0.021,0.307,0.188,0.505,0.009,0.219,0.005,0.865,0.413,0.104,0.172,0.204,0.072,0.358,0.086,0.703,0.944,0.003,0.019,0.792,0.891,0.908,0.105,0.905,0.329,0.332,0.073,0.438,0.665,0.787,0.813,0.994,0.935,0.061,0.094,0.8,0.429,0.115,0.018,0.08,0.047,0.003,0.028,0.001,0.934,0.005,0.473,0.204,0.908,0.876,0.136,0.185,0.005,0.244,0.975,0.047,0.004,0.054,0.01,0.032,0.456,0.999,0.004,0.547,0.998,0.001,0.021,0.004,0.007,0.711,0.886,0.258,0.314,0.998,0.107,0.031,0.088,0.756,0.094,0.003,0.551,0.009,0.438,0.917,0.0,0.004,0.384,0.807,0.399,0.458,0.924,0.302,0.952,0.383,0.416,0.748,0.067,0.125,0.001,0.01,0.352,0.671,0.911,0.073,0.054,0.009,0.981,0.012,0.036,0.078,0.005,0.185,0.088,0.111,0.949,0.0,0.607,0.086,0.794,0.049,0.009'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub15 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub15['pred'] = sub15['pred'].apply(lambda x: round(x,3))\n",
    "sub15_txt = ''\n",
    "for prob in list(sub15['pred'].values):\n",
    "    sub15_txt = sub15_txt+','+str(prob)\n",
    "sub15_txt = sub15_txt[1:]\n",
    "\n",
    "sub15_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf21e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf2 = pred_train.copy()\n",
    "pred_val_rf2 = pred_val.copy()\n",
    "pred_test_rf2 = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2e8bf",
   "metadata": {},
   "source": [
    "### Submission 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183cbfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_flag</th>\n",
       "      <th>pred_x</th>\n",
       "      <th>pred_cold</th>\n",
       "      <th>pred_y</th>\n",
       "      <th>pred_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997364</td>\n",
       "      <td>0.997364</td>\n",
       "      <td>0.998875</td>\n",
       "      <td>0.998875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.939222</td>\n",
       "      <td>0.939222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.994372</td>\n",
       "      <td>0.994372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.999699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.996662</td>\n",
       "      <td>0.996662</td>\n",
       "      <td>0.999612</td>\n",
       "      <td>0.999612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>0</td>\n",
       "      <td>0.077783</td>\n",
       "      <td>0.077783</td>\n",
       "      <td>0.606835</td>\n",
       "      <td>0.606835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>0</td>\n",
       "      <td>0.422620</td>\n",
       "      <td>0.422620</td>\n",
       "      <td>0.086012</td>\n",
       "      <td>0.086012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>0</td>\n",
       "      <td>0.380592</td>\n",
       "      <td>0.380592</td>\n",
       "      <td>0.794154</td>\n",
       "      <td>0.794154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>0</td>\n",
       "      <td>0.880442</td>\n",
       "      <td>0.880442</td>\n",
       "      <td>0.049052</td>\n",
       "      <td>0.049052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.123080</td>\n",
       "      <td>0.123080</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.009409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           experience_flag    pred_x  pred_cold    pred_y  pred_exp\n",
       "unique_id                                                          \n",
       "9                        1  0.997364   0.997364  0.998875  0.998875\n",
       "18                       1  0.964852   0.964852  0.939222  0.939222\n",
       "21                       1  0.994150   0.994150  0.994372  0.994372\n",
       "25                       1  0.998056   0.998056  0.999699  0.999699\n",
       "31                       1  0.996662   0.996662  0.999612  0.999612\n",
       "...                    ...       ...        ...       ...       ...\n",
       "7982                     0  0.077783   0.077783  0.606835  0.606835\n",
       "7990                     0  0.422620   0.422620  0.086012  0.086012\n",
       "7993                     0  0.380592   0.380592  0.794154  0.794154\n",
       "7994                     0  0.880442   0.880442  0.049052  0.049052\n",
       "7998                     0  0.123080   0.123080  0.009409  0.009409\n",
       "\n",
       "[2380 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_rf1['pred_cold'] = pred_test_rf1['pred']\n",
    "pred_test_rf2['pred_exp'] = pred_test_rf2['pred']\n",
    "\n",
    "sub16_df = pd.merge(X_test[['experience_flag']],pred_test_rf1,how='left',right_index=True,left_index=True)\n",
    "sub16_df = pd.merge(sub16_df,pred_test_rf2,how='left',right_index=True,left_index=True)\n",
    "sub16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357a84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub16_df['pred'] = np.where(sub16_df['experience_flag']==1,sub16_df['pred_exp'],sub16_df['pred_cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20a5a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.939,0.994,1.0,1.0,0.999,0.361,0.005,0.981,0.997,0.005,0.001,0.999,1.0,0.974,1.0,0.02,0.007,0.982,0.997,1.0,0.998,1.0,1.0,1.0,0.986,0.997,1.0,0.981,0.997,1.0,0.994,0.994,0.982,0.992,1.0,0.546,0.96,0.997,0.963,0.982,0.993,0.998,1.0,1.0,0.994,1.0,0.293,0.56,0.966,0.019,1.0,0.99,0.004,0.009,0.003,0.999,1.0,0.998,1.0,1.0,0.435,0.009,0.997,1.0,0.759,1.0,0.998,0.991,1.0,1.0,0.99,0.999,0.998,0.829,0.99,0.999,1.0,0.995,0.987,0.002,0.714,0.999,0.086,1.0,0.981,0.849,1.0,0.994,0.997,1.0,0.999,0.933,0.998,0.482,0.998,0.997,0.932,0.714,1.0,0.949,0.997,1.0,0.999,0.886,0.348,0.775,0.32,0.983,0.336,0.996,1.0,0.996,1.0,0.931,0.999,0.984,0.998,0.994,0.999,1.0,1.0,0.519,0.859,0.995,0.964,1.0,1.0,0.002,0.0,0.999,0.842,0.008,0.998,0.994,1.0,0.758,0.999,0.993,0.999,0.105,0.557,0.993,0.981,0.907,1.0,0.803,0.995,0.019,0.997,1.0,0.996,0.998,1.0,0.401,0.974,0.695,0.996,0.999,0.9,1.0,0.968,0.997,0.971,1.0,0.995,0.85,0.997,0.999,0.212,1.0,0.03,0.749,1.0,0.994,0.838,0.999,0.967,0.113,0.936,0.067,0.002,0.997,0.215,0.111,1.0,0.999,1.0,1.0,0.996,1.0,0.001,0.34,0.954,0.935,0.0,0.999,0.031,1.0,0.628,0.979,1.0,1.0,0.998,0.971,0.523,0.093,0.578,0.075,0.001,1.0,0.998,0.095,1.0,0.997,0.129,1.0,1.0,0.654,0.999,0.99,0.998,0.051,0.999,1.0,0.048,0.009,0.998,0.157,0.192,0.004,0.965,0.916,1.0,0.041,1.0,0.999,0.992,1.0,1.0,0.004,0.999,0.971,1.0,1.0,0.029,0.894,0.006,0.996,0.972,0.907,0.002,0.999,1.0,0.012,0.95,1.0,1.0,0.06,0.016,1.0,0.894,0.012,1.0,1.0,1.0,0.743,1.0,0.995,0.002,0.99,0.995,1.0,0.046,0.992,0.99,1.0,0.999,1.0,0.981,0.997,0.988,0.002,1.0,0.464,0.001,0.999,0.997,0.614,0.999,0.032,0.017,0.011,0.998,0.968,0.026,0.998,0.003,0.085,1.0,1.0,0.994,0.997,0.998,1.0,1.0,0.987,1.0,1.0,1.0,1.0,0.999,0.004,1.0,0.97,0.0,1.0,0.096,0.757,0.212,0.991,0.998,0.001,0.615,0.999,0.959,0.266,0.835,0.606,0.904,0.04,0.999,0.052,1.0,0.711,0.804,1.0,0.999,0.0,0.937,0.996,0.99,0.588,0.999,0.996,0.998,0.961,0.002,0.744,0.999,0.002,0.011,0.992,0.999,0.053,1.0,0.841,0.999,1.0,1.0,1.0,0.999,0.006,0.994,0.914,0.995,0.011,1.0,1.0,0.034,1.0,0.993,0.996,1.0,0.998,0.993,0.994,1.0,0.999,0.912,0.895,0.999,0.014,0.973,0.998,0.999,0.0,1.0,1.0,0.99,0.088,0.999,0.278,1.0,0.997,1.0,1.0,1.0,0.998,0.997,0.994,1.0,0.999,0.928,1.0,0.91,0.018,0.001,0.977,0.023,0.008,0.007,0.983,0.463,0.96,0.002,0.999,0.0,0.263,0.749,1.0,0.877,0.526,1.0,0.023,0.024,0.072,0.081,0.999,0.967,0.982,0.003,0.004,1.0,0.903,0.999,0.999,0.922,0.004,0.999,0.016,0.998,1.0,0.685,1.0,0.543,0.208,1.0,0.004,0.088,0.991,0.988,0.014,0.998,0.931,0.997,1.0,1.0,0.998,1.0,1.0,0.023,0.967,0.996,0.955,0.999,0.005,0.762,0.003,0.994,0.347,0.586,0.006,0.999,0.872,0.991,0.999,0.998,0.005,0.768,0.14,0.993,1.0,0.006,0.001,0.003,0.998,0.987,0.994,0.004,0.001,0.888,0.049,0.911,0.22,1.0,1.0,0.452,0.204,0.001,1.0,0.985,1.0,0.961,0.0,1.0,0.467,1.0,1.0,0.449,0.994,0.994,0.987,0.827,0.035,0.002,0.006,0.015,0.968,0.009,0.487,0.138,0.001,0.036,0.998,0.997,0.996,1.0,1.0,0.997,0.001,1.0,0.632,0.001,1.0,0.695,0.591,0.001,0.972,0.859,0.004,0.999,0.082,0.007,0.999,0.017,0.001,1.0,0.027,0.129,1.0,1.0,0.999,0.998,0.999,0.999,0.001,0.993,0.017,0.994,0.999,0.003,0.971,0.999,0.001,1.0,0.926,0.059,0.051,0.962,1.0,0.836,0.001,0.01,0.999,0.998,0.95,0.985,0.999,1.0,1.0,0.023,0.999,0.999,0.555,1.0,0.999,0.787,0.997,0.957,0.96,1.0,0.973,0.028,0.969,0.941,1.0,0.002,0.002,0.998,0.985,1.0,0.916,0.999,0.017,0.84,0.06,1.0,1.0,0.97,1.0,0.999,0.0,0.535,0.007,0.999,0.988,0.009,0.066,0.002,1.0,0.995,0.997,1.0,0.998,0.052,0.003,1.0,0.999,1.0,0.847,0.985,0.064,0.001,0.991,0.612,0.026,0.005,0.998,1.0,1.0,0.996,0.998,0.442,1.0,0.003,0.036,0.882,1.0,0.954,0.998,0.139,0.003,0.896,1.0,0.998,0.995,0.037,0.999,0.999,0.949,0.896,0.004,0.055,0.065,0.076,0.997,1.0,1.0,0.988,0.119,0.989,0.96,0.022,0.847,0.941,1.0,0.992,0.08,0.03,0.18,0.037,0.706,0.998,0.917,0.999,0.001,0.941,0.919,0.928,0.006,0.008,0.988,0.982,0.955,0.785,1.0,1.0,0.009,0.999,0.98,0.998,0.36,1.0,0.996,0.998,1.0,0.017,1.0,0.126,1.0,0.043,0.996,0.005,0.998,0.796,0.991,1.0,0.887,0.045,0.918,0.478,1.0,0.019,0.999,0.878,0.031,1.0,0.001,0.034,1.0,0.959,0.994,0.999,0.682,0.919,1.0,0.996,1.0,1.0,1.0,0.998,0.996,1.0,1.0,0.078,0.678,0.999,0.06,0.996,1.0,0.007,0.383,0.998,0.156,0.866,1.0,0.006,0.128,0.998,0.999,0.999,0.004,0.874,1.0,0.994,0.973,0.025,0.995,0.121,0.003,0.996,1.0,0.005,0.142,0.078,0.07,0.958,1.0,0.957,0.999,0.191,0.973,0.862,1.0,0.252,0.043,0.676,1.0,0.01,0.017,0.999,0.255,0.778,0.809,0.044,0.997,0.03,1.0,0.962,0.002,0.003,0.997,0.0,0.414,1.0,0.948,0.983,1.0,1.0,1.0,0.007,0.971,0.598,0.044,0.972,1.0,1.0,0.965,1.0,1.0,1.0,0.457,0.872,0.922,0.066,0.994,0.871,0.009,0.999,0.996,0.998,0.996,0.018,0.031,0.951,0.21,0.0,0.981,0.008,1.0,0.978,1.0,0.999,0.989,0.997,1.0,0.609,0.999,0.957,0.783,0.994,0.984,0.029,0.013,1.0,0.97,0.027,1.0,0.996,0.999,1.0,0.04,1.0,0.029,0.77,0.005,0.99,0.97,0.107,0.999,0.893,0.999,0.536,0.866,0.992,1.0,1.0,0.007,0.028,0.387,0.075,1.0,0.996,0.74,0.079,1.0,0.73,0.229,0.015,1.0,0.164,0.004,0.91,1.0,0.998,0.874,0.101,0.001,0.12,0.039,0.957,0.417,0.111,0.005,0.997,0.035,0.027,0.769,0.232,0.088,0.02,1.0,1.0,0.032,0.968,1.0,0.999,0.652,0.999,0.992,0.048,0.995,0.598,0.879,0.003,0.998,0.045,0.106,0.003,0.984,0.999,0.999,1.0,0.097,0.999,0.992,0.001,0.998,0.673,1.0,0.048,0.128,1.0,1.0,1.0,0.97,1.0,0.998,0.0,1.0,0.296,0.0,1.0,0.932,1.0,1.0,1.0,1.0,0.439,1.0,0.145,0.038,0.996,0.997,0.999,0.991,0.018,1.0,0.024,0.962,0.001,0.001,0.999,1.0,0.994,0.999,0.993,0.98,0.009,1.0,0.948,0.981,0.297,0.97,0.069,0.997,0.798,0.994,1.0,0.999,1.0,0.975,0.997,0.004,0.999,0.559,0.984,0.977,0.999,1.0,0.971,0.075,1.0,0.997,0.925,0.874,0.018,0.992,0.978,0.137,1.0,0.857,0.96,0.987,0.015,0.102,1.0,0.094,1.0,0.007,0.956,0.069,0.999,0.001,0.443,0.007,0.051,0.038,0.998,0.999,0.999,1.0,0.38,0.999,0.115,0.884,0.003,0.916,0.944,0.004,0.062,0.028,0.642,0.002,0.004,1.0,1.0,0.19,0.999,1.0,0.13,0.893,0.766,0.984,0.997,1.0,0.0,0.163,0.884,0.209,0.245,1.0,0.999,0.448,1.0,0.993,0.995,0.965,0.11,0.948,0.029,0.998,0.996,0.891,0.999,0.912,1.0,0.04,0.999,0.138,0.639,0.028,0.116,0.069,0.987,0.086,0.92,0.451,0.003,0.032,0.183,0.895,0.002,0.041,0.679,0.872,0.998,0.872,1.0,0.425,0.005,0.008,0.67,0.953,0.072,0.592,0.755,0.032,0.011,0.121,0.997,0.999,0.823,0.014,0.195,0.006,0.996,0.066,0.575,0.002,1.0,0.095,0.018,0.949,0.997,0.141,0.265,0.998,0.215,0.02,0.993,0.002,1.0,0.013,0.969,0.966,0.916,1.0,0.016,0.063,0.971,0.283,0.17,1.0,0.023,0.89,0.017,0.991,0.953,0.349,0.005,0.978,1.0,1.0,0.367,0.993,1.0,0.007,0.002,0.117,0.945,0.998,0.987,0.971,0.964,0.942,1.0,0.994,0.257,0.691,1.0,0.001,0.992,0.062,1.0,0.238,0.9,0.003,1.0,0.994,0.002,0.0,0.052,1.0,0.904,0.994,0.11,1.0,0.008,0.0,0.999,0.026,0.136,0.702,0.002,0.973,0.553,0.971,0.11,0.956,0.859,0.882,0.065,0.997,0.005,0.704,0.989,0.853,0.986,0.998,0.024,0.997,0.973,0.998,0.65,1.0,0.923,0.729,0.952,0.988,1.0,0.85,0.353,0.013,0.013,0.244,0.088,0.968,0.956,0.001,0.26,0.999,0.604,0.994,1.0,0.048,1.0,0.033,0.342,0.0,0.004,1.0,0.998,0.01,0.719,1.0,1.0,0.998,0.998,0.584,1.0,0.992,0.038,0.995,0.011,0.999,0.014,0.578,0.485,0.999,0.949,1.0,0.024,0.009,0.002,0.009,1.0,1.0,0.026,0.996,0.005,0.098,0.99,0.999,0.924,0.001,0.998,0.186,1.0,0.999,0.139,0.997,1.0,0.002,0.163,0.741,0.111,0.969,0.999,0.817,0.082,0.127,0.001,0.0,0.895,0.001,0.246,0.988,0.97,1.0,0.394,0.0,0.994,0.009,0.999,0.004,1.0,0.003,0.913,0.771,0.999,0.004,0.02,0.967,0.015,0.859,1.0,0.384,0.004,0.136,0.972,1.0,0.055,0.991,0.088,0.003,0.172,0.309,0.999,0.048,0.996,0.221,0.013,0.5,0.999,0.09,0.829,0.406,0.1,0.002,1.0,0.004,0.813,0.035,0.951,0.997,1.0,0.999,0.025,0.999,0.469,0.047,0.809,0.139,0.052,0.637,0.994,0.129,0.997,1.0,0.01,0.045,0.059,0.993,0.13,0.088,1.0,0.003,0.665,0.966,0.999,0.899,1.0,0.787,0.033,0.07,0.008,0.829,0.21,0.114,0.033,0.062,0.02,0.041,0.999,0.025,0.059,0.018,0.027,0.376,0.999,0.405,0.36,0.371,0.183,0.09,0.016,0.727,0.041,0.875,0.033,0.007,0.997,0.209,0.739,0.073,0.649,0.075,1.0,0.181,0.979,0.764,0.115,0.039,0.017,0.725,0.021,0.813,0.935,0.036,0.767,0.067,0.912,0.875,0.015,0.998,0.992,0.072,0.557,0.004,0.372,0.008,0.963,1.0,0.938,0.382,0.113,0.152,0.092,0.991,0.989,0.104,0.038,0.034,1.0,0.117,0.007,0.0,0.397,0.975,0.17,0.009,0.363,0.86,0.023,0.066,0.992,0.132,0.995,0.883,0.006,0.003,0.996,0.367,0.561,0.291,0.999,0.919,0.015,0.032,0.885,0.032,0.399,1.0,0.061,0.002,0.999,0.009,0.091,0.996,1.0,0.998,0.989,0.613,0.998,0.998,0.011,0.89,0.273,0.997,0.021,0.982,0.996,0.261,0.28,0.978,0.999,0.089,0.751,0.265,0.968,1.0,1.0,0.579,1.0,0.843,0.998,0.988,0.932,0.35,0.402,0.0,0.987,0.021,0.996,0.062,0.996,0.111,0.783,0.077,0.231,1.0,0.996,0.444,1.0,1.0,0.987,0.008,1.0,0.17,0.924,0.999,0.001,1.0,0.031,0.0,0.976,0.189,0.943,1.0,0.832,0.992,0.996,0.043,0.008,0.011,0.783,0.024,0.008,0.202,0.992,0.119,0.01,1.0,0.049,0.992,0.923,1.0,0.989,0.344,0.015,0.999,0.0,0.119,0.985,1.0,0.996,0.006,0.104,0.962,0.999,0.982,0.969,0.987,0.0,0.927,0.001,0.009,0.988,0.141,0.953,0.016,0.705,0.995,0.016,0.97,0.133,0.945,0.996,0.999,0.993,0.877,1.0,0.1,0.988,0.33,0.019,0.997,1.0,0.999,0.808,0.978,0.988,0.001,0.021,0.008,0.038,0.698,0.051,0.001,0.997,0.989,0.023,0.98,1.0,0.035,0.001,0.997,0.059,0.929,0.32,0.999,0.0,1.0,0.001,0.006,0.004,0.194,0.979,0.644,0.007,0.168,0.104,0.252,0.992,0.921,0.992,0.999,0.245,0.917,0.991,0.003,0.022,0.006,0.993,0.997,0.917,0.999,0.998,0.986,0.999,0.993,0.979,0.132,0.961,0.932,0.882,0.997,0.002,0.0,0.999,0.999,0.079,0.97,0.524,0.999,0.996,0.999,0.897,0.985,0.994,0.994,0.999,0.614,0.985,0.188,0.412,0.275,0.391,0.94,0.003,0.995,0.987,0.022,0.156,0.988,0.002,0.993,0.961,0.989,0.255,0.997,0.122,0.108,0.998,0.993,0.04,0.978,0.001,0.134,0.123,0.054,0.018,0.998,0.994,0.083,1.0,0.997,0.665,0.64,0.987,0.139,0.956,0.988,0.776,0.999,0.998,0.847,0.854,0.992,0.803,0.95,0.862,0.626,0.091,0.028,0.003,0.014,0.897,1.0,0.99,0.003,0.847,0.72,0.01,0.858,0.973,0.035,0.004,0.001,0.099,0.448,0.999,0.978,0.025,0.99,0.212,0.039,0.096,0.008,0.006,0.173,0.984,0.748,0.088,0.985,1.0,0.018,0.014,0.229,0.752,0.021,0.283,0.313,0.994,0.888,0.048,0.007,0.124,0.024,0.999,0.958,0.148,0.051,0.964,0.917,0.803,0.017,0.198,0.271,0.991,0.962,0.998,0.999,0.067,0.509,0.982,0.21,0.952,0.026,0.995,0.858,0.977,0.991,0.107,0.001,0.0,0.113,0.999,0.079,0.031,0.978,0.067,0.993,0.033,0.003,0.183,0.009,0.006,0.756,0.989,0.253,0.989,0.019,0.475,0.041,0.416,0.92,0.938,0.938,0.38,0.073,0.619,0.813,0.703,0.157,0.815,0.876,0.055,0.631,0.638,0.88,0.005,0.896,0.079,0.934,0.24,0.848,0.001,0.49,0.217,0.883,0.032,0.579,0.298,0.462,0.92,0.001,0.971,0.696,0.935,0.019,0.107,0.059,0.6,0.039,0.397,0.702,0.104,0.751,0.435,0.782,0.011,0.021,0.94,0.769,0.982,0.843,0.086,0.974,0.71,0.003,0.877,0.011,0.991,0.054,0.022,0.699,0.991,0.987,0.997,1.0,0.647,0.99,0.281,0.44,0.846,0.887,0.976,0.966,0.991,0.346,0.318,0.969,0.995,0.194,0.975,0.013,0.574,0.828,0.372,0.121,0.547,0.363,0.992,0.83,0.996,0.039,0.933,0.985,0.998,0.992,0.992,0.983,0.905,0.002,0.962,0.005,0.065,0.99,0.328,0.081,0.022,0.966,0.73,0.003,0.969,0.217,0.995,0.888,0.049,0.33,0.599,0.004,0.305,0.003,0.725,0.798,0.078,0.034,0.086,0.992,0.004,0.962,0.936,0.011,0.804,0.011,0.735,0.039,0.008,0.984,0.96,0.02,0.765,0.669,0.017,0.648,0.99,0.404,0.621,0.915,0.985,0.991,0.999,0.008,0.626,0.89,0.015,0.029,0.047,0.001,0.753,0.117,0.089,0.01,0.009,0.112,0.814,0.99,0.002,0.035,0.047,0.133,0.047,0.189,0.002,0.095,0.47,0.065,0.249,0.997,0.1,0.079,0.128,0.088,0.602,0.237,0.992,0.294,0.406,0.208,0.268,0.693,0.161,0.64,0.029,0.666,0.912,0.05,0.042,0.026,0.048,0.175,0.687,0.998,0.906,0.218,0.444,0.996,0.985,0.156,0.995,0.029,0.284,0.182,0.035,0.363,0.983,0.689,0.593,0.021,0.004,0.052,0.018,0.724,0.736,0.25,0.005,0.229,0.099,0.406,0.041,0.717,0.541,0.621,0.819,0.944,0.762,0.027,0.862,0.98,0.399,0.965,0.277,0.04,0.017,0.561,0.225,0.04,0.014,0.034,0.109,0.009,0.044,0.481,0.913,0.745,0.999,0.992,0.034,0.089,0.39,0.869,0.483,0.021,0.115,0.487,0.142,0.673,0.629,0.645,0.387,0.009,0.771,0.314,0.464,0.014,0.087,0.039,0.025,0.85,0.012,0.271,0.996,0.05,0.036,0.602,0.385,0.991,0.692,0.001,0.104,0.14,0.898,0.428,0.015,0.017,0.962,0.85,0.114,0.085,0.015,0.005,0.041,0.028,0.029,0.163,0.827,0.27,0.449,0.002,0.015,0.005,0.061,0.831,0.995,0.023,0.826,0.006,0.971,0.966,0.998,0.982,0.237,0.018,0.001,0.039,0.085,0.544,0.153,0.329,0.428,0.849,0.209,0.144,0.931,0.691,0.382,0.177,0.019,0.603,0.169,0.016,0.978,0.052,0.647,0.841,0.808,0.827,0.89,0.003,0.135,0.002,0.005,0.993,0.887,0.008,0.795,0.018,0.974,0.415,0.515,0.019,0.037,0.548,0.023,0.008,0.064,0.87,0.767,0.272,0.687,0.732,0.966,0.004,0.43,0.912,0.365,0.137,0.026,0.125,0.176,0.021,0.78,0.037,0.173,0.895,0.011,0.167,0.123,0.817,0.042,0.832,0.148,0.131,0.154,0.745,0.025,0.846,0.068,0.007,0.007,0.056,0.901,0.39,0.181,0.082,0.994,0.019,0.938,0.328,0.995,0.068,0.302,0.013,0.504,0.97,0.973,0.026,0.992,0.914,0.665,0.721,0.923,0.918,0.942,0.637,0.008,0.422,0.06,0.009,0.099,0.863,0.748,0.252,0.053,0.016,0.002,0.081,0.047,0.005,0.048,0.13,0.292,0.919,0.902,0.036,0.121,0.594,0.467,0.391,0.04,0.268,0.048,0.85,0.196,0.035,0.584,0.366,0.12,0.895,0.031,0.085,0.994,0.04,0.002,0.108,0.362,0.091,0.152,0.871,0.021,0.945,0.338,0.532,0.038,0.982,0.556,0.967,0.843,0.353,0.103,0.81,0.461,0.09,0.071,0.533,0.111,0.025,0.019,0.003,0.989,0.013,0.052,0.014,0.79,0.229,0.15,0.043,0.033,0.297,0.948,0.237,0.02,0.078,0.011,0.657,0.468,0.993,0.005,0.206,0.996,0.001,0.07,0.007,0.004,0.986,0.936,0.017,0.128,0.983,0.1,0.148,0.12,0.502,0.847,0.073,0.73,0.007,0.062,0.949,0.006,0.02,0.495,0.758,0.144,0.218,0.996,0.872,0.454,0.07,0.131,0.773,0.13,0.004,0.002,0.401,0.732,0.931,0.836,0.055,0.099,0.004,0.995,0.003,0.07,0.046,0.008,0.741,0.023,0.427,0.946,0.001,0.078,0.423,0.381,0.88,0.123'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub16 = pd.merge(id_test_df,sub16_df.reset_index(),how='left',on='unique_id')\n",
    "sub16['pred'] = sub16['pred'].apply(lambda x: round(x,3))\n",
    "sub16_txt = ''\n",
    "for prob in list(sub16['pred'].values):\n",
    "    sub16_txt = sub16_txt+','+str(prob)\n",
    "sub16_txt = sub16_txt[1:]\n",
    "\n",
    "sub16_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc6c57",
   "metadata": {},
   "source": [
    "### Submission 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913d867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(max_depth=2,learning_rate=0.1,n_estimators=200,min_child_samples=20,colsample_bytree=1)\n",
    "lgbm.fit(X_train,y_train)\n",
    "\n",
    "pred_train = pd.DataFrame(lgbm.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(lgbm.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(lgbm.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051553f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9425785329076932\n",
      "Val ROC:  0.8703101827017133\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0445ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.094494</td>\n",
       "      <td>0.21134</td>\n",
       "      <td>0.305834</td>\n",
       "      <td>0.137042</td>\n",
       "      <td>0.317829</td>\n",
       "      <td>0.454872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.094494          0.21134        0.305834       0.137042   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.317829      0.454872  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937d2b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.987,0.965,0.946,0.994,0.961,0.996,0.686,0.291,0.898,0.982,0.371,0.088,0.994,0.992,0.943,0.992,0.353,0.169,0.952,0.959,0.998,0.917,0.994,0.981,0.991,0.98,0.974,0.994,0.956,0.83,0.99,0.879,0.958,0.988,0.984,0.99,0.734,0.653,0.991,0.966,0.951,0.895,0.887,0.927,0.986,0.947,0.996,0.429,0.737,0.751,0.217,0.99,0.953,0.051,0.294,0.101,0.984,0.994,0.985,0.991,0.99,0.732,0.296,0.981,0.982,0.786,0.99,0.957,0.966,0.986,0.994,0.961,0.98,0.976,0.926,0.834,0.926,0.995,0.983,0.945,0.329,0.853,0.995,0.095,0.988,0.973,0.713,0.993,0.946,0.934,0.995,0.95,0.857,0.993,0.459,0.952,0.916,0.804,0.388,0.984,0.959,0.993,0.991,0.97,0.677,0.65,0.678,0.598,0.946,0.399,0.987,0.999,0.966,0.998,0.482,0.948,0.913,0.982,0.968,0.943,0.998,0.996,0.537,0.898,0.962,0.993,0.975,0.991,0.247,0.067,0.996,0.818,0.11,0.965,0.925,0.983,0.809,0.978,0.727,0.984,0.191,0.758,0.968,0.954,0.671,0.982,0.678,0.962,0.22,0.972,0.993,0.983,0.965,0.995,0.37,0.785,0.967,0.946,0.986,0.816,0.983,0.958,0.989,0.831,0.989,0.966,0.865,0.954,0.976,0.495,0.989,0.537,0.679,0.996,0.991,0.945,0.979,0.887,0.71,0.94,0.517,0.24,0.931,0.442,0.459,0.99,0.99,0.994,0.983,0.965,0.986,0.264,0.729,0.955,0.892,0.149,0.977,0.323,0.991,0.86,0.869,0.995,0.995,0.979,0.733,0.595,0.26,0.275,0.419,0.11,0.993,0.987,0.471,0.997,0.964,0.587,0.99,0.994,0.74,0.989,0.82,0.962,0.531,0.988,0.985,0.531,0.097,0.967,0.45,0.205,0.098,0.945,0.828,0.987,0.338,0.971,0.998,0.922,0.989,0.988,0.038,0.987,0.962,0.99,0.996,0.394,0.886,0.133,0.982,0.92,0.733,0.216,0.976,0.995,0.029,0.861,0.997,0.989,0.307,0.095,0.99,0.866,0.169,0.987,0.977,0.991,0.856,0.995,0.967,0.122,0.879,0.912,0.992,0.481,0.95,0.91,0.968,0.972,0.973,0.941,0.994,0.972,0.124,0.988,0.854,0.193,0.992,0.951,0.599,0.977,0.053,0.213,0.249,0.972,0.649,0.138,0.954,0.068,0.59,0.997,0.987,0.983,0.98,0.983,0.998,0.986,0.934,0.978,0.989,0.946,0.996,0.923,0.295,0.886,0.979,0.228,0.991,0.433,0.792,0.684,0.963,0.968,0.169,0.707,0.982,0.87,0.356,0.745,0.738,0.587,0.118,0.99,0.151,0.995,0.946,0.818,0.946,0.994,0.098,0.887,0.913,0.953,0.933,0.994,0.941,0.99,0.839,0.097,0.838,0.99,0.181,0.342,0.974,0.961,0.421,0.993,0.97,0.977,0.995,0.998,0.986,0.936,0.227,0.909,0.945,0.962,0.213,0.983,0.975,0.21,0.973,0.925,0.979,0.987,0.807,0.944,0.95,0.988,0.98,0.873,0.859,0.982,0.261,0.775,0.989,0.972,0.048,0.992,0.994,0.924,0.243,0.994,0.659,0.995,0.946,0.989,0.993,0.998,0.978,0.971,0.848,0.99,0.978,0.787,0.992,0.852,0.203,0.116,0.809,0.083,0.234,0.243,0.855,0.642,0.909,0.095,0.968,0.264,0.511,0.769,0.991,0.911,0.938,0.981,0.338,0.201,0.539,0.225,0.981,0.939,0.733,0.17,0.107,0.996,0.486,0.972,0.971,0.889,0.142,0.978,0.265,0.979,0.983,0.493,0.98,0.84,0.418,0.993,0.063,0.588,0.96,0.939,0.232,0.988,0.785,0.96,0.993,0.993,0.967,0.974,0.993,0.447,0.96,0.877,0.906,0.972,0.154,0.632,0.305,0.948,0.737,0.504,0.207,0.976,0.857,0.931,0.938,0.983,0.392,0.916,0.194,0.934,0.988,0.11,0.147,0.155,0.949,0.956,0.983,0.12,0.106,0.916,0.32,0.953,0.65,0.993,0.986,0.752,0.411,0.169,0.977,0.84,0.992,0.659,0.09,0.985,0.376,0.993,0.989,0.578,0.953,0.97,0.92,0.503,0.27,0.078,0.102,0.482,0.716,0.121,0.482,0.436,0.154,0.267,0.977,0.979,0.943,0.991,0.993,0.961,0.09,0.99,0.756,0.102,0.996,0.665,0.668,0.183,0.936,0.855,0.174,0.984,0.479,0.563,0.906,0.306,0.157,0.98,0.463,0.456,0.976,0.987,0.96,0.985,0.965,0.977,0.054,0.799,0.358,0.948,0.995,0.393,0.844,0.982,0.187,0.955,0.979,0.419,0.271,0.961,0.967,0.829,0.158,0.198,0.979,0.969,0.464,0.918,0.99,0.988,0.981,0.275,0.938,0.996,0.418,0.993,0.98,0.743,0.979,0.955,0.947,0.996,0.941,0.319,0.962,0.872,0.997,0.396,0.133,0.949,0.98,0.996,0.603,0.983,0.336,0.894,0.39,0.993,0.982,0.97,0.99,0.961,0.066,0.718,0.383,0.972,0.8,0.087,0.178,0.082,0.989,0.935,0.951,0.98,0.989,0.432,0.137,0.981,0.985,0.966,0.818,0.924,0.596,0.063,0.97,0.559,0.305,0.151,0.96,0.997,0.985,0.977,0.959,0.62,0.971,0.283,0.495,0.946,0.997,0.891,0.974,0.66,0.087,0.886,0.958,0.961,0.9,0.326,0.982,0.986,0.87,0.925,0.278,0.485,0.108,0.248,0.992,0.994,0.987,0.926,0.287,0.958,0.939,0.553,0.794,0.709,0.975,0.955,0.192,0.318,0.427,0.117,0.812,0.993,0.958,0.948,0.19,0.786,0.882,0.789,0.116,0.548,0.829,0.83,0.913,0.912,0.998,0.999,0.156,0.935,0.977,0.983,0.23,0.988,0.882,0.965,0.984,0.423,0.984,0.271,0.992,0.303,0.958,0.131,0.943,0.948,0.973,0.906,0.881,0.322,0.844,0.82,0.992,0.231,0.971,0.926,0.235,0.984,0.379,0.25,0.997,0.85,0.831,0.982,0.905,0.895,0.991,0.993,0.985,0.979,0.955,0.983,0.952,0.989,0.982,0.456,0.614,0.977,0.536,0.967,0.974,0.176,0.634,0.98,0.412,0.872,0.985,0.196,0.404,0.951,0.992,0.994,0.099,0.784,0.996,0.965,0.842,0.35,0.979,0.8,0.08,0.956,0.995,0.28,0.773,0.255,0.278,0.845,0.988,0.814,0.984,0.449,0.963,0.96,0.993,0.554,0.334,0.893,0.998,0.148,0.169,0.972,0.616,0.855,0.722,0.457,0.99,0.207,0.988,0.902,0.231,0.124,0.928,0.048,0.577,0.997,0.834,0.899,0.99,0.987,0.995,0.448,0.957,0.828,0.656,0.899,0.993,0.988,0.965,0.994,0.98,0.975,0.889,0.924,0.809,0.527,0.955,0.876,0.572,0.997,0.883,0.944,0.891,0.496,0.185,0.891,0.401,0.078,0.848,0.432,0.991,0.846,0.992,0.97,0.931,0.983,0.968,0.827,0.989,0.894,0.811,0.867,0.899,0.462,0.434,0.995,0.831,0.473,0.997,0.918,0.957,0.993,0.684,0.993,0.229,0.174,0.175,0.973,0.779,0.425,0.993,0.762,0.979,0.546,0.757,0.959,0.97,0.989,0.057,0.419,0.699,0.338,0.974,0.815,0.682,0.185,0.997,0.695,0.518,0.348,0.987,0.193,0.305,0.653,0.993,0.98,0.897,0.446,0.191,0.621,0.175,0.947,0.837,0.54,0.21,0.884,0.141,0.205,0.698,0.936,0.373,0.552,0.996,0.983,0.6,0.908,0.994,0.953,0.525,0.979,0.937,0.133,0.981,0.912,0.888,0.254,0.976,0.509,0.452,0.124,0.968,0.998,0.977,0.98,0.229,0.982,0.965,0.116,0.956,0.705,0.99,0.197,0.517,0.996,0.961,0.939,0.917,0.994,0.985,0.082,0.984,0.725,0.048,0.992,0.806,0.991,0.988,0.993,0.994,0.481,0.993,0.733,0.595,0.857,0.958,0.97,0.96,0.286,0.968,0.25,0.533,0.212,0.142,0.959,0.99,0.87,0.968,0.953,0.977,0.323,0.988,0.739,0.888,0.485,0.887,0.213,0.983,0.695,0.911,0.996,0.975,0.995,0.915,0.985,0.213,0.99,0.916,0.923,0.956,0.981,0.989,0.679,0.644,0.967,0.974,0.855,0.534,0.26,0.985,0.885,0.29,0.989,0.628,0.92,0.963,0.291,0.376,0.997,0.544,0.992,0.213,0.922,0.561,0.989,0.108,0.58,0.313,0.656,0.421,0.983,0.98,0.988,0.982,0.591,0.987,0.5,0.465,0.131,0.773,0.909,0.175,0.311,0.185,0.401,0.096,0.298,0.968,0.977,0.815,0.984,0.986,0.403,0.856,0.905,0.782,0.946,0.986,0.176,0.67,0.884,0.641,0.554,0.995,0.986,0.871,0.99,0.967,0.974,0.946,0.485,0.971,0.255,0.93,0.978,0.762,0.956,0.868,0.993,0.633,0.95,0.39,0.492,0.406,0.319,0.604,0.825,0.44,0.803,0.435,0.441,0.317,0.542,0.944,0.189,0.441,0.299,0.87,0.983,0.807,0.994,0.816,0.137,0.299,0.752,0.843,0.486,0.526,0.812,0.46,0.251,0.298,0.964,0.993,0.691,0.331,0.567,0.271,0.98,0.845,0.564,0.206,0.998,0.39,0.419,0.823,0.935,0.324,0.46,0.955,0.616,0.133,0.921,0.112,0.984,0.489,0.827,0.949,0.679,0.983,0.127,0.492,0.892,0.615,0.526,0.956,0.322,0.531,0.196,0.958,0.845,0.822,0.185,0.92,0.96,0.976,0.661,0.994,0.992,0.353,0.132,0.178,0.795,0.987,0.98,0.94,0.941,0.683,0.975,0.917,0.632,0.65,0.993,0.119,0.881,0.41,0.992,0.547,0.765,0.104,0.959,0.861,0.483,0.33,0.122,0.9,0.787,0.935,0.085,0.984,0.16,0.123,0.954,0.341,0.152,0.807,0.179,0.883,0.74,0.738,0.479,0.773,0.702,0.838,0.471,0.877,0.201,0.718,0.792,0.636,0.938,0.978,0.369,0.979,0.953,0.955,0.761,0.994,0.859,0.847,0.885,0.873,0.994,0.832,0.397,0.126,0.129,0.64,0.169,0.823,0.683,0.116,0.191,0.938,0.538,0.962,0.988,0.37,0.994,0.396,0.518,0.028,0.253,0.989,0.896,0.121,0.672,0.987,0.988,0.913,0.898,0.803,0.992,0.965,0.542,0.943,0.305,0.981,0.507,0.734,0.604,0.981,0.697,0.994,0.434,0.254,0.125,0.463,0.995,0.989,0.442,0.883,0.153,0.648,0.951,0.968,0.785,0.096,0.968,0.496,0.987,0.978,0.17,0.872,0.974,0.293,0.829,0.823,0.435,0.923,0.851,0.646,0.325,0.185,0.193,0.238,0.8,0.46,0.726,0.964,0.945,0.994,0.701,0.058,0.95,0.271,0.988,0.367,0.975,0.08,0.725,0.803,0.951,0.127,0.185,0.875,0.511,0.636,0.997,0.324,0.111,0.185,0.91,0.984,0.61,0.95,0.23,0.166,0.391,0.518,0.987,0.333,0.968,0.644,0.371,0.61,0.991,0.544,0.793,0.602,0.263,0.285,0.993,0.62,0.749,0.39,0.833,0.864,0.986,0.991,0.286,0.994,0.614,0.414,0.665,0.55,0.308,0.638,0.866,0.695,0.85,0.993,0.352,0.234,0.438,0.909,0.627,0.272,0.992,0.157,0.418,0.92,0.971,0.603,0.95,0.863,0.306,0.524,0.197,0.48,0.525,0.425,0.533,0.518,0.446,0.346,0.953,0.316,0.399,0.256,0.59,0.608,0.966,0.43,0.528,0.555,0.318,0.407,0.409,0.628,0.267,0.706,0.287,0.282,0.937,0.297,0.624,0.57,0.615,0.359,0.958,0.271,0.678,0.521,0.449,0.537,0.217,0.585,0.106,0.751,0.939,0.43,0.451,0.271,0.958,0.847,0.291,0.952,0.945,0.313,0.188,0.148,0.629,0.357,0.73,0.994,0.752,0.734,0.571,0.375,0.373,0.847,0.843,0.733,0.272,0.375,0.972,0.596,0.18,0.189,0.595,0.757,0.391,0.336,0.729,0.908,0.254,0.671,0.915,0.291,0.985,0.904,0.488,0.283,0.969,0.164,0.665,0.263,0.974,0.892,0.225,0.495,0.736,0.6,0.734,0.97,0.661,0.456,0.985,0.255,0.498,0.9,0.99,0.965,0.917,0.692,0.924,0.982,0.277,0.912,0.605,0.838,0.21,0.936,0.942,0.497,0.339,0.929,0.995,0.619,0.862,0.553,0.949,0.989,0.978,0.613,0.988,0.841,0.974,0.86,0.941,0.454,0.554,0.157,0.976,0.341,0.937,0.222,0.965,0.656,0.801,0.761,0.596,0.989,0.933,0.622,0.965,0.998,0.796,0.424,0.965,0.1,0.953,0.974,0.064,0.985,0.2,0.272,0.894,0.54,0.882,0.996,0.737,0.938,0.927,0.126,0.136,0.132,0.782,0.327,0.301,0.294,0.962,0.347,0.346,0.978,0.213,0.947,0.871,0.982,0.937,0.565,0.284,0.979,0.029,0.777,0.93,0.985,0.947,0.354,0.673,0.914,0.99,0.851,0.917,0.947,0.14,0.817,0.202,0.265,0.908,0.605,0.926,0.28,0.6,0.901,0.166,0.909,0.506,0.893,0.95,0.987,0.852,0.293,0.992,0.139,0.902,0.483,0.302,0.871,0.982,0.957,0.69,0.911,0.97,0.109,0.41,0.423,0.169,0.761,0.317,0.284,0.976,0.959,0.53,0.944,0.983,0.178,0.242,0.943,0.357,0.916,0.57,0.944,0.147,0.983,0.084,0.521,0.312,0.424,0.757,0.455,0.161,0.415,0.244,0.903,0.928,0.893,0.827,0.957,0.271,0.85,0.952,0.275,0.301,0.123,0.992,0.956,0.935,0.895,0.973,0.777,0.954,0.976,0.859,0.17,0.95,0.925,0.816,0.961,0.184,0.228,0.978,0.879,0.232,0.916,0.745,0.978,0.982,0.966,0.868,0.879,0.984,0.961,0.952,0.828,0.69,0.536,0.312,0.767,0.493,0.918,0.171,0.933,0.975,0.234,0.393,0.879,0.483,0.96,0.84,0.954,0.801,0.95,0.32,0.272,0.982,0.975,0.351,0.955,0.209,0.309,0.78,0.275,0.61,0.965,0.879,0.481,0.988,0.97,0.844,0.568,0.957,0.136,0.868,0.92,0.823,0.978,0.938,0.691,0.88,0.927,0.774,0.906,0.736,0.656,0.233,0.204,0.215,0.165,0.584,0.976,0.933,0.109,0.957,0.884,0.116,0.788,0.827,0.627,0.278,0.113,0.264,0.66,0.972,0.948,0.123,0.962,0.718,0.571,0.258,0.533,0.286,0.259,0.849,0.391,0.436,0.874,0.989,0.232,0.517,0.291,0.822,0.359,0.582,0.687,0.9,0.687,0.169,0.376,0.302,0.432,0.967,0.747,0.188,0.362,0.871,0.902,0.906,0.232,0.697,0.605,0.847,0.835,0.958,0.927,0.709,0.845,0.923,0.772,0.902,0.419,0.794,0.768,0.897,0.869,0.405,0.102,0.109,0.358,0.945,0.408,0.14,0.874,0.376,0.756,0.114,0.301,0.801,0.179,0.204,0.763,0.621,0.717,0.621,0.571,0.586,0.2,0.229,0.746,0.722,0.763,0.41,0.81,0.739,0.619,0.914,0.495,0.755,0.809,0.12,0.63,0.715,0.828,0.09,0.735,0.253,0.89,0.63,0.737,0.071,0.721,0.647,0.756,0.618,0.65,0.729,0.65,0.662,0.085,0.657,0.675,0.693,0.227,0.547,0.367,0.432,0.29,0.576,0.589,0.668,0.634,0.537,0.385,0.391,0.171,0.835,0.714,0.677,0.647,0.49,0.734,0.631,0.144,0.457,0.245,0.747,0.419,0.204,0.896,0.849,0.935,0.891,0.97,0.661,0.856,0.898,0.638,0.84,0.641,0.896,0.906,0.828,0.847,0.768,0.786,0.828,0.347,0.849,0.577,0.728,0.826,0.448,0.612,0.628,0.46,0.962,0.811,0.931,0.232,0.775,0.928,0.991,0.975,0.902,0.895,0.843,0.2,0.601,0.466,0.353,0.884,0.527,0.545,0.417,0.823,0.796,0.308,0.79,0.463,0.845,0.82,0.438,0.487,0.793,0.483,0.679,0.378,0.736,0.841,0.505,0.333,0.321,0.787,0.302,0.84,0.587,0.227,0.755,0.228,0.881,0.349,0.145,0.828,0.816,0.386,0.637,0.802,0.399,0.789,0.946,0.868,0.732,0.771,0.911,0.898,0.913,0.29,0.712,0.845,0.552,0.436,0.492,0.245,0.641,0.799,0.221,0.296,0.226,0.239,0.869,0.78,0.359,0.469,0.269,0.61,0.46,0.649,0.082,0.236,0.812,0.236,0.658,0.918,0.554,0.64,0.478,0.487,0.761,0.475,0.827,0.487,0.652,0.58,0.651,0.733,0.798,0.892,0.514,0.661,0.62,0.534,0.421,0.475,0.445,0.671,0.726,0.888,0.834,0.804,0.6,0.877,0.807,0.781,0.897,0.508,0.533,0.419,0.28,0.455,0.758,0.794,0.463,0.476,0.254,0.374,0.473,0.767,0.735,0.66,0.388,0.556,0.495,0.458,0.343,0.832,0.833,0.702,0.676,0.867,0.551,0.423,0.452,0.663,0.527,0.725,0.792,0.416,0.405,0.783,0.508,0.612,0.502,0.516,0.474,0.265,0.457,0.494,0.802,0.597,0.903,0.811,0.218,0.572,0.868,0.857,0.67,0.404,0.542,0.584,0.403,0.601,0.281,0.716,0.687,0.181,0.589,0.508,0.594,0.251,0.415,0.205,0.527,0.739,0.453,0.54,0.898,0.753,0.402,0.823,0.612,0.853,0.521,0.253,0.588,0.754,0.799,0.688,0.291,0.322,0.885,0.84,0.611,0.518,0.45,0.229,0.336,0.567,0.419,0.455,0.801,0.574,0.732,0.225,0.244,0.317,0.719,0.65,0.901,0.238,0.758,0.451,0.862,0.689,0.88,0.744,0.301,0.595,0.262,0.571,0.481,0.607,0.62,0.376,0.687,0.62,0.646,0.595,0.8,0.71,0.695,0.647,0.474,0.557,0.311,0.436,0.85,0.517,0.551,0.779,0.64,0.73,0.757,0.422,0.538,0.268,0.545,0.902,0.711,0.297,0.791,0.319,0.9,0.702,0.33,0.275,0.279,0.498,0.459,0.65,0.584,0.732,0.502,0.563,0.545,0.737,0.891,0.401,0.653,0.821,0.483,0.41,0.67,0.448,0.638,0.386,0.696,0.232,0.538,0.84,0.467,0.43,0.536,0.64,0.41,0.676,0.443,0.519,0.582,0.666,0.479,0.716,0.568,0.243,0.219,0.585,0.585,0.597,0.617,0.546,0.841,0.428,0.773,0.629,0.783,0.447,0.751,0.595,0.526,0.756,0.778,0.684,0.806,0.596,0.567,0.561,0.578,0.729,0.661,0.773,0.433,0.69,0.509,0.411,0.463,0.699,0.776,0.583,0.38,0.325,0.498,0.432,0.239,0.486,0.426,0.378,0.729,0.813,0.755,0.332,0.576,0.656,0.685,0.495,0.474,0.575,0.6,0.708,0.59,0.434,0.536,0.632,0.43,0.632,0.505,0.565,0.818,0.526,0.416,0.266,0.744,0.624,0.56,0.691,0.418,0.587,0.715,0.623,0.375,0.734,0.683,0.791,0.656,0.584,0.466,0.693,0.44,0.441,0.477,0.568,0.533,0.556,0.446,0.297,0.75,0.477,0.432,0.573,0.69,0.653,0.693,0.645,0.311,0.696,0.708,0.536,0.539,0.556,0.5,0.641,0.656,0.82,0.388,0.499,0.74,0.424,0.453,0.421,0.434,0.696,0.704,0.557,0.612,0.89,0.519,0.482,0.55,0.577,0.508,0.522,0.478,0.426,0.707,0.611,0.51,0.424,0.732,0.691,0.547,0.689,0.617,0.466,0.551,0.538,0.363,0.541,0.508,0.363,0.326,0.639,0.64,0.475,0.62,0.591,0.589,0.44,0.842,0.417,0.578,0.578,0.472,0.599,0.509,0.523,0.731,0.212,0.648,0.38,0.55,0.585,0.583'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub17 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub17['pred'] = sub17['pred'].apply(lambda x: round(x,3))\n",
    "sub17_txt = ''\n",
    "for prob in list(sub17['pred'].values):\n",
    "    sub17_txt = sub17_txt+','+str(prob)\n",
    "sub17_txt = sub17_txt[1:]\n",
    "\n",
    "sub17_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0264b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_lgbm_cold = pred_train.copy()\n",
    "pred_val_lgbm_cold = pred_val.copy()\n",
    "pred_test_lgbm_cold = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757bcb5",
   "metadata": {},
   "source": [
    "### Submission 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f53ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(max_depth=2,learning_rate=0.1,n_estimators=200,min_child_samples=20,colsample_bytree=0.5)\n",
    "lgbm.fit(X_train,y_train)\n",
    "\n",
    "pred_train = pd.DataFrame(lgbm.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(lgbm.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(lgbm.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f8a5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9392778205092265\n",
      "Val ROC:  0.8698596517062327\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367d8284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.094154</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.320958</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>0.297158</td>\n",
       "      <td>0.439627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.094154         0.226804        0.320958       0.142469   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.297158      0.439627  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c9d485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.988,0.964,0.936,0.993,0.962,0.995,0.724,0.281,0.841,0.978,0.376,0.108,0.99,0.991,0.906,0.995,0.198,0.23,0.916,0.947,0.996,0.871,0.986,0.983,0.99,0.976,0.979,0.992,0.958,0.751,0.988,0.924,0.964,0.984,0.978,0.989,0.73,0.603,0.99,0.969,0.952,0.887,0.898,0.881,0.991,0.958,0.996,0.463,0.782,0.67,0.175,0.99,0.942,0.072,0.32,0.093,0.973,0.986,0.982,0.992,0.986,0.668,0.172,0.977,0.983,0.803,0.983,0.956,0.98,0.983,0.996,0.947,0.955,0.972,0.935,0.829,0.882,0.99,0.978,0.923,0.306,0.851,0.992,0.144,0.983,0.978,0.645,0.993,0.946,0.893,0.995,0.964,0.866,0.987,0.514,0.931,0.957,0.755,0.357,0.977,0.951,0.983,0.988,0.974,0.672,0.547,0.641,0.574,0.953,0.521,0.974,0.999,0.951,0.996,0.572,0.95,0.903,0.968,0.943,0.94,0.996,0.997,0.585,0.862,0.955,0.99,0.963,0.985,0.267,0.115,0.996,0.861,0.118,0.968,0.939,0.986,0.729,0.984,0.736,0.971,0.194,0.71,0.968,0.948,0.55,0.974,0.674,0.936,0.201,0.976,0.991,0.984,0.951,0.993,0.334,0.746,0.969,0.948,0.991,0.828,0.98,0.958,0.97,0.84,0.991,0.956,0.798,0.913,0.966,0.34,0.981,0.627,0.71,0.996,0.974,0.935,0.99,0.87,0.651,0.949,0.438,0.197,0.892,0.459,0.423,0.986,0.981,0.994,0.982,0.962,0.99,0.226,0.685,0.89,0.902,0.153,0.965,0.414,0.99,0.853,0.91,0.991,0.993,0.98,0.748,0.748,0.229,0.278,0.461,0.083,0.991,0.986,0.429,0.997,0.968,0.576,0.984,0.988,0.728,0.99,0.808,0.968,0.523,0.986,0.986,0.451,0.078,0.973,0.506,0.161,0.076,0.939,0.679,0.984,0.352,0.974,0.996,0.916,0.989,0.983,0.035,0.987,0.964,0.984,0.993,0.38,0.898,0.117,0.984,0.933,0.72,0.196,0.969,0.994,0.038,0.852,0.995,0.99,0.339,0.104,0.985,0.882,0.197,0.982,0.976,0.993,0.874,0.991,0.975,0.143,0.849,0.948,0.994,0.433,0.934,0.878,0.958,0.97,0.978,0.947,0.992,0.97,0.218,0.984,0.828,0.149,0.989,0.953,0.614,0.976,0.066,0.223,0.188,0.948,0.581,0.11,0.975,0.098,0.625,0.996,0.983,0.982,0.979,0.98,0.992,0.983,0.894,0.976,0.983,0.952,0.996,0.932,0.18,0.919,0.963,0.23,0.991,0.472,0.76,0.627,0.952,0.952,0.151,0.631,0.992,0.837,0.406,0.78,0.674,0.62,0.069,0.989,0.14,0.99,0.935,0.828,0.935,0.994,0.09,0.866,0.884,0.967,0.923,0.994,0.936,0.989,0.852,0.071,0.748,0.991,0.247,0.335,0.965,0.951,0.461,0.983,0.946,0.971,0.993,0.998,0.987,0.952,0.293,0.939,0.923,0.948,0.174,0.978,0.976,0.192,0.97,0.919,0.981,0.985,0.829,0.913,0.934,0.982,0.974,0.788,0.867,0.989,0.251,0.783,0.978,0.958,0.088,0.992,0.992,0.946,0.204,0.996,0.643,0.996,0.933,0.987,0.994,0.995,0.984,0.94,0.844,0.991,0.969,0.814,0.992,0.827,0.165,0.117,0.858,0.094,0.315,0.179,0.865,0.724,0.908,0.103,0.961,0.2,0.463,0.714,0.988,0.943,0.963,0.976,0.249,0.29,0.579,0.221,0.98,0.956,0.79,0.153,0.095,0.993,0.617,0.973,0.971,0.891,0.117,0.966,0.269,0.973,0.98,0.596,0.981,0.879,0.334,0.989,0.096,0.527,0.96,0.912,0.193,0.975,0.778,0.937,0.99,0.994,0.955,0.975,0.993,0.618,0.955,0.886,0.882,0.971,0.137,0.571,0.336,0.968,0.753,0.491,0.122,0.964,0.894,0.925,0.964,0.984,0.382,0.866,0.13,0.946,0.989,0.098,0.154,0.164,0.919,0.942,0.986,0.14,0.099,0.892,0.42,0.96,0.544,0.992,0.984,0.703,0.49,0.17,0.982,0.898,0.993,0.649,0.105,0.99,0.452,0.993,0.987,0.575,0.938,0.973,0.952,0.615,0.236,0.062,0.142,0.52,0.719,0.131,0.501,0.364,0.114,0.303,0.983,0.974,0.947,0.991,0.993,0.98,0.077,0.99,0.86,0.086,0.995,0.689,0.698,0.165,0.893,0.714,0.161,0.981,0.499,0.474,0.932,0.334,0.109,0.98,0.515,0.473,0.97,0.977,0.955,0.98,0.946,0.978,0.095,0.76,0.388,0.931,0.991,0.36,0.864,0.973,0.217,0.958,0.97,0.338,0.262,0.962,0.977,0.829,0.198,0.16,0.98,0.967,0.46,0.958,0.984,0.975,0.979,0.275,0.961,0.997,0.59,0.993,0.986,0.695,0.975,0.943,0.945,0.993,0.939,0.41,0.956,0.849,0.997,0.429,0.158,0.956,0.975,0.994,0.602,0.986,0.385,0.895,0.384,0.987,0.976,0.976,0.981,0.927,0.05,0.713,0.33,0.964,0.754,0.116,0.157,0.093,0.986,0.942,0.94,0.977,0.98,0.416,0.066,0.977,0.986,0.958,0.858,0.924,0.628,0.076,0.954,0.515,0.399,0.111,0.953,0.993,0.985,0.973,0.972,0.588,0.968,0.235,0.383,0.965,0.997,0.904,0.974,0.544,0.092,0.892,0.954,0.962,0.909,0.307,0.976,0.989,0.843,0.919,0.272,0.501,0.108,0.252,0.99,0.99,0.985,0.956,0.234,0.952,0.903,0.477,0.736,0.72,0.979,0.959,0.13,0.354,0.461,0.092,0.813,0.989,0.958,0.919,0.183,0.829,0.825,0.821,0.129,0.479,0.847,0.882,0.91,0.913,0.997,0.998,0.128,0.919,0.972,0.983,0.296,0.988,0.841,0.973,0.985,0.379,0.986,0.311,0.993,0.279,0.961,0.155,0.937,0.927,0.962,0.868,0.847,0.241,0.834,0.811,0.988,0.411,0.953,0.927,0.33,0.982,0.314,0.174,0.995,0.83,0.894,0.986,0.877,0.887,0.993,0.993,0.983,0.986,0.95,0.97,0.949,0.993,0.987,0.487,0.627,0.981,0.558,0.961,0.96,0.228,0.581,0.974,0.416,0.89,0.986,0.21,0.385,0.969,0.987,0.992,0.111,0.822,0.995,0.962,0.815,0.383,0.975,0.786,0.075,0.941,0.993,0.242,0.768,0.205,0.284,0.807,0.979,0.799,0.983,0.45,0.961,0.953,0.989,0.533,0.3,0.912,0.996,0.174,0.211,0.966,0.631,0.755,0.778,0.484,0.984,0.199,0.986,0.909,0.248,0.147,0.942,0.053,0.626,0.996,0.928,0.894,0.99,0.989,0.996,0.453,0.97,0.823,0.701,0.906,0.99,0.991,0.964,0.993,0.979,0.978,0.847,0.92,0.796,0.653,0.942,0.804,0.58,0.996,0.91,0.954,0.909,0.561,0.259,0.881,0.366,0.087,0.85,0.4,0.986,0.804,0.988,0.967,0.933,0.986,0.96,0.859,0.986,0.943,0.777,0.876,0.908,0.44,0.418,0.996,0.92,0.427,0.993,0.959,0.949,0.994,0.693,0.992,0.254,0.161,0.193,0.968,0.82,0.384,0.99,0.699,0.981,0.558,0.805,0.96,0.972,0.985,0.058,0.362,0.755,0.39,0.975,0.903,0.724,0.175,0.994,0.708,0.422,0.347,0.99,0.131,0.209,0.675,0.991,0.98,0.88,0.429,0.248,0.64,0.212,0.934,0.862,0.48,0.215,0.878,0.108,0.177,0.615,0.923,0.452,0.656,0.996,0.983,0.575,0.944,0.994,0.949,0.464,0.982,0.933,0.108,0.984,0.843,0.906,0.319,0.953,0.522,0.531,0.115,0.97,0.996,0.98,0.942,0.213,0.965,0.949,0.141,0.937,0.673,0.993,0.185,0.605,0.996,0.971,0.93,0.928,0.993,0.983,0.092,0.986,0.768,0.076,0.993,0.828,0.993,0.975,0.99,0.992,0.438,0.99,0.732,0.476,0.859,0.966,0.967,0.969,0.351,0.964,0.199,0.541,0.218,0.119,0.978,0.989,0.927,0.973,0.915,0.967,0.368,0.989,0.745,0.73,0.567,0.893,0.243,0.971,0.688,0.908,0.997,0.975,0.991,0.9,0.986,0.192,0.982,0.86,0.885,0.958,0.984,0.988,0.697,0.63,0.964,0.962,0.819,0.62,0.3,0.976,0.885,0.25,0.985,0.574,0.924,0.959,0.258,0.291,0.997,0.505,0.986,0.306,0.912,0.555,0.986,0.084,0.619,0.28,0.654,0.399,0.978,0.975,0.983,0.986,0.598,0.986,0.539,0.629,0.098,0.746,0.908,0.174,0.316,0.117,0.382,0.104,0.237,0.969,0.981,0.828,0.979,0.989,0.318,0.853,0.912,0.788,0.931,0.992,0.256,0.7,0.883,0.676,0.551,0.996,0.99,0.841,0.989,0.958,0.972,0.955,0.492,0.957,0.205,0.931,0.981,0.721,0.964,0.882,0.994,0.568,0.953,0.381,0.473,0.417,0.269,0.653,0.848,0.436,0.744,0.539,0.356,0.479,0.533,0.927,0.174,0.497,0.344,0.871,0.976,0.707,0.993,0.829,0.101,0.249,0.767,0.847,0.5,0.44,0.826,0.398,0.223,0.232,0.969,0.989,0.703,0.39,0.552,0.327,0.971,0.817,0.569,0.185,0.997,0.392,0.357,0.824,0.941,0.344,0.473,0.944,0.61,0.146,0.916,0.083,0.983,0.464,0.859,0.957,0.688,0.987,0.156,0.512,0.902,0.609,0.517,0.95,0.358,0.52,0.178,0.957,0.85,0.815,0.162,0.943,0.968,0.966,0.442,0.993,0.994,0.341,0.173,0.264,0.726,0.984,0.979,0.893,0.938,0.729,0.976,0.879,0.588,0.651,0.991,0.124,0.875,0.36,0.991,0.492,0.728,0.078,0.975,0.844,0.476,0.297,0.083,0.922,0.743,0.944,0.106,0.982,0.145,0.14,0.959,0.305,0.156,0.756,0.163,0.877,0.713,0.754,0.46,0.741,0.672,0.843,0.403,0.829,0.252,0.662,0.782,0.627,0.849,0.979,0.376,0.988,0.948,0.97,0.805,0.992,0.922,0.759,0.881,0.863,0.995,0.854,0.302,0.099,0.115,0.584,0.152,0.729,0.71,0.093,0.19,0.941,0.563,0.973,0.983,0.355,0.99,0.4,0.532,0.045,0.267,0.991,0.896,0.118,0.678,0.988,0.991,0.913,0.865,0.809,0.99,0.969,0.327,0.958,0.38,0.975,0.522,0.7,0.586,0.986,0.642,0.993,0.594,0.292,0.138,0.431,0.989,0.99,0.485,0.885,0.144,0.616,0.932,0.96,0.779,0.089,0.97,0.444,0.985,0.986,0.187,0.863,0.97,0.226,0.862,0.81,0.498,0.958,0.832,0.69,0.313,0.18,0.222,0.221,0.86,0.442,0.642,0.978,0.941,0.993,0.705,0.056,0.957,0.335,0.987,0.366,0.984,0.072,0.641,0.849,0.96,0.144,0.169,0.865,0.522,0.524,0.996,0.329,0.084,0.279,0.923,0.982,0.525,0.96,0.186,0.168,0.423,0.523,0.986,0.35,0.976,0.622,0.392,0.63,0.988,0.457,0.754,0.617,0.28,0.268,0.993,0.602,0.742,0.377,0.853,0.858,0.982,0.993,0.251,0.995,0.625,0.425,0.61,0.523,0.281,0.611,0.798,0.677,0.791,0.993,0.368,0.299,0.337,0.937,0.567,0.263,0.987,0.121,0.423,0.901,0.97,0.625,0.944,0.828,0.338,0.489,0.23,0.459,0.529,0.39,0.536,0.575,0.559,0.389,0.963,0.307,0.345,0.224,0.617,0.603,0.944,0.42,0.548,0.572,0.333,0.445,0.361,0.611,0.342,0.719,0.358,0.226,0.944,0.277,0.698,0.599,0.588,0.314,0.955,0.147,0.691,0.541,0.456,0.576,0.222,0.598,0.088,0.725,0.93,0.338,0.378,0.28,0.928,0.859,0.285,0.968,0.941,0.259,0.201,0.149,0.543,0.359,0.753,0.991,0.814,0.76,0.584,0.429,0.345,0.897,0.836,0.784,0.222,0.463,0.978,0.623,0.21,0.254,0.575,0.783,0.322,0.289,0.709,0.886,0.216,0.683,0.91,0.324,0.983,0.926,0.48,0.278,0.97,0.203,0.709,0.277,0.979,0.855,0.189,0.521,0.653,0.653,0.693,0.959,0.665,0.365,0.977,0.26,0.536,0.936,0.989,0.966,0.898,0.702,0.936,0.989,0.327,0.905,0.61,0.841,0.156,0.931,0.951,0.445,0.503,0.938,0.994,0.583,0.843,0.593,0.959,0.987,0.983,0.654,0.987,0.835,0.955,0.864,0.934,0.452,0.622,0.142,0.973,0.276,0.931,0.271,0.932,0.688,0.756,0.765,0.581,0.99,0.932,0.646,0.972,0.997,0.803,0.394,0.961,0.084,0.945,0.968,0.068,0.986,0.208,0.176,0.922,0.441,0.857,0.994,0.753,0.926,0.961,0.105,0.166,0.135,0.756,0.336,0.297,0.253,0.96,0.191,0.417,0.975,0.243,0.926,0.897,0.978,0.939,0.564,0.294,0.978,0.052,0.798,0.941,0.987,0.954,0.323,0.649,0.923,0.99,0.752,0.892,0.945,0.113,0.832,0.162,0.239,0.948,0.583,0.938,0.249,0.566,0.921,0.149,0.919,0.439,0.886,0.932,0.987,0.893,0.281,0.993,0.113,0.909,0.468,0.284,0.873,0.987,0.96,0.699,0.903,0.957,0.108,0.366,0.366,0.196,0.676,0.262,0.217,0.973,0.944,0.478,0.948,0.984,0.167,0.267,0.955,0.35,0.904,0.564,0.957,0.147,0.979,0.093,0.447,0.293,0.383,0.809,0.404,0.123,0.313,0.222,0.941,0.921,0.854,0.816,0.955,0.317,0.835,0.943,0.251,0.33,0.129,0.986,0.955,0.913,0.897,0.955,0.818,0.941,0.977,0.839,0.133,0.951,0.93,0.808,0.966,0.151,0.253,0.979,0.892,0.276,0.926,0.745,0.981,0.984,0.974,0.841,0.883,0.986,0.947,0.957,0.845,0.683,0.524,0.338,0.78,0.519,0.917,0.176,0.931,0.976,0.202,0.44,0.89,0.396,0.961,0.839,0.947,0.813,0.949,0.289,0.287,0.98,0.964,0.397,0.969,0.146,0.266,0.803,0.232,0.606,0.962,0.92,0.605,0.985,0.969,0.852,0.533,0.958,0.139,0.878,0.919,0.859,0.978,0.953,0.693,0.851,0.937,0.74,0.883,0.717,0.733,0.228,0.227,0.182,0.118,0.556,0.978,0.926,0.104,0.966,0.886,0.12,0.771,0.831,0.597,0.34,0.157,0.271,0.696,0.947,0.955,0.123,0.962,0.632,0.557,0.283,0.533,0.389,0.269,0.852,0.424,0.462,0.885,0.993,0.218,0.52,0.378,0.853,0.362,0.626,0.696,0.915,0.68,0.164,0.358,0.181,0.403,0.968,0.772,0.147,0.427,0.868,0.872,0.889,0.345,0.69,0.563,0.861,0.858,0.956,0.936,0.695,0.804,0.918,0.802,0.902,0.478,0.798,0.813,0.87,0.85,0.406,0.1,0.111,0.395,0.953,0.392,0.137,0.918,0.376,0.769,0.153,0.334,0.782,0.175,0.171,0.831,0.606,0.741,0.606,0.55,0.596,0.153,0.229,0.715,0.715,0.747,0.41,0.738,0.74,0.598,0.864,0.511,0.754,0.81,0.123,0.598,0.727,0.761,0.173,0.738,0.238,0.867,0.667,0.729,0.068,0.722,0.566,0.759,0.627,0.667,0.662,0.643,0.627,0.1,0.689,0.691,0.666,0.163,0.532,0.361,0.39,0.305,0.589,0.608,0.637,0.599,0.516,0.422,0.378,0.202,0.867,0.745,0.671,0.645,0.516,0.745,0.623,0.135,0.407,0.24,0.766,0.394,0.143,0.899,0.863,0.939,0.873,0.968,0.736,0.774,0.908,0.648,0.829,0.669,0.928,0.878,0.821,0.836,0.764,0.807,0.812,0.324,0.821,0.628,0.754,0.819,0.589,0.624,0.645,0.421,0.939,0.852,0.908,0.259,0.764,0.911,0.987,0.962,0.896,0.895,0.845,0.272,0.602,0.436,0.408,0.863,0.554,0.578,0.427,0.808,0.737,0.315,0.806,0.433,0.848,0.833,0.486,0.51,0.748,0.456,0.7,0.301,0.728,0.853,0.587,0.271,0.21,0.798,0.352,0.798,0.514,0.265,0.748,0.256,0.892,0.468,0.175,0.82,0.815,0.276,0.678,0.811,0.395,0.824,0.953,0.851,0.811,0.743,0.906,0.906,0.852,0.235,0.745,0.829,0.528,0.294,0.384,0.243,0.714,0.748,0.121,0.315,0.233,0.239,0.851,0.809,0.358,0.409,0.26,0.603,0.427,0.597,0.09,0.209,0.792,0.224,0.729,0.892,0.561,0.673,0.463,0.527,0.802,0.5,0.861,0.51,0.639,0.531,0.688,0.664,0.791,0.832,0.563,0.665,0.632,0.533,0.351,0.499,0.475,0.699,0.717,0.888,0.859,0.791,0.581,0.855,0.822,0.793,0.91,0.498,0.552,0.441,0.297,0.445,0.767,0.769,0.426,0.485,0.258,0.377,0.507,0.781,0.769,0.642,0.362,0.588,0.428,0.432,0.324,0.793,0.869,0.737,0.697,0.895,0.494,0.431,0.424,0.663,0.488,0.748,0.807,0.396,0.44,0.752,0.521,0.627,0.502,0.535,0.492,0.244,0.438,0.591,0.775,0.6,0.915,0.85,0.24,0.6,0.885,0.883,0.665,0.401,0.596,0.588,0.381,0.638,0.242,0.694,0.663,0.124,0.589,0.539,0.59,0.34,0.422,0.226,0.5,0.752,0.462,0.56,0.873,0.667,0.395,0.824,0.64,0.841,0.545,0.182,0.614,0.645,0.807,0.693,0.305,0.361,0.887,0.826,0.594,0.484,0.426,0.32,0.347,0.57,0.503,0.513,0.804,0.543,0.709,0.228,0.249,0.253,0.729,0.662,0.891,0.255,0.768,0.41,0.878,0.714,0.886,0.804,0.275,0.617,0.276,0.557,0.457,0.611,0.597,0.445,0.653,0.653,0.655,0.59,0.813,0.739,0.716,0.663,0.39,0.573,0.395,0.413,0.826,0.479,0.599,0.732,0.699,0.74,0.709,0.463,0.527,0.249,0.495,0.889,0.717,0.315,0.749,0.318,0.878,0.661,0.343,0.299,0.242,0.487,0.441,0.606,0.56,0.686,0.523,0.59,0.499,0.728,0.893,0.431,0.661,0.826,0.42,0.425,0.685,0.358,0.656,0.318,0.711,0.259,0.533,0.824,0.364,0.491,0.521,0.638,0.418,0.697,0.426,0.504,0.576,0.66,0.552,0.735,0.585,0.207,0.216,0.538,0.574,0.561,0.672,0.595,0.848,0.403,0.766,0.704,0.782,0.401,0.777,0.556,0.469,0.756,0.764,0.633,0.808,0.573,0.545,0.546,0.557,0.716,0.689,0.675,0.459,0.684,0.508,0.367,0.431,0.727,0.777,0.573,0.402,0.326,0.513,0.467,0.243,0.492,0.418,0.352,0.713,0.81,0.764,0.323,0.605,0.632,0.669,0.517,0.462,0.566,0.573,0.707,0.614,0.489,0.553,0.577,0.435,0.634,0.477,0.543,0.821,0.438,0.381,0.227,0.756,0.615,0.575,0.726,0.48,0.458,0.668,0.659,0.395,0.762,0.698,0.83,0.644,0.6,0.479,0.67,0.476,0.446,0.476,0.627,0.55,0.553,0.364,0.255,0.729,0.467,0.531,0.57,0.697,0.637,0.674,0.623,0.307,0.686,0.705,0.537,0.528,0.575,0.477,0.625,0.673,0.819,0.427,0.504,0.732,0.36,0.462,0.425,0.451,0.708,0.743,0.551,0.597,0.872,0.543,0.485,0.565,0.598,0.558,0.483,0.457,0.415,0.682,0.605,0.438,0.394,0.724,0.688,0.531,0.703,0.623,0.462,0.558,0.522,0.35,0.562,0.505,0.403,0.348,0.664,0.666,0.47,0.62,0.569,0.615,0.449,0.83,0.433,0.603,0.577,0.508,0.594,0.422,0.495,0.731,0.171,0.641,0.407,0.568,0.61,0.601'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub18 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub18['pred'] = sub18['pred'].apply(lambda x: round(x,3))\n",
    "sub18_txt = ''\n",
    "for prob in list(sub18['pred'].values):\n",
    "    sub18_txt = sub18_txt+','+str(prob)\n",
    "sub18_txt = sub18_txt[1:]\n",
    "\n",
    "sub18_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95811f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_lgbm_exp = pred_train.copy()\n",
    "pred_val_lgbm_exp = pred_val.copy()\n",
    "pred_test_lgbm_exp = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79479602",
   "metadata": {},
   "source": [
    "### Submission 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11845dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_flag</th>\n",
       "      <th>pred_x</th>\n",
       "      <th>pred_cold</th>\n",
       "      <th>pred_y</th>\n",
       "      <th>pred_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986811</td>\n",
       "      <td>0.986811</td>\n",
       "      <td>0.987962</td>\n",
       "      <td>0.987962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.964566</td>\n",
       "      <td>0.964566</td>\n",
       "      <td>0.964265</td>\n",
       "      <td>0.964265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.945521</td>\n",
       "      <td>0.945521</td>\n",
       "      <td>0.935638</td>\n",
       "      <td>0.935638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.994099</td>\n",
       "      <td>0.994099</td>\n",
       "      <td>0.993278</td>\n",
       "      <td>0.993278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.960582</td>\n",
       "      <td>0.960582</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>0</td>\n",
       "      <td>0.648155</td>\n",
       "      <td>0.648155</td>\n",
       "      <td>0.640795</td>\n",
       "      <td>0.640795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>0</td>\n",
       "      <td>0.380371</td>\n",
       "      <td>0.380371</td>\n",
       "      <td>0.406924</td>\n",
       "      <td>0.406924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>0</td>\n",
       "      <td>0.549757</td>\n",
       "      <td>0.549757</td>\n",
       "      <td>0.568044</td>\n",
       "      <td>0.568044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>0</td>\n",
       "      <td>0.584830</td>\n",
       "      <td>0.584830</td>\n",
       "      <td>0.610451</td>\n",
       "      <td>0.610451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.582827</td>\n",
       "      <td>0.582827</td>\n",
       "      <td>0.600872</td>\n",
       "      <td>0.600872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           experience_flag    pred_x  pred_cold    pred_y  pred_exp\n",
       "unique_id                                                          \n",
       "9                        1  0.986811   0.986811  0.987962  0.987962\n",
       "18                       1  0.964566   0.964566  0.964265  0.964265\n",
       "21                       1  0.945521   0.945521  0.935638  0.935638\n",
       "25                       1  0.994099   0.994099  0.993278  0.993278\n",
       "31                       1  0.960582   0.960582  0.962000  0.962000\n",
       "...                    ...       ...        ...       ...       ...\n",
       "7982                     0  0.648155   0.648155  0.640795  0.640795\n",
       "7990                     0  0.380371   0.380371  0.406924  0.406924\n",
       "7993                     0  0.549757   0.549757  0.568044  0.568044\n",
       "7994                     0  0.584830   0.584830  0.610451  0.610451\n",
       "7998                     0  0.582827   0.582827  0.600872  0.600872\n",
       "\n",
       "[2380 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_lgbm_cold['pred_cold'] = pred_test_lgbm_cold['pred']\n",
    "pred_test_lgbm_exp['pred_exp'] = pred_test_lgbm_exp['pred']\n",
    "\n",
    "sub19_df = pd.merge(X_test[['experience_flag']],pred_test_lgbm_cold,how='left',right_index=True,left_index=True)\n",
    "sub19_df = pd.merge(sub19_df,pred_test_lgbm_exp,how='left',right_index=True,left_index=True)\n",
    "sub19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd6b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub19_df['pred'] = np.where(sub19_df['experience_flag']==1,sub19_df['pred_exp'],sub19_df['pred_cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f905484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.988,0.964,0.936,0.993,0.962,0.995,0.724,0.281,0.841,0.978,0.376,0.108,0.99,0.991,0.906,0.995,0.198,0.23,0.916,0.947,0.996,0.871,0.986,0.983,0.99,0.976,0.979,0.992,0.958,0.751,0.988,0.924,0.964,0.984,0.978,0.989,0.734,0.603,0.99,0.969,0.952,0.887,0.887,0.881,0.991,0.958,0.996,0.463,0.737,0.67,0.175,0.99,0.942,0.072,0.294,0.093,0.973,0.986,0.982,0.992,0.986,0.668,0.172,0.977,0.983,0.786,0.983,0.956,0.98,0.983,0.996,0.947,0.955,0.972,0.935,0.834,0.882,0.99,0.978,0.923,0.306,0.851,0.992,0.144,0.983,0.978,0.645,0.993,0.946,0.893,0.995,0.964,0.866,0.987,0.514,0.931,0.957,0.755,0.357,0.977,0.951,0.983,0.988,0.974,0.672,0.65,0.678,0.598,0.953,0.521,0.974,0.999,0.951,0.996,0.482,0.95,0.903,0.968,0.943,0.94,0.996,0.997,0.537,0.862,0.955,0.99,0.963,0.985,0.267,0.115,0.996,0.861,0.118,0.968,0.939,0.986,0.729,0.984,0.727,0.971,0.194,0.71,0.968,0.948,0.55,0.974,0.674,0.936,0.201,0.976,0.991,0.984,0.951,0.993,0.334,0.785,0.969,0.948,0.991,0.828,0.98,0.958,0.97,0.831,0.991,0.956,0.798,0.913,0.966,0.495,0.981,0.627,0.71,0.996,0.974,0.935,0.99,0.87,0.71,0.949,0.438,0.197,0.892,0.442,0.423,0.986,0.981,0.994,0.982,0.962,0.99,0.226,0.685,0.89,0.902,0.153,0.965,0.323,0.99,0.853,0.91,0.991,0.993,0.98,0.733,0.748,0.229,0.278,0.461,0.083,0.991,0.986,0.429,0.997,0.968,0.576,0.984,0.988,0.728,0.99,0.808,0.968,0.523,0.986,0.986,0.531,0.078,0.973,0.45,0.161,0.076,0.939,0.828,0.984,0.352,0.974,0.996,0.916,0.989,0.983,0.035,0.987,0.964,0.984,0.993,0.394,0.898,0.117,0.984,0.933,0.72,0.196,0.969,0.994,0.038,0.852,0.995,0.99,0.339,0.104,0.985,0.882,0.197,0.982,0.976,0.993,0.856,0.991,0.975,0.143,0.879,0.948,0.994,0.433,0.934,0.91,0.958,0.97,0.978,0.947,0.992,0.97,0.218,0.984,0.828,0.149,0.989,0.953,0.599,0.976,0.066,0.223,0.188,0.948,0.649,0.11,0.975,0.098,0.59,0.996,0.983,0.982,0.979,0.98,0.992,0.983,0.894,0.976,0.983,0.952,0.996,0.932,0.18,0.919,0.963,0.23,0.991,0.433,0.76,0.627,0.952,0.952,0.169,0.631,0.992,0.837,0.356,0.78,0.738,0.587,0.069,0.989,0.14,0.99,0.935,0.818,0.935,0.994,0.09,0.866,0.884,0.967,0.923,0.994,0.936,0.989,0.852,0.071,0.748,0.991,0.247,0.335,0.965,0.951,0.421,0.983,0.946,0.971,0.993,0.998,0.987,0.952,0.227,0.939,0.923,0.948,0.213,0.978,0.976,0.192,0.97,0.919,0.981,0.985,0.807,0.913,0.934,0.982,0.974,0.788,0.867,0.989,0.251,0.775,0.978,0.958,0.088,0.992,0.992,0.946,0.204,0.996,0.659,0.996,0.933,0.987,0.994,0.995,0.984,0.94,0.848,0.991,0.969,0.787,0.992,0.852,0.203,0.117,0.858,0.094,0.315,0.243,0.865,0.642,0.909,0.103,0.961,0.2,0.463,0.769,0.988,0.943,0.963,0.976,0.249,0.29,0.539,0.225,0.98,0.956,0.733,0.153,0.095,0.993,0.617,0.973,0.971,0.891,0.117,0.966,0.269,0.973,0.98,0.596,0.981,0.84,0.334,0.989,0.096,0.588,0.96,0.912,0.232,0.975,0.785,0.937,0.99,0.994,0.955,0.975,0.993,0.618,0.955,0.877,0.882,0.971,0.154,0.571,0.336,0.968,0.737,0.504,0.122,0.964,0.894,0.931,0.964,0.984,0.392,0.866,0.13,0.946,0.989,0.098,0.154,0.155,0.919,0.942,0.986,0.14,0.099,0.892,0.32,0.96,0.544,0.992,0.984,0.703,0.49,0.17,0.982,0.898,0.993,0.659,0.105,0.99,0.452,0.993,0.987,0.578,0.938,0.973,0.952,0.615,0.236,0.062,0.142,0.482,0.716,0.121,0.501,0.364,0.154,0.267,0.983,0.974,0.947,0.991,0.993,0.98,0.077,0.99,0.86,0.086,0.995,0.689,0.698,0.165,0.893,0.855,0.161,0.981,0.479,0.474,0.932,0.306,0.109,0.98,0.463,0.456,0.97,0.977,0.955,0.98,0.946,0.978,0.095,0.76,0.358,0.931,0.991,0.36,0.864,0.973,0.217,0.958,0.97,0.338,0.271,0.962,0.977,0.829,0.198,0.16,0.98,0.967,0.46,0.958,0.984,0.975,0.979,0.275,0.961,0.997,0.59,0.993,0.986,0.743,0.975,0.943,0.945,0.993,0.939,0.41,0.962,0.849,0.997,0.429,0.133,0.956,0.975,0.994,0.602,0.986,0.336,0.895,0.39,0.987,0.976,0.976,0.981,0.927,0.05,0.718,0.33,0.964,0.754,0.116,0.178,0.082,0.986,0.935,0.94,0.977,0.98,0.416,0.066,0.977,0.986,0.958,0.818,0.924,0.596,0.076,0.954,0.559,0.305,0.111,0.953,0.993,0.985,0.973,0.972,0.588,0.968,0.235,0.495,0.965,0.997,0.904,0.974,0.66,0.092,0.892,0.954,0.962,0.9,0.307,0.976,0.989,0.843,0.919,0.272,0.485,0.108,0.248,0.99,0.99,0.985,0.926,0.287,0.952,0.903,0.553,0.736,0.72,0.979,0.959,0.13,0.318,0.461,0.092,0.812,0.989,0.958,0.919,0.183,0.786,0.882,0.789,0.129,0.479,0.847,0.882,0.91,0.913,0.997,0.998,0.156,0.919,0.972,0.983,0.23,0.988,0.882,0.973,0.985,0.423,0.986,0.311,0.993,0.303,0.961,0.131,0.937,0.927,0.962,0.868,0.847,0.241,0.834,0.82,0.988,0.411,0.953,0.927,0.235,0.982,0.314,0.174,0.995,0.83,0.894,0.986,0.877,0.895,0.993,0.993,0.983,0.986,0.95,0.97,0.949,0.993,0.987,0.456,0.627,0.981,0.558,0.961,0.96,0.228,0.581,0.974,0.412,0.89,0.986,0.196,0.404,0.951,0.987,0.992,0.111,0.784,0.995,0.962,0.842,0.35,0.975,0.786,0.075,0.941,0.993,0.28,0.773,0.255,0.278,0.807,0.979,0.799,0.983,0.449,0.961,0.953,0.989,0.554,0.3,0.912,0.996,0.174,0.169,0.966,0.631,0.755,0.778,0.457,0.984,0.199,0.986,0.909,0.231,0.147,0.942,0.053,0.626,0.996,0.928,0.894,0.99,0.989,0.996,0.448,0.97,0.823,0.701,0.906,0.99,0.991,0.964,0.993,0.979,0.978,0.847,0.924,0.809,0.527,0.942,0.804,0.572,0.996,0.91,0.954,0.909,0.496,0.185,0.881,0.401,0.087,0.85,0.432,0.986,0.804,0.988,0.967,0.933,0.986,0.96,0.827,0.986,0.943,0.811,0.876,0.908,0.462,0.434,0.996,0.831,0.427,0.993,0.918,0.949,0.994,0.684,0.992,0.229,0.161,0.193,0.968,0.82,0.384,0.99,0.699,0.981,0.546,0.805,0.96,0.972,0.985,0.058,0.419,0.755,0.338,0.975,0.903,0.724,0.175,0.994,0.695,0.422,0.348,0.99,0.131,0.305,0.653,0.991,0.98,0.88,0.429,0.248,0.621,0.175,0.934,0.837,0.48,0.21,0.878,0.108,0.177,0.615,0.923,0.452,0.656,0.996,0.983,0.6,0.944,0.994,0.949,0.525,0.982,0.933,0.108,0.981,0.843,0.888,0.319,0.953,0.509,0.531,0.115,0.97,0.996,0.98,0.942,0.229,0.965,0.949,0.141,0.937,0.673,0.993,0.197,0.605,0.996,0.971,0.93,0.928,0.993,0.983,0.092,0.986,0.768,0.076,0.993,0.806,0.993,0.975,0.99,0.994,0.438,0.99,0.733,0.476,0.857,0.966,0.967,0.96,0.286,0.964,0.199,0.541,0.212,0.119,0.978,0.989,0.87,0.973,0.915,0.967,0.368,0.989,0.745,0.73,0.485,0.887,0.213,0.971,0.695,0.911,0.997,0.975,0.991,0.915,0.986,0.192,0.982,0.86,0.885,0.958,0.981,0.988,0.679,0.644,0.964,0.962,0.819,0.62,0.3,0.976,0.885,0.25,0.985,0.574,0.924,0.959,0.291,0.291,0.997,0.544,0.986,0.306,0.912,0.561,0.986,0.108,0.58,0.313,0.656,0.421,0.978,0.975,0.983,0.986,0.591,0.986,0.5,0.629,0.098,0.773,0.908,0.174,0.311,0.185,0.382,0.104,0.298,0.969,0.981,0.815,0.979,0.989,0.403,0.856,0.912,0.782,0.931,0.992,0.256,0.67,0.883,0.676,0.554,0.996,0.99,0.871,0.989,0.958,0.972,0.946,0.492,0.957,0.205,0.931,0.981,0.762,0.964,0.882,0.994,0.568,0.95,0.39,0.492,0.406,0.319,0.604,0.848,0.44,0.744,0.539,0.356,0.317,0.542,0.927,0.189,0.441,0.344,0.871,0.976,0.707,0.993,0.829,0.137,0.299,0.752,0.847,0.486,0.526,0.812,0.46,0.251,0.298,0.969,0.989,0.691,0.39,0.567,0.327,0.971,0.845,0.569,0.185,0.997,0.39,0.419,0.823,0.941,0.324,0.46,0.944,0.616,0.133,0.921,0.083,0.983,0.489,0.827,0.957,0.679,0.987,0.127,0.492,0.902,0.615,0.526,0.95,0.358,0.531,0.178,0.957,0.845,0.822,0.162,0.943,0.968,0.966,0.661,0.993,0.994,0.353,0.173,0.178,0.726,0.984,0.979,0.893,0.941,0.683,0.976,0.917,0.632,0.65,0.991,0.124,0.881,0.41,0.991,0.547,0.765,0.078,0.959,0.844,0.483,0.297,0.122,0.922,0.743,0.935,0.085,0.982,0.16,0.14,0.954,0.341,0.152,0.756,0.163,0.883,0.74,0.738,0.479,0.773,0.702,0.838,0.471,0.829,0.201,0.718,0.782,0.636,0.849,0.979,0.369,0.988,0.953,0.97,0.761,0.992,0.922,0.759,0.881,0.873,0.995,0.832,0.397,0.126,0.129,0.64,0.152,0.729,0.683,0.093,0.191,0.941,0.538,0.962,0.983,0.37,0.99,0.396,0.518,0.045,0.253,0.991,0.896,0.121,0.672,0.988,0.991,0.913,0.865,0.803,0.99,0.965,0.542,0.958,0.38,0.975,0.507,0.734,0.604,0.986,0.642,0.993,0.594,0.254,0.125,0.463,0.989,0.99,0.442,0.885,0.153,0.648,0.932,0.96,0.779,0.089,0.97,0.496,0.985,0.986,0.17,0.863,0.97,0.293,0.829,0.823,0.498,0.923,0.851,0.646,0.325,0.185,0.193,0.221,0.8,0.442,0.726,0.964,0.941,0.993,0.701,0.058,0.957,0.271,0.987,0.367,0.984,0.08,0.641,0.803,0.96,0.127,0.185,0.875,0.511,0.524,0.996,0.324,0.084,0.279,0.91,0.982,0.525,0.95,0.23,0.168,0.391,0.518,0.986,0.333,0.976,0.644,0.371,0.61,0.988,0.544,0.754,0.602,0.263,0.268,0.993,0.602,0.749,0.39,0.833,0.864,0.982,0.993,0.251,0.995,0.614,0.414,0.665,0.55,0.308,0.638,0.866,0.677,0.791,0.993,0.352,0.234,0.438,0.937,0.627,0.263,0.987,0.157,0.423,0.92,0.97,0.603,0.944,0.863,0.338,0.524,0.197,0.48,0.525,0.425,0.533,0.518,0.559,0.346,0.963,0.316,0.399,0.224,0.59,0.608,0.944,0.43,0.528,0.555,0.318,0.407,0.409,0.628,0.267,0.706,0.287,0.282,0.944,0.277,0.624,0.57,0.615,0.359,0.955,0.147,0.678,0.521,0.449,0.537,0.217,0.585,0.106,0.751,0.93,0.338,0.378,0.271,0.958,0.847,0.291,0.968,0.945,0.313,0.188,0.148,0.629,0.359,0.753,0.991,0.752,0.734,0.571,0.375,0.373,0.897,0.843,0.733,0.272,0.375,0.978,0.596,0.21,0.254,0.595,0.757,0.391,0.336,0.729,0.886,0.216,0.671,0.915,0.291,0.983,0.926,0.488,0.283,0.97,0.164,0.665,0.277,0.979,0.855,0.189,0.495,0.736,0.6,0.734,0.97,0.661,0.365,0.985,0.26,0.498,0.9,0.989,0.966,0.898,0.692,0.936,0.989,0.327,0.905,0.605,0.838,0.156,0.936,0.942,0.497,0.339,0.938,0.994,0.619,0.843,0.553,0.949,0.987,0.983,0.613,0.987,0.841,0.955,0.86,0.941,0.454,0.554,0.142,0.973,0.341,0.937,0.271,0.965,0.656,0.801,0.761,0.596,0.99,0.932,0.622,0.972,0.997,0.803,0.424,0.961,0.1,0.945,0.974,0.068,0.986,0.208,0.176,0.922,0.54,0.857,0.994,0.737,0.926,0.927,0.105,0.166,0.135,0.782,0.336,0.301,0.294,0.96,0.191,0.417,0.975,0.243,0.947,0.897,0.978,0.939,0.565,0.284,0.979,0.052,0.777,0.941,0.987,0.954,0.354,0.673,0.923,0.99,0.752,0.892,0.945,0.113,0.817,0.162,0.265,0.948,0.583,0.938,0.249,0.6,0.901,0.166,0.909,0.506,0.893,0.95,0.987,0.852,0.293,0.993,0.139,0.909,0.483,0.302,0.871,0.987,0.96,0.69,0.911,0.957,0.109,0.41,0.423,0.169,0.761,0.317,0.284,0.973,0.959,0.53,0.948,0.984,0.178,0.242,0.943,0.35,0.904,0.564,0.957,0.147,0.979,0.084,0.521,0.312,0.383,0.809,0.455,0.161,0.415,0.244,0.941,0.928,0.893,0.827,0.955,0.271,0.85,0.943,0.275,0.33,0.123,0.986,0.956,0.913,0.895,0.973,0.777,0.954,0.977,0.839,0.17,0.95,0.93,0.816,0.961,0.151,0.253,0.979,0.879,0.232,0.916,0.745,0.978,0.984,0.974,0.868,0.879,0.984,0.961,0.957,0.828,0.69,0.524,0.312,0.767,0.493,0.917,0.171,0.933,0.975,0.234,0.393,0.879,0.396,0.96,0.84,0.954,0.801,0.95,0.32,0.272,0.982,0.975,0.351,0.955,0.209,0.309,0.78,0.275,0.61,0.965,0.879,0.481,0.985,0.97,0.844,0.568,0.957,0.136,0.868,0.92,0.823,0.978,0.938,0.691,0.851,0.927,0.774,0.906,0.736,0.656,0.233,0.204,0.182,0.165,0.584,0.976,0.933,0.109,0.957,0.884,0.116,0.788,0.827,0.627,0.278,0.113,0.264,0.66,0.972,0.948,0.123,0.962,0.718,0.571,0.258,0.533,0.389,0.259,0.849,0.391,0.436,0.874,0.989,0.232,0.517,0.291,0.822,0.359,0.582,0.687,0.9,0.68,0.169,0.376,0.302,0.432,0.967,0.772,0.188,0.427,0.871,0.902,0.906,0.232,0.697,0.605,0.847,0.835,0.958,0.927,0.709,0.845,0.923,0.772,0.902,0.419,0.794,0.768,0.897,0.869,0.405,0.102,0.109,0.358,0.945,0.408,0.14,0.874,0.376,0.756,0.114,0.301,0.801,0.179,0.204,0.763,0.621,0.717,0.621,0.571,0.586,0.2,0.229,0.746,0.722,0.763,0.41,0.81,0.739,0.619,0.864,0.495,0.755,0.809,0.12,0.63,0.715,0.828,0.09,0.735,0.253,0.89,0.63,0.737,0.071,0.721,0.647,0.756,0.618,0.65,0.729,0.65,0.662,0.085,0.657,0.675,0.693,0.227,0.547,0.367,0.39,0.29,0.576,0.589,0.668,0.634,0.537,0.385,0.391,0.171,0.835,0.714,0.677,0.647,0.49,0.734,0.631,0.144,0.457,0.245,0.747,0.419,0.204,0.899,0.849,0.939,0.873,0.968,0.661,0.774,0.908,0.648,0.84,0.669,0.896,0.906,0.828,0.847,0.768,0.786,0.828,0.347,0.849,0.577,0.728,0.826,0.448,0.612,0.628,0.46,0.962,0.811,0.908,0.259,0.775,0.928,0.987,0.962,0.902,0.895,0.843,0.2,0.601,0.466,0.353,0.884,0.527,0.545,0.417,0.823,0.796,0.308,0.79,0.463,0.845,0.82,0.438,0.487,0.793,0.483,0.679,0.378,0.736,0.841,0.505,0.333,0.321,0.787,0.302,0.84,0.587,0.227,0.755,0.228,0.881,0.349,0.145,0.828,0.816,0.386,0.637,0.802,0.399,0.789,0.946,0.868,0.732,0.771,0.911,0.898,0.852,0.29,0.712,0.829,0.552,0.436,0.492,0.245,0.641,0.799,0.221,0.296,0.226,0.239,0.869,0.78,0.359,0.469,0.269,0.61,0.46,0.649,0.082,0.236,0.812,0.236,0.658,0.918,0.554,0.64,0.478,0.487,0.761,0.475,0.827,0.487,0.652,0.58,0.651,0.664,0.798,0.892,0.514,0.661,0.62,0.534,0.421,0.475,0.445,0.671,0.726,0.888,0.834,0.804,0.6,0.877,0.807,0.781,0.897,0.508,0.533,0.419,0.28,0.455,0.758,0.794,0.463,0.476,0.254,0.374,0.473,0.767,0.735,0.66,0.388,0.556,0.495,0.458,0.343,0.832,0.833,0.702,0.676,0.867,0.551,0.423,0.452,0.663,0.527,0.725,0.792,0.416,0.405,0.783,0.508,0.612,0.502,0.516,0.474,0.265,0.457,0.494,0.802,0.597,0.903,0.811,0.218,0.572,0.868,0.857,0.67,0.404,0.542,0.584,0.403,0.601,0.281,0.716,0.687,0.181,0.589,0.508,0.594,0.251,0.415,0.205,0.527,0.739,0.453,0.54,0.898,0.753,0.402,0.823,0.612,0.853,0.521,0.253,0.588,0.754,0.799,0.688,0.291,0.322,0.885,0.84,0.611,0.518,0.45,0.229,0.336,0.567,0.419,0.455,0.801,0.574,0.732,0.225,0.244,0.317,0.719,0.65,0.901,0.238,0.758,0.451,0.862,0.689,0.88,0.744,0.301,0.595,0.262,0.571,0.481,0.607,0.62,0.376,0.687,0.62,0.646,0.595,0.8,0.71,0.695,0.647,0.474,0.557,0.311,0.436,0.85,0.517,0.551,0.779,0.64,0.73,0.757,0.422,0.538,0.268,0.545,0.902,0.711,0.297,0.791,0.319,0.9,0.702,0.33,0.275,0.279,0.498,0.459,0.65,0.584,0.732,0.502,0.563,0.545,0.737,0.891,0.401,0.653,0.821,0.483,0.41,0.67,0.448,0.638,0.386,0.696,0.232,0.538,0.84,0.467,0.43,0.536,0.64,0.41,0.676,0.443,0.519,0.582,0.666,0.479,0.716,0.568,0.243,0.219,0.585,0.585,0.597,0.617,0.546,0.841,0.428,0.773,0.629,0.783,0.447,0.751,0.595,0.526,0.756,0.778,0.684,0.806,0.596,0.567,0.561,0.578,0.729,0.661,0.773,0.433,0.69,0.509,0.411,0.463,0.699,0.776,0.583,0.38,0.325,0.498,0.432,0.239,0.486,0.426,0.378,0.729,0.813,0.755,0.332,0.576,0.656,0.685,0.495,0.474,0.575,0.6,0.708,0.59,0.434,0.536,0.632,0.43,0.632,0.505,0.565,0.818,0.526,0.416,0.266,0.744,0.624,0.56,0.691,0.418,0.587,0.715,0.623,0.375,0.734,0.683,0.791,0.656,0.584,0.466,0.693,0.44,0.441,0.477,0.568,0.533,0.556,0.446,0.297,0.75,0.477,0.432,0.573,0.69,0.653,0.693,0.645,0.311,0.696,0.708,0.536,0.539,0.556,0.5,0.641,0.656,0.82,0.388,0.499,0.74,0.424,0.453,0.421,0.434,0.696,0.704,0.557,0.612,0.89,0.519,0.482,0.55,0.577,0.508,0.522,0.478,0.426,0.707,0.611,0.51,0.424,0.732,0.691,0.547,0.689,0.617,0.466,0.551,0.538,0.363,0.541,0.508,0.363,0.326,0.639,0.64,0.475,0.62,0.591,0.589,0.44,0.842,0.417,0.578,0.578,0.472,0.599,0.509,0.523,0.731,0.212,0.648,0.38,0.55,0.585,0.583'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub19 = pd.merge(id_test_df,sub19_df.reset_index(),how='left',on='unique_id')\n",
    "sub19['pred'] = sub19['pred'].apply(lambda x: round(x,3))\n",
    "sub19_txt = ''\n",
    "for prob in list(sub19['pred'].values):\n",
    "    sub19_txt = sub19_txt+','+str(prob)\n",
    "sub19_txt = sub19_txt[1:]\n",
    "\n",
    "sub19_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c8661",
   "metadata": {},
   "source": [
    "### Submission 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09999b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:17:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=50,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5)\n",
    "rf.fit(X_train_experienced_under_imputed,y_train_experienced_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee2d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9922903803429884\n",
      "Val ROC:  0.8491632745364088\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b2332fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.408062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.126785              0.0        0.126785       0.265943   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.142119      0.408062  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdddfbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.993,0.863,1.0,0.997,1.0,0.483,0.001,0.997,0.99,0.041,0.007,1.0,1.0,0.978,0.999,0.008,0.011,0.998,0.998,1.0,0.988,0.999,1.0,1.0,0.999,0.994,0.999,0.84,0.991,0.999,0.984,0.993,0.98,0.999,0.999,0.788,0.418,0.997,0.825,0.981,0.978,0.991,0.996,1.0,0.994,1.0,0.182,0.31,0.961,0.002,1.0,1.0,0.024,0.014,0.007,0.998,1.0,1.0,0.999,1.0,0.296,0.097,1.0,0.999,0.91,0.996,0.993,0.997,1.0,0.998,0.999,0.998,0.999,0.955,0.265,0.998,1.0,0.992,0.998,0.024,0.918,1.0,0.112,1.0,0.929,0.63,1.0,0.995,0.994,1.0,0.993,0.313,1.0,0.049,0.999,0.977,0.951,0.375,0.996,0.993,0.999,1.0,1.0,0.963,0.104,0.67,0.641,0.999,0.667,0.994,1.0,0.996,0.999,0.826,0.991,0.992,0.996,0.998,0.994,1.0,1.0,0.188,0.967,0.999,0.968,1.0,1.0,0.004,0.002,0.998,0.725,0.001,0.996,0.917,1.0,0.527,0.999,0.996,0.996,0.028,0.525,0.999,0.956,0.965,0.999,0.956,0.999,0.017,1.0,1.0,0.999,1.0,1.0,0.2,0.967,0.991,0.995,1.0,0.945,1.0,0.994,0.998,0.971,0.999,0.987,0.961,0.992,0.996,0.145,1.0,0.714,0.717,0.996,1.0,0.903,0.999,0.993,0.025,0.999,0.28,0.02,0.998,0.612,0.86,1.0,0.997,1.0,1.0,0.956,0.999,0.025,0.049,0.985,0.982,0.0,0.995,0.064,1.0,0.771,0.993,1.0,1.0,0.999,0.915,0.247,0.022,0.068,0.047,0.001,1.0,0.997,0.013,1.0,0.999,0.296,0.996,1.0,0.582,0.999,0.955,0.999,0.013,0.999,0.999,0.092,0.007,0.997,0.112,0.016,0.001,0.983,0.679,0.999,0.046,1.0,1.0,0.957,1.0,1.0,0.002,0.998,0.994,1.0,1.0,0.045,0.643,0.002,0.996,0.983,0.685,0.001,0.999,1.0,0.003,0.978,1.0,1.0,0.012,0.001,1.0,0.85,0.002,0.998,1.0,1.0,0.961,1.0,0.996,0.007,0.979,0.981,1.0,0.215,0.999,0.467,1.0,1.0,1.0,0.921,0.999,0.995,0.003,1.0,0.979,0.008,1.0,0.998,0.15,0.999,0.014,0.004,0.099,1.0,0.749,0.003,0.992,0.0,0.258,1.0,0.999,0.992,0.994,0.998,1.0,0.997,0.997,1.0,1.0,1.0,1.0,0.988,0.023,1.0,0.995,0.0,1.0,0.247,0.815,0.356,0.991,0.99,0.001,0.49,1.0,0.991,0.039,0.627,0.498,0.769,0.011,0.998,0.008,0.999,0.9,0.538,1.0,1.0,0.005,0.92,0.995,0.979,0.977,0.998,0.997,0.998,0.97,0.002,0.993,0.999,0.002,0.115,0.999,0.995,0.027,1.0,0.401,0.994,1.0,1.0,1.0,0.999,0.05,0.986,0.916,0.998,0.003,1.0,0.999,0.007,1.0,0.999,0.994,1.0,0.952,0.993,0.999,1.0,0.999,0.918,0.961,0.991,0.005,0.973,0.996,0.999,0.005,1.0,1.0,0.993,0.052,1.0,0.516,1.0,0.998,1.0,1.0,1.0,1.0,0.998,0.991,1.0,1.0,0.957,1.0,0.992,0.002,0.001,0.972,0.009,0.013,0.044,0.976,0.339,0.998,0.002,0.998,0.002,0.006,0.977,0.999,0.985,0.992,1.0,0.017,0.013,0.176,0.046,1.0,0.992,0.762,0.009,0.004,1.0,0.02,1.0,0.998,0.988,0.001,0.999,0.032,0.997,1.0,0.091,1.0,0.981,0.164,1.0,0.007,0.186,0.992,0.665,0.005,1.0,0.981,0.999,1.0,1.0,0.997,1.0,0.999,0.031,0.98,0.993,0.833,0.999,0.002,0.597,0.006,0.994,0.399,0.168,0.059,0.987,0.981,0.976,0.989,0.998,0.007,0.984,0.02,0.999,1.0,0.001,0.005,0.013,0.992,0.995,0.999,0.003,0.001,0.981,0.019,0.997,0.453,1.0,1.0,0.216,0.022,0.009,1.0,0.954,1.0,0.987,0.001,1.0,0.058,0.999,1.0,0.702,0.998,0.939,0.995,0.873,0.12,0.01,0.009,0.004,0.705,0.012,0.646,0.114,0.003,0.069,1.0,0.999,0.99,1.0,1.0,0.98,0.004,0.998,0.628,0.001,0.999,0.979,0.083,0.002,0.999,0.953,0.271,0.999,0.003,0.041,0.999,0.013,0.003,0.996,0.11,0.089,0.999,0.991,0.992,0.999,0.966,0.994,0.003,0.928,0.082,1.0,1.0,0.003,0.856,1.0,0.022,1.0,0.946,0.062,0.041,0.985,1.0,0.977,0.0,0.004,1.0,0.999,0.53,0.997,0.998,1.0,1.0,0.051,0.998,0.998,0.305,1.0,0.999,0.688,1.0,0.942,0.966,1.0,0.984,0.002,0.998,0.979,1.0,0.004,0.001,0.993,0.988,1.0,0.732,0.998,0.004,0.416,0.023,1.0,0.997,0.938,0.999,0.996,0.002,0.257,0.004,1.0,0.951,0.002,0.005,0.0,0.999,0.989,0.999,0.999,0.998,0.186,0.005,1.0,1.0,1.0,0.244,0.987,0.028,0.002,0.999,0.74,0.038,0.005,1.0,0.999,0.999,1.0,0.895,0.249,1.0,0.005,0.032,0.971,1.0,0.992,0.999,0.128,0.001,0.964,0.999,0.999,0.992,0.042,0.997,0.999,0.735,0.961,0.017,0.012,0.006,0.111,0.999,1.0,1.0,0.996,0.192,1.0,0.939,0.171,0.871,0.861,1.0,0.985,0.002,0.0,0.076,0.0,0.968,0.999,0.982,0.994,0.003,0.894,0.992,0.965,0.009,0.033,0.803,0.912,0.99,0.287,1.0,1.0,0.019,0.996,0.995,1.0,0.164,1.0,0.998,1.0,1.0,0.006,1.0,0.032,1.0,0.05,0.995,0.002,0.996,0.99,0.993,0.998,0.966,0.107,0.927,0.857,1.0,0.004,0.999,0.951,0.014,0.999,0.005,0.004,1.0,0.879,0.977,0.999,0.931,0.991,1.0,0.999,0.999,1.0,0.999,0.999,0.975,1.0,1.0,0.009,0.069,0.992,0.059,0.997,0.998,0.022,0.336,0.999,0.034,0.957,0.999,0.016,0.024,0.994,1.0,0.997,0.004,0.948,1.0,0.9,0.991,0.005,0.999,0.439,0.003,1.0,1.0,0.006,0.135,0.026,0.153,0.981,0.999,0.581,0.999,0.045,0.994,0.965,1.0,0.12,0.006,0.997,1.0,0.006,0.004,1.0,0.341,0.752,0.934,0.115,1.0,0.001,0.999,0.708,0.004,0.001,0.987,0.002,0.334,1.0,0.957,0.99,1.0,1.0,0.999,0.013,0.979,0.991,0.436,0.988,1.0,0.999,0.987,0.999,0.997,1.0,0.74,0.542,0.98,0.029,0.991,0.878,0.001,1.0,0.953,0.996,0.998,0.136,0.115,0.999,0.074,0.001,0.981,0.008,0.999,0.886,1.0,0.999,0.995,0.996,1.0,0.849,0.999,0.966,0.986,0.995,0.993,0.013,0.003,1.0,0.985,0.017,1.0,0.999,1.0,1.0,0.021,1.0,0.044,0.911,0.003,0.778,0.604,0.102,1.0,0.927,1.0,0.084,0.092,1.0,0.998,1.0,0.003,0.026,0.493,0.05,1.0,0.99,0.673,0.03,1.0,0.479,0.627,0.066,1.0,0.006,0.002,0.988,1.0,0.999,0.96,0.098,0.006,0.075,0.018,0.821,0.72,0.023,0.019,0.999,0.022,0.001,0.068,0.898,0.426,0.013,1.0,1.0,0.005,0.976,1.0,0.998,0.81,0.999,0.983,0.007,0.998,0.946,0.96,0.172,0.999,0.15,0.029,0.002,0.997,1.0,0.999,1.0,0.235,0.999,1.0,0.001,0.994,0.66,1.0,0.006,0.126,1.0,1.0,1.0,0.992,0.999,0.992,0.001,1.0,0.762,0.0,1.0,0.966,0.999,1.0,0.999,1.0,0.192,1.0,0.553,0.01,0.997,0.997,0.999,0.998,0.019,0.999,0.005,0.644,0.001,0.001,1.0,1.0,0.916,0.996,0.994,0.996,0.025,1.0,0.929,0.942,0.117,0.984,0.022,0.991,0.891,0.997,1.0,0.988,1.0,0.987,0.995,0.025,1.0,0.661,0.951,0.968,0.996,0.999,0.985,0.089,1.0,0.999,0.992,0.776,0.007,0.994,0.989,0.09,1.0,0.923,0.947,0.984,0.014,0.228,1.0,0.012,1.0,0.001,0.991,0.031,0.999,0.0,0.455,0.01,0.03,0.168,0.993,1.0,0.993,0.994,0.175,1.0,0.044,0.791,0.003,0.921,0.963,0.001,0.03,0.007,0.322,0.001,0.002,0.994,0.999,0.73,0.999,1.0,0.288,0.99,0.989,0.974,0.986,1.0,0.002,0.671,0.615,0.15,0.139,1.0,0.999,0.766,1.0,0.999,0.998,0.989,0.65,0.999,0.048,0.999,0.999,0.749,0.99,0.991,1.0,0.669,0.999,0.126,0.16,0.02,0.144,0.287,0.974,0.066,0.234,0.819,0.021,0.002,0.225,0.953,0.003,0.007,0.245,0.986,0.999,0.676,1.0,0.711,0.001,0.032,0.402,0.894,0.187,0.531,0.144,0.005,0.32,0.129,1.0,0.998,0.978,0.189,0.582,0.024,0.994,0.024,0.706,0.007,1.0,0.051,0.001,0.956,0.991,0.008,0.68,0.991,0.106,0.017,0.998,0.008,1.0,0.102,0.978,0.906,0.983,1.0,0.058,0.024,0.972,0.769,0.062,1.0,0.086,0.494,0.003,0.948,0.988,0.902,0.034,0.957,0.986,0.999,0.118,0.997,1.0,0.028,0.003,0.006,0.973,0.969,0.996,0.905,0.989,0.831,0.986,0.942,0.352,0.319,1.0,0.014,1.0,0.125,1.0,0.016,0.868,0.003,0.995,0.991,0.007,0.004,0.026,0.998,0.971,0.973,0.004,1.0,0.003,0.001,1.0,0.22,0.011,0.801,0.009,0.99,0.79,0.783,0.04,0.98,0.536,0.956,0.015,0.925,0.006,0.722,0.92,0.969,0.992,0.999,0.008,0.995,0.988,1.0,0.463,1.0,0.987,0.918,0.96,0.997,1.0,0.763,0.037,0.004,0.002,0.322,0.015,0.682,0.989,0.003,0.16,0.978,0.117,1.0,0.999,0.013,1.0,0.002,0.349,0.001,0.01,0.999,0.997,0.002,0.974,0.998,1.0,1.0,0.99,0.686,0.999,0.995,0.101,0.979,0.005,0.999,0.197,0.942,0.271,1.0,0.879,0.998,0.048,0.004,0.001,0.012,1.0,1.0,0.043,0.999,0.001,0.704,0.997,0.999,0.958,0.001,0.999,0.018,1.0,1.0,0.021,0.973,0.998,0.001,0.549,0.834,0.049,0.991,0.966,0.417,0.016,0.012,0.0,0.002,0.989,0.006,0.771,0.996,0.777,0.995,0.363,0.001,0.997,0.003,1.0,0.002,0.998,0.001,0.629,0.061,0.997,0.001,0.006,0.953,0.127,0.991,1.0,0.115,0.005,0.744,0.991,1.0,0.018,0.977,0.04,0.007,0.307,0.154,0.999,0.008,0.99,0.086,0.002,0.125,0.995,0.016,0.469,0.305,0.004,0.002,1.0,0.01,0.565,0.009,0.841,0.998,0.999,1.0,0.007,1.0,0.683,0.148,0.948,0.222,0.017,0.612,0.986,0.249,0.989,1.0,0.003,0.036,0.037,0.95,0.615,0.058,0.999,0.001,0.111,0.994,1.0,0.788,0.999,0.966,0.038,0.027,0.004,0.848,0.023,0.061,0.039,0.011,0.12,0.011,1.0,0.002,0.017,0.021,0.139,0.335,0.998,0.794,0.557,0.548,0.124,0.131,0.002,0.635,0.035,0.957,0.007,0.014,0.997,0.004,0.742,0.029,0.104,0.01,0.995,0.354,0.972,0.315,0.021,0.845,0.052,0.758,0.043,0.724,0.76,0.065,0.334,0.001,0.951,0.931,0.17,0.954,0.982,0.028,0.034,0.004,0.105,0.293,0.892,1.0,0.968,0.324,0.033,0.025,0.031,0.857,0.977,0.446,0.039,0.005,0.999,0.408,0.0,0.005,0.959,0.987,0.103,0.034,0.507,0.98,0.022,0.016,0.999,0.019,1.0,0.946,0.004,0.004,0.943,0.106,0.833,0.05,0.999,0.987,0.101,0.068,0.922,0.026,0.629,1.0,0.193,0.01,0.999,0.029,0.036,0.996,0.999,0.996,0.982,0.791,0.996,0.997,0.029,0.879,0.452,0.999,0.011,0.993,0.988,0.21,0.079,0.943,0.999,0.081,0.95,0.052,0.996,1.0,1.0,0.822,1.0,0.604,0.999,0.994,0.998,0.112,0.524,0.007,0.998,0.012,0.998,0.013,0.994,0.115,0.99,0.039,0.137,1.0,0.964,0.94,1.0,1.0,0.607,0.016,0.998,0.009,0.995,1.0,0.0,1.0,0.017,0.016,0.935,0.026,0.979,1.0,0.115,0.996,0.995,0.001,0.0,0.006,0.928,0.045,0.001,0.019,0.994,0.125,0.056,1.0,0.011,0.999,0.98,1.0,1.0,0.071,0.022,1.0,0.001,0.816,0.778,1.0,0.997,0.056,0.43,0.965,0.999,0.956,0.993,0.992,0.004,0.848,0.001,0.004,0.992,0.476,0.959,0.007,0.577,0.987,0.003,0.996,0.098,0.981,0.998,1.0,0.944,0.265,1.0,0.127,0.987,0.346,0.014,0.995,0.987,0.999,0.778,0.994,0.99,0.001,0.027,0.001,0.002,0.996,0.065,0.001,0.999,0.995,0.134,0.991,1.0,0.004,0.075,0.989,0.004,0.668,0.029,0.999,0.039,0.999,0.001,0.002,0.0,0.048,0.82,0.074,0.004,0.013,0.018,0.77,0.998,0.998,0.999,0.999,0.022,0.962,0.999,0.007,0.067,0.009,0.995,0.993,0.984,0.999,0.999,0.967,0.993,0.999,0.993,0.001,0.995,0.993,0.949,0.997,0.001,0.003,0.999,0.998,0.004,0.984,0.881,0.999,0.995,0.998,0.989,0.998,0.995,0.997,0.981,0.599,0.979,0.661,0.153,0.348,0.279,0.896,0.006,0.999,0.986,0.002,0.238,0.992,0.028,1.0,0.974,0.975,0.307,0.996,0.035,0.029,0.997,0.992,0.003,0.932,0.003,0.104,0.2,0.005,0.064,0.998,0.999,0.022,1.0,0.996,0.874,0.66,0.977,0.035,0.995,1.0,0.49,0.999,0.999,0.981,0.707,0.991,0.737,0.985,0.515,0.778,0.019,0.001,0.001,0.018,0.555,1.0,0.999,0.0,0.977,0.991,0.016,0.732,0.971,0.134,0.001,0.002,0.12,0.391,0.999,0.995,0.008,0.982,0.107,0.009,0.062,0.063,0.287,0.074,0.995,0.517,0.092,0.998,1.0,0.005,0.001,0.035,0.976,0.01,0.167,0.215,0.996,0.971,0.009,0.03,0.045,0.036,0.999,0.895,0.148,0.165,0.974,0.966,0.794,0.01,0.761,0.116,0.994,0.96,0.998,0.997,0.095,0.826,0.996,0.043,0.952,0.01,0.997,0.977,0.958,0.94,0.022,0.0,0.0,0.017,0.998,0.011,0.004,0.982,0.046,0.977,0.033,0.003,0.184,0.005,0.0,0.94,0.997,0.054,0.997,0.006,0.112,0.017,0.186,0.576,0.89,0.991,0.03,0.33,0.879,0.331,0.932,0.364,0.955,0.867,0.02,0.212,0.742,0.955,0.025,0.669,0.053,0.963,0.098,0.567,0.0,0.785,0.235,0.96,0.074,0.778,0.582,0.86,0.941,0.0,0.716,0.513,0.981,0.001,0.241,0.019,0.201,0.015,0.499,0.236,0.182,0.865,0.078,0.908,0.03,0.087,0.528,0.978,0.994,0.9,0.074,0.967,0.896,0.003,0.584,0.007,0.93,0.024,0.001,0.969,0.992,0.967,0.995,0.999,0.833,0.988,0.143,0.682,0.978,0.786,0.998,0.991,0.98,0.596,0.823,0.754,0.929,0.477,0.997,0.079,0.319,0.957,0.431,0.122,0.04,0.082,0.999,0.95,0.998,0.026,0.988,0.776,1.0,0.999,0.995,0.992,0.835,0.007,0.94,0.003,0.141,0.976,0.205,0.082,0.017,0.988,0.937,0.01,0.963,0.077,0.989,0.979,0.013,0.518,0.437,0.015,0.228,0.002,0.809,0.954,0.311,0.046,0.258,0.959,0.026,0.747,0.907,0.002,0.72,0.008,0.953,0.032,0.005,0.994,0.878,0.015,0.694,0.618,0.028,0.798,0.996,0.822,0.808,0.935,0.963,0.991,0.999,0.019,0.819,0.859,0.104,0.072,0.06,0.001,0.763,0.498,0.01,0.016,0.002,0.106,0.913,0.993,0.0,0.009,0.047,0.007,0.221,0.098,0.0,0.01,0.73,0.003,0.151,0.975,0.398,0.147,0.097,0.079,0.752,0.212,0.999,0.029,0.07,0.091,0.94,0.836,0.684,0.336,0.002,0.317,0.871,0.021,0.008,0.004,0.01,0.137,0.236,0.983,0.996,0.858,0.345,0.999,0.981,0.635,0.992,0.031,0.443,0.005,0.033,0.309,0.988,0.93,0.402,0.008,0.003,0.006,0.002,0.955,0.096,0.068,0.006,0.218,0.413,0.783,0.128,0.8,0.584,0.288,0.89,0.989,0.286,0.012,0.92,0.967,0.639,0.988,0.503,0.009,0.021,0.65,0.095,0.031,0.038,0.327,0.015,0.001,0.007,0.347,0.196,0.549,1.0,0.991,0.031,0.191,0.134,0.956,0.741,0.015,0.017,0.951,0.07,0.173,0.112,0.469,0.107,0.028,0.575,0.492,0.123,0.004,0.139,0.178,0.082,0.67,0.015,0.134,1.0,0.259,0.036,0.893,0.074,0.989,0.481,0.03,0.092,0.336,0.991,0.511,0.004,0.019,0.874,0.981,0.045,0.026,0.007,0.0,0.01,0.011,0.013,0.176,0.951,0.106,0.825,0.002,0.027,0.002,0.04,0.915,0.991,0.002,0.692,0.001,0.997,0.949,0.978,0.993,0.281,0.028,0.0,0.008,0.018,0.145,0.552,0.087,0.906,0.784,0.102,0.283,0.971,0.619,0.52,0.068,0.004,0.635,0.492,0.109,0.998,0.055,0.61,0.743,0.07,0.988,0.641,0.038,0.257,0.003,0.008,0.972,0.853,0.002,0.772,0.016,0.991,0.393,0.171,0.293,0.019,0.68,0.012,0.005,0.039,0.617,0.806,0.214,0.484,0.437,0.98,0.005,0.754,0.926,0.243,0.031,0.022,0.084,0.857,0.011,0.964,0.062,0.057,0.982,0.04,0.071,0.076,0.923,0.055,0.851,0.086,0.574,0.303,0.423,0.018,0.612,0.122,0.004,0.001,0.046,0.477,0.039,0.033,0.192,0.992,0.004,0.946,0.401,0.991,0.009,0.336,0.002,0.243,0.976,0.961,0.007,0.995,0.97,0.655,0.958,0.927,0.829,0.841,0.75,0.004,0.374,0.004,0.022,0.108,0.877,0.578,0.089,0.005,0.02,0.001,0.016,0.017,0.009,0.004,0.08,0.515,0.743,0.781,0.026,0.349,0.486,0.38,0.773,0.006,0.059,0.008,0.961,0.172,0.01,0.2,0.358,0.009,0.69,0.015,0.108,0.993,0.003,0.0,0.062,0.377,0.312,0.014,0.905,0.024,0.586,0.061,0.227,0.014,0.974,0.812,0.965,0.715,0.763,0.015,0.858,0.153,0.17,0.291,0.235,0.019,0.074,0.03,0.001,0.963,0.004,0.019,0.013,0.673,0.272,0.711,0.018,0.003,0.194,0.996,0.085,0.162,0.03,0.067,0.561,0.749,0.999,0.002,0.313,0.985,0.001,0.064,0.034,0.037,0.987,0.493,0.005,0.049,0.997,0.015,0.029,0.509,0.797,0.895,0.002,0.154,0.014,0.055,0.976,0.007,0.009,0.249,0.72,0.051,0.128,0.947,0.898,0.244,0.021,0.008,0.645,0.106,0.007,0.001,0.373,0.581,0.852,0.349,0.025,0.186,0.001,0.953,0.001,0.049,0.02,0.002,0.484,0.027,0.266,0.786,0.0,0.039,0.053,0.158,0.385,0.023'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub20 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub20['pred'] = sub20['pred'].apply(lambda x: round(x,3))\n",
    "sub20_txt = ''\n",
    "for prob in list(sub20['pred'].values):\n",
    "    sub20_txt = sub20_txt+','+str(prob)\n",
    "sub20_txt = sub20_txt[1:]\n",
    "\n",
    "sub20_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51419e",
   "metadata": {},
   "source": [
    "### Submission 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfd5fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:23:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=200,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5)\n",
    "rf.fit(X_train_experienced_under_imputed,y_train_experienced_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b63d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9922903803429884\n",
      "Val ROC:  0.8491632745364088\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ffa74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.408062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.126785              0.0        0.126785       0.265943   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.142119      0.408062  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53c3181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.993,0.863,1.0,0.997,1.0,0.483,0.001,0.997,0.99,0.041,0.007,1.0,1.0,0.978,0.999,0.008,0.011,0.998,0.998,1.0,0.988,0.999,1.0,1.0,0.999,0.994,0.999,0.84,0.991,0.999,0.984,0.993,0.98,0.999,0.999,0.788,0.418,0.997,0.825,0.981,0.978,0.991,0.996,1.0,0.994,1.0,0.182,0.31,0.961,0.002,1.0,1.0,0.024,0.014,0.007,0.998,1.0,1.0,0.999,1.0,0.296,0.097,1.0,0.999,0.91,0.996,0.993,0.997,1.0,0.998,0.999,0.998,0.999,0.955,0.265,0.998,1.0,0.992,0.998,0.024,0.918,1.0,0.112,1.0,0.929,0.63,1.0,0.995,0.994,1.0,0.993,0.313,1.0,0.049,0.999,0.977,0.951,0.375,0.996,0.993,0.999,1.0,1.0,0.963,0.104,0.67,0.641,0.999,0.667,0.994,1.0,0.996,0.999,0.826,0.991,0.992,0.996,0.998,0.994,1.0,1.0,0.188,0.967,0.999,0.968,1.0,1.0,0.004,0.002,0.998,0.725,0.001,0.996,0.917,1.0,0.527,0.999,0.996,0.996,0.028,0.525,0.999,0.956,0.965,0.999,0.956,0.999,0.017,1.0,1.0,0.999,1.0,1.0,0.2,0.967,0.991,0.995,1.0,0.945,1.0,0.994,0.998,0.971,0.999,0.987,0.961,0.992,0.996,0.145,1.0,0.714,0.717,0.996,1.0,0.903,0.999,0.993,0.025,0.999,0.28,0.02,0.998,0.612,0.86,1.0,0.997,1.0,1.0,0.956,0.999,0.025,0.049,0.985,0.982,0.0,0.995,0.064,1.0,0.771,0.993,1.0,1.0,0.999,0.915,0.247,0.022,0.068,0.047,0.001,1.0,0.997,0.013,1.0,0.999,0.296,0.996,1.0,0.582,0.999,0.955,0.999,0.013,0.999,0.999,0.092,0.007,0.997,0.112,0.016,0.001,0.983,0.679,0.999,0.046,1.0,1.0,0.957,1.0,1.0,0.002,0.998,0.994,1.0,1.0,0.045,0.643,0.002,0.996,0.983,0.685,0.001,0.999,1.0,0.003,0.978,1.0,1.0,0.012,0.001,1.0,0.85,0.002,0.998,1.0,1.0,0.961,1.0,0.996,0.007,0.979,0.981,1.0,0.215,0.999,0.467,1.0,1.0,1.0,0.921,0.999,0.995,0.003,1.0,0.979,0.008,1.0,0.998,0.15,0.999,0.014,0.004,0.099,1.0,0.749,0.003,0.992,0.0,0.258,1.0,0.999,0.992,0.994,0.998,1.0,0.997,0.997,1.0,1.0,1.0,1.0,0.988,0.023,1.0,0.995,0.0,1.0,0.247,0.815,0.356,0.991,0.99,0.001,0.49,1.0,0.991,0.039,0.627,0.498,0.769,0.011,0.998,0.008,0.999,0.9,0.538,1.0,1.0,0.005,0.92,0.995,0.979,0.977,0.998,0.997,0.998,0.97,0.002,0.993,0.999,0.002,0.115,0.999,0.995,0.027,1.0,0.401,0.994,1.0,1.0,1.0,0.999,0.05,0.986,0.916,0.998,0.003,1.0,0.999,0.007,1.0,0.999,0.994,1.0,0.952,0.993,0.999,1.0,0.999,0.918,0.961,0.991,0.005,0.973,0.996,0.999,0.005,1.0,1.0,0.993,0.052,1.0,0.516,1.0,0.998,1.0,1.0,1.0,1.0,0.998,0.991,1.0,1.0,0.957,1.0,0.992,0.002,0.001,0.972,0.009,0.013,0.044,0.976,0.339,0.998,0.002,0.998,0.002,0.006,0.977,0.999,0.985,0.992,1.0,0.017,0.013,0.176,0.046,1.0,0.992,0.762,0.009,0.004,1.0,0.02,1.0,0.998,0.988,0.001,0.999,0.032,0.997,1.0,0.091,1.0,0.981,0.164,1.0,0.007,0.186,0.992,0.665,0.005,1.0,0.981,0.999,1.0,1.0,0.997,1.0,0.999,0.031,0.98,0.993,0.833,0.999,0.002,0.597,0.006,0.994,0.399,0.168,0.059,0.987,0.981,0.976,0.989,0.998,0.007,0.984,0.02,0.999,1.0,0.001,0.005,0.013,0.992,0.995,0.999,0.003,0.001,0.981,0.019,0.997,0.453,1.0,1.0,0.216,0.022,0.009,1.0,0.954,1.0,0.987,0.001,1.0,0.058,0.999,1.0,0.702,0.998,0.939,0.995,0.873,0.12,0.01,0.009,0.004,0.705,0.012,0.646,0.114,0.003,0.069,1.0,0.999,0.99,1.0,1.0,0.98,0.004,0.998,0.628,0.001,0.999,0.979,0.083,0.002,0.999,0.953,0.271,0.999,0.003,0.041,0.999,0.013,0.003,0.996,0.11,0.089,0.999,0.991,0.992,0.999,0.966,0.994,0.003,0.928,0.082,1.0,1.0,0.003,0.856,1.0,0.022,1.0,0.946,0.062,0.041,0.985,1.0,0.977,0.0,0.004,1.0,0.999,0.53,0.997,0.998,1.0,1.0,0.051,0.998,0.998,0.305,1.0,0.999,0.688,1.0,0.942,0.966,1.0,0.984,0.002,0.998,0.979,1.0,0.004,0.001,0.993,0.988,1.0,0.732,0.998,0.004,0.416,0.023,1.0,0.997,0.938,0.999,0.996,0.002,0.257,0.004,1.0,0.951,0.002,0.005,0.0,0.999,0.989,0.999,0.999,0.998,0.186,0.005,1.0,1.0,1.0,0.244,0.987,0.028,0.002,0.999,0.74,0.038,0.005,1.0,0.999,0.999,1.0,0.895,0.249,1.0,0.005,0.032,0.971,1.0,0.992,0.999,0.128,0.001,0.964,0.999,0.999,0.992,0.042,0.997,0.999,0.735,0.961,0.017,0.012,0.006,0.111,0.999,1.0,1.0,0.996,0.192,1.0,0.939,0.171,0.871,0.861,1.0,0.985,0.002,0.0,0.076,0.0,0.968,0.999,0.982,0.994,0.003,0.894,0.992,0.965,0.009,0.033,0.803,0.912,0.99,0.287,1.0,1.0,0.019,0.996,0.995,1.0,0.164,1.0,0.998,1.0,1.0,0.006,1.0,0.032,1.0,0.05,0.995,0.002,0.996,0.99,0.993,0.998,0.966,0.107,0.927,0.857,1.0,0.004,0.999,0.951,0.014,0.999,0.005,0.004,1.0,0.879,0.977,0.999,0.931,0.991,1.0,0.999,0.999,1.0,0.999,0.999,0.975,1.0,1.0,0.009,0.069,0.992,0.059,0.997,0.998,0.022,0.336,0.999,0.034,0.957,0.999,0.016,0.024,0.994,1.0,0.997,0.004,0.948,1.0,0.9,0.991,0.005,0.999,0.439,0.003,1.0,1.0,0.006,0.135,0.026,0.153,0.981,0.999,0.581,0.999,0.045,0.994,0.965,1.0,0.12,0.006,0.997,1.0,0.006,0.004,1.0,0.341,0.752,0.934,0.115,1.0,0.001,0.999,0.708,0.004,0.001,0.987,0.002,0.334,1.0,0.957,0.99,1.0,1.0,0.999,0.013,0.979,0.991,0.436,0.988,1.0,0.999,0.987,0.999,0.997,1.0,0.74,0.542,0.98,0.029,0.991,0.878,0.001,1.0,0.953,0.996,0.998,0.136,0.115,0.999,0.074,0.001,0.981,0.008,0.999,0.886,1.0,0.999,0.995,0.996,1.0,0.849,0.999,0.966,0.986,0.995,0.993,0.013,0.003,1.0,0.985,0.017,1.0,0.999,1.0,1.0,0.021,1.0,0.044,0.911,0.003,0.778,0.604,0.102,1.0,0.927,1.0,0.084,0.092,1.0,0.998,1.0,0.003,0.026,0.493,0.05,1.0,0.99,0.673,0.03,1.0,0.479,0.627,0.066,1.0,0.006,0.002,0.988,1.0,0.999,0.96,0.098,0.006,0.075,0.018,0.821,0.72,0.023,0.019,0.999,0.022,0.001,0.068,0.898,0.426,0.013,1.0,1.0,0.005,0.976,1.0,0.998,0.81,0.999,0.983,0.007,0.998,0.946,0.96,0.172,0.999,0.15,0.029,0.002,0.997,1.0,0.999,1.0,0.235,0.999,1.0,0.001,0.994,0.66,1.0,0.006,0.126,1.0,1.0,1.0,0.992,0.999,0.992,0.001,1.0,0.762,0.0,1.0,0.966,0.999,1.0,0.999,1.0,0.192,1.0,0.553,0.01,0.997,0.997,0.999,0.998,0.019,0.999,0.005,0.644,0.001,0.001,1.0,1.0,0.916,0.996,0.994,0.996,0.025,1.0,0.929,0.942,0.117,0.984,0.022,0.991,0.891,0.997,1.0,0.988,1.0,0.987,0.995,0.025,1.0,0.661,0.951,0.968,0.996,0.999,0.985,0.089,1.0,0.999,0.992,0.776,0.007,0.994,0.989,0.09,1.0,0.923,0.947,0.984,0.014,0.228,1.0,0.012,1.0,0.001,0.991,0.031,0.999,0.0,0.455,0.01,0.03,0.168,0.993,1.0,0.993,0.994,0.175,1.0,0.044,0.791,0.003,0.921,0.963,0.001,0.03,0.007,0.322,0.001,0.002,0.994,0.999,0.73,0.999,1.0,0.288,0.99,0.989,0.974,0.986,1.0,0.002,0.671,0.615,0.15,0.139,1.0,0.999,0.766,1.0,0.999,0.998,0.989,0.65,0.999,0.048,0.999,0.999,0.749,0.99,0.991,1.0,0.669,0.999,0.126,0.16,0.02,0.144,0.287,0.974,0.066,0.234,0.819,0.021,0.002,0.225,0.953,0.003,0.007,0.245,0.986,0.999,0.676,1.0,0.711,0.001,0.032,0.402,0.894,0.187,0.531,0.144,0.005,0.32,0.129,1.0,0.998,0.978,0.189,0.582,0.024,0.994,0.024,0.706,0.007,1.0,0.051,0.001,0.956,0.991,0.008,0.68,0.991,0.106,0.017,0.998,0.008,1.0,0.102,0.978,0.906,0.983,1.0,0.058,0.024,0.972,0.769,0.062,1.0,0.086,0.494,0.003,0.948,0.988,0.902,0.034,0.957,0.986,0.999,0.118,0.997,1.0,0.028,0.003,0.006,0.973,0.969,0.996,0.905,0.989,0.831,0.986,0.942,0.352,0.319,1.0,0.014,1.0,0.125,1.0,0.016,0.868,0.003,0.995,0.991,0.007,0.004,0.026,0.998,0.971,0.973,0.004,1.0,0.003,0.001,1.0,0.22,0.011,0.801,0.009,0.99,0.79,0.783,0.04,0.98,0.536,0.956,0.015,0.925,0.006,0.722,0.92,0.969,0.992,0.999,0.008,0.995,0.988,1.0,0.463,1.0,0.987,0.918,0.96,0.997,1.0,0.763,0.037,0.004,0.002,0.322,0.015,0.682,0.989,0.003,0.16,0.978,0.117,1.0,0.999,0.013,1.0,0.002,0.349,0.001,0.01,0.999,0.997,0.002,0.974,0.998,1.0,1.0,0.99,0.686,0.999,0.995,0.101,0.979,0.005,0.999,0.197,0.942,0.271,1.0,0.879,0.998,0.048,0.004,0.001,0.012,1.0,1.0,0.043,0.999,0.001,0.704,0.997,0.999,0.958,0.001,0.999,0.018,1.0,1.0,0.021,0.973,0.998,0.001,0.549,0.834,0.049,0.991,0.966,0.417,0.016,0.012,0.0,0.002,0.989,0.006,0.771,0.996,0.777,0.995,0.363,0.001,0.997,0.003,1.0,0.002,0.998,0.001,0.629,0.061,0.997,0.001,0.006,0.953,0.127,0.991,1.0,0.115,0.005,0.744,0.991,1.0,0.018,0.977,0.04,0.007,0.307,0.154,0.999,0.008,0.99,0.086,0.002,0.125,0.995,0.016,0.469,0.305,0.004,0.002,1.0,0.01,0.565,0.009,0.841,0.998,0.999,1.0,0.007,1.0,0.683,0.148,0.948,0.222,0.017,0.612,0.986,0.249,0.989,1.0,0.003,0.036,0.037,0.95,0.615,0.058,0.999,0.001,0.111,0.994,1.0,0.788,0.999,0.966,0.038,0.027,0.004,0.848,0.023,0.061,0.039,0.011,0.12,0.011,1.0,0.002,0.017,0.021,0.139,0.335,0.998,0.794,0.557,0.548,0.124,0.131,0.002,0.635,0.035,0.957,0.007,0.014,0.997,0.004,0.742,0.029,0.104,0.01,0.995,0.354,0.972,0.315,0.021,0.845,0.052,0.758,0.043,0.724,0.76,0.065,0.334,0.001,0.951,0.931,0.17,0.954,0.982,0.028,0.034,0.004,0.105,0.293,0.892,1.0,0.968,0.324,0.033,0.025,0.031,0.857,0.977,0.446,0.039,0.005,0.999,0.408,0.0,0.005,0.959,0.987,0.103,0.034,0.507,0.98,0.022,0.016,0.999,0.019,1.0,0.946,0.004,0.004,0.943,0.106,0.833,0.05,0.999,0.987,0.101,0.068,0.922,0.026,0.629,1.0,0.193,0.01,0.999,0.029,0.036,0.996,0.999,0.996,0.982,0.791,0.996,0.997,0.029,0.879,0.452,0.999,0.011,0.993,0.988,0.21,0.079,0.943,0.999,0.081,0.95,0.052,0.996,1.0,1.0,0.822,1.0,0.604,0.999,0.994,0.998,0.112,0.524,0.007,0.998,0.012,0.998,0.013,0.994,0.115,0.99,0.039,0.137,1.0,0.964,0.94,1.0,1.0,0.607,0.016,0.998,0.009,0.995,1.0,0.0,1.0,0.017,0.016,0.935,0.026,0.979,1.0,0.115,0.996,0.995,0.001,0.0,0.006,0.928,0.045,0.001,0.019,0.994,0.125,0.056,1.0,0.011,0.999,0.98,1.0,1.0,0.071,0.022,1.0,0.001,0.816,0.778,1.0,0.997,0.056,0.43,0.965,0.999,0.956,0.993,0.992,0.004,0.848,0.001,0.004,0.992,0.476,0.959,0.007,0.577,0.987,0.003,0.996,0.098,0.981,0.998,1.0,0.944,0.265,1.0,0.127,0.987,0.346,0.014,0.995,0.987,0.999,0.778,0.994,0.99,0.001,0.027,0.001,0.002,0.996,0.065,0.001,0.999,0.995,0.134,0.991,1.0,0.004,0.075,0.989,0.004,0.668,0.029,0.999,0.039,0.999,0.001,0.002,0.0,0.048,0.82,0.074,0.004,0.013,0.018,0.77,0.998,0.998,0.999,0.999,0.022,0.962,0.999,0.007,0.067,0.009,0.995,0.993,0.984,0.999,0.999,0.967,0.993,0.999,0.993,0.001,0.995,0.993,0.949,0.997,0.001,0.003,0.999,0.998,0.004,0.984,0.881,0.999,0.995,0.998,0.989,0.998,0.995,0.997,0.981,0.599,0.979,0.661,0.153,0.348,0.279,0.896,0.006,0.999,0.986,0.002,0.238,0.992,0.028,1.0,0.974,0.975,0.307,0.996,0.035,0.029,0.997,0.992,0.003,0.932,0.003,0.104,0.2,0.005,0.064,0.998,0.999,0.022,1.0,0.996,0.874,0.66,0.977,0.035,0.995,1.0,0.49,0.999,0.999,0.981,0.707,0.991,0.737,0.985,0.515,0.778,0.019,0.001,0.001,0.018,0.555,1.0,0.999,0.0,0.977,0.991,0.016,0.732,0.971,0.134,0.001,0.002,0.12,0.391,0.999,0.995,0.008,0.982,0.107,0.009,0.062,0.063,0.287,0.074,0.995,0.517,0.092,0.998,1.0,0.005,0.001,0.035,0.976,0.01,0.167,0.215,0.996,0.971,0.009,0.03,0.045,0.036,0.999,0.895,0.148,0.165,0.974,0.966,0.794,0.01,0.761,0.116,0.994,0.96,0.998,0.997,0.095,0.826,0.996,0.043,0.952,0.01,0.997,0.977,0.958,0.94,0.022,0.0,0.0,0.017,0.998,0.011,0.004,0.982,0.046,0.977,0.033,0.003,0.184,0.005,0.0,0.94,0.997,0.054,0.997,0.006,0.112,0.017,0.186,0.576,0.89,0.991,0.03,0.33,0.879,0.331,0.932,0.364,0.955,0.867,0.02,0.212,0.742,0.955,0.025,0.669,0.053,0.963,0.098,0.567,0.0,0.785,0.235,0.96,0.074,0.778,0.582,0.86,0.941,0.0,0.716,0.513,0.981,0.001,0.241,0.019,0.201,0.015,0.499,0.236,0.182,0.865,0.078,0.908,0.03,0.087,0.528,0.978,0.994,0.9,0.074,0.967,0.896,0.003,0.584,0.007,0.93,0.024,0.001,0.969,0.992,0.967,0.995,0.999,0.833,0.988,0.143,0.682,0.978,0.786,0.998,0.991,0.98,0.596,0.823,0.754,0.929,0.477,0.997,0.079,0.319,0.957,0.431,0.122,0.04,0.082,0.999,0.95,0.998,0.026,0.988,0.776,1.0,0.999,0.995,0.992,0.835,0.007,0.94,0.003,0.141,0.976,0.205,0.082,0.017,0.988,0.937,0.01,0.963,0.077,0.989,0.979,0.013,0.518,0.437,0.015,0.228,0.002,0.809,0.954,0.311,0.046,0.258,0.959,0.026,0.747,0.907,0.002,0.72,0.008,0.953,0.032,0.005,0.994,0.878,0.015,0.694,0.618,0.028,0.798,0.996,0.822,0.808,0.935,0.963,0.991,0.999,0.019,0.819,0.859,0.104,0.072,0.06,0.001,0.763,0.498,0.01,0.016,0.002,0.106,0.913,0.993,0.0,0.009,0.047,0.007,0.221,0.098,0.0,0.01,0.73,0.003,0.151,0.975,0.398,0.147,0.097,0.079,0.752,0.212,0.999,0.029,0.07,0.091,0.94,0.836,0.684,0.336,0.002,0.317,0.871,0.021,0.008,0.004,0.01,0.137,0.236,0.983,0.996,0.858,0.345,0.999,0.981,0.635,0.992,0.031,0.443,0.005,0.033,0.309,0.988,0.93,0.402,0.008,0.003,0.006,0.002,0.955,0.096,0.068,0.006,0.218,0.413,0.783,0.128,0.8,0.584,0.288,0.89,0.989,0.286,0.012,0.92,0.967,0.639,0.988,0.503,0.009,0.021,0.65,0.095,0.031,0.038,0.327,0.015,0.001,0.007,0.347,0.196,0.549,1.0,0.991,0.031,0.191,0.134,0.956,0.741,0.015,0.017,0.951,0.07,0.173,0.112,0.469,0.107,0.028,0.575,0.492,0.123,0.004,0.139,0.178,0.082,0.67,0.015,0.134,1.0,0.259,0.036,0.893,0.074,0.989,0.481,0.03,0.092,0.336,0.991,0.511,0.004,0.019,0.874,0.981,0.045,0.026,0.007,0.0,0.01,0.011,0.013,0.176,0.951,0.106,0.825,0.002,0.027,0.002,0.04,0.915,0.991,0.002,0.692,0.001,0.997,0.949,0.978,0.993,0.281,0.028,0.0,0.008,0.018,0.145,0.552,0.087,0.906,0.784,0.102,0.283,0.971,0.619,0.52,0.068,0.004,0.635,0.492,0.109,0.998,0.055,0.61,0.743,0.07,0.988,0.641,0.038,0.257,0.003,0.008,0.972,0.853,0.002,0.772,0.016,0.991,0.393,0.171,0.293,0.019,0.68,0.012,0.005,0.039,0.617,0.806,0.214,0.484,0.437,0.98,0.005,0.754,0.926,0.243,0.031,0.022,0.084,0.857,0.011,0.964,0.062,0.057,0.982,0.04,0.071,0.076,0.923,0.055,0.851,0.086,0.574,0.303,0.423,0.018,0.612,0.122,0.004,0.001,0.046,0.477,0.039,0.033,0.192,0.992,0.004,0.946,0.401,0.991,0.009,0.336,0.002,0.243,0.976,0.961,0.007,0.995,0.97,0.655,0.958,0.927,0.829,0.841,0.75,0.004,0.374,0.004,0.022,0.108,0.877,0.578,0.089,0.005,0.02,0.001,0.016,0.017,0.009,0.004,0.08,0.515,0.743,0.781,0.026,0.349,0.486,0.38,0.773,0.006,0.059,0.008,0.961,0.172,0.01,0.2,0.358,0.009,0.69,0.015,0.108,0.993,0.003,0.0,0.062,0.377,0.312,0.014,0.905,0.024,0.586,0.061,0.227,0.014,0.974,0.812,0.965,0.715,0.763,0.015,0.858,0.153,0.17,0.291,0.235,0.019,0.074,0.03,0.001,0.963,0.004,0.019,0.013,0.673,0.272,0.711,0.018,0.003,0.194,0.996,0.085,0.162,0.03,0.067,0.561,0.749,0.999,0.002,0.313,0.985,0.001,0.064,0.034,0.037,0.987,0.493,0.005,0.049,0.997,0.015,0.029,0.509,0.797,0.895,0.002,0.154,0.014,0.055,0.976,0.007,0.009,0.249,0.72,0.051,0.128,0.947,0.898,0.244,0.021,0.008,0.645,0.106,0.007,0.001,0.373,0.581,0.852,0.349,0.025,0.186,0.001,0.953,0.001,0.049,0.02,0.002,0.484,0.027,0.266,0.786,0.0,0.039,0.053,0.158,0.385,0.023'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub21 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub21['pred'] = sub21['pred'].apply(lambda x: round(x,3))\n",
    "sub21_txt = ''\n",
    "for prob in list(sub21['pred'].values):\n",
    "    sub21_txt = sub21_txt+','+str(prob)\n",
    "sub21_txt = sub21_txt[1:]\n",
    "\n",
    "sub21_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78328c8",
   "metadata": {},
   "source": [
    "### Submission 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1731b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:04:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"class_weight\", \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:04:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=200,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5,class_weight='balanced')\n",
    "rf.fit(X_train_experienced_under_imputed,y_train_experienced_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6ab384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9922903803429884\n",
      "Val ROC:  0.8491632745364088\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c04bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.408062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.126785              0.0        0.126785       0.265943   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.142119      0.408062  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "948fd629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.993,0.863,1.0,0.997,1.0,0.483,0.001,0.997,0.99,0.041,0.007,1.0,1.0,0.978,0.999,0.008,0.011,0.998,0.998,1.0,0.988,0.999,1.0,1.0,0.999,0.994,0.999,0.84,0.991,0.999,0.984,0.993,0.98,0.999,0.999,0.788,0.418,0.997,0.825,0.981,0.978,0.991,0.996,1.0,0.994,1.0,0.182,0.31,0.961,0.002,1.0,1.0,0.024,0.014,0.007,0.998,1.0,1.0,0.999,1.0,0.296,0.097,1.0,0.999,0.91,0.996,0.993,0.997,1.0,0.998,0.999,0.998,0.999,0.955,0.265,0.998,1.0,0.992,0.998,0.024,0.918,1.0,0.112,1.0,0.929,0.63,1.0,0.995,0.994,1.0,0.993,0.313,1.0,0.049,0.999,0.977,0.951,0.375,0.996,0.993,0.999,1.0,1.0,0.963,0.104,0.67,0.641,0.999,0.667,0.994,1.0,0.996,0.999,0.826,0.991,0.992,0.996,0.998,0.994,1.0,1.0,0.188,0.967,0.999,0.968,1.0,1.0,0.004,0.002,0.998,0.725,0.001,0.996,0.917,1.0,0.527,0.999,0.996,0.996,0.028,0.525,0.999,0.956,0.965,0.999,0.956,0.999,0.017,1.0,1.0,0.999,1.0,1.0,0.2,0.967,0.991,0.995,1.0,0.945,1.0,0.994,0.998,0.971,0.999,0.987,0.961,0.992,0.996,0.145,1.0,0.714,0.717,0.996,1.0,0.903,0.999,0.993,0.025,0.999,0.28,0.02,0.998,0.612,0.86,1.0,0.997,1.0,1.0,0.956,0.999,0.025,0.049,0.985,0.982,0.0,0.995,0.064,1.0,0.771,0.993,1.0,1.0,0.999,0.915,0.247,0.022,0.068,0.047,0.001,1.0,0.997,0.013,1.0,0.999,0.296,0.996,1.0,0.582,0.999,0.955,0.999,0.013,0.999,0.999,0.092,0.007,0.997,0.112,0.016,0.001,0.983,0.679,0.999,0.046,1.0,1.0,0.957,1.0,1.0,0.002,0.998,0.994,1.0,1.0,0.045,0.643,0.002,0.996,0.983,0.685,0.001,0.999,1.0,0.003,0.978,1.0,1.0,0.012,0.001,1.0,0.85,0.002,0.998,1.0,1.0,0.961,1.0,0.996,0.007,0.979,0.981,1.0,0.215,0.999,0.467,1.0,1.0,1.0,0.921,0.999,0.995,0.003,1.0,0.979,0.008,1.0,0.998,0.15,0.999,0.014,0.004,0.099,1.0,0.749,0.003,0.992,0.0,0.258,1.0,0.999,0.992,0.994,0.998,1.0,0.997,0.997,1.0,1.0,1.0,1.0,0.988,0.023,1.0,0.995,0.0,1.0,0.247,0.815,0.356,0.991,0.99,0.001,0.49,1.0,0.991,0.039,0.627,0.498,0.769,0.011,0.998,0.008,0.999,0.9,0.538,1.0,1.0,0.005,0.92,0.995,0.979,0.977,0.998,0.997,0.998,0.97,0.002,0.993,0.999,0.002,0.115,0.999,0.995,0.027,1.0,0.401,0.994,1.0,1.0,1.0,0.999,0.05,0.986,0.916,0.998,0.003,1.0,0.999,0.007,1.0,0.999,0.994,1.0,0.952,0.993,0.999,1.0,0.999,0.918,0.961,0.991,0.005,0.973,0.996,0.999,0.005,1.0,1.0,0.993,0.052,1.0,0.516,1.0,0.998,1.0,1.0,1.0,1.0,0.998,0.991,1.0,1.0,0.957,1.0,0.992,0.002,0.001,0.972,0.009,0.013,0.044,0.976,0.339,0.998,0.002,0.998,0.002,0.006,0.977,0.999,0.985,0.992,1.0,0.017,0.013,0.176,0.046,1.0,0.992,0.762,0.009,0.004,1.0,0.02,1.0,0.998,0.988,0.001,0.999,0.032,0.997,1.0,0.091,1.0,0.981,0.164,1.0,0.007,0.186,0.992,0.665,0.005,1.0,0.981,0.999,1.0,1.0,0.997,1.0,0.999,0.031,0.98,0.993,0.833,0.999,0.002,0.597,0.006,0.994,0.399,0.168,0.059,0.987,0.981,0.976,0.989,0.998,0.007,0.984,0.02,0.999,1.0,0.001,0.005,0.013,0.992,0.995,0.999,0.003,0.001,0.981,0.019,0.997,0.453,1.0,1.0,0.216,0.022,0.009,1.0,0.954,1.0,0.987,0.001,1.0,0.058,0.999,1.0,0.702,0.998,0.939,0.995,0.873,0.12,0.01,0.009,0.004,0.705,0.012,0.646,0.114,0.003,0.069,1.0,0.999,0.99,1.0,1.0,0.98,0.004,0.998,0.628,0.001,0.999,0.979,0.083,0.002,0.999,0.953,0.271,0.999,0.003,0.041,0.999,0.013,0.003,0.996,0.11,0.089,0.999,0.991,0.992,0.999,0.966,0.994,0.003,0.928,0.082,1.0,1.0,0.003,0.856,1.0,0.022,1.0,0.946,0.062,0.041,0.985,1.0,0.977,0.0,0.004,1.0,0.999,0.53,0.997,0.998,1.0,1.0,0.051,0.998,0.998,0.305,1.0,0.999,0.688,1.0,0.942,0.966,1.0,0.984,0.002,0.998,0.979,1.0,0.004,0.001,0.993,0.988,1.0,0.732,0.998,0.004,0.416,0.023,1.0,0.997,0.938,0.999,0.996,0.002,0.257,0.004,1.0,0.951,0.002,0.005,0.0,0.999,0.989,0.999,0.999,0.998,0.186,0.005,1.0,1.0,1.0,0.244,0.987,0.028,0.002,0.999,0.74,0.038,0.005,1.0,0.999,0.999,1.0,0.895,0.249,1.0,0.005,0.032,0.971,1.0,0.992,0.999,0.128,0.001,0.964,0.999,0.999,0.992,0.042,0.997,0.999,0.735,0.961,0.017,0.012,0.006,0.111,0.999,1.0,1.0,0.996,0.192,1.0,0.939,0.171,0.871,0.861,1.0,0.985,0.002,0.0,0.076,0.0,0.968,0.999,0.982,0.994,0.003,0.894,0.992,0.965,0.009,0.033,0.803,0.912,0.99,0.287,1.0,1.0,0.019,0.996,0.995,1.0,0.164,1.0,0.998,1.0,1.0,0.006,1.0,0.032,1.0,0.05,0.995,0.002,0.996,0.99,0.993,0.998,0.966,0.107,0.927,0.857,1.0,0.004,0.999,0.951,0.014,0.999,0.005,0.004,1.0,0.879,0.977,0.999,0.931,0.991,1.0,0.999,0.999,1.0,0.999,0.999,0.975,1.0,1.0,0.009,0.069,0.992,0.059,0.997,0.998,0.022,0.336,0.999,0.034,0.957,0.999,0.016,0.024,0.994,1.0,0.997,0.004,0.948,1.0,0.9,0.991,0.005,0.999,0.439,0.003,1.0,1.0,0.006,0.135,0.026,0.153,0.981,0.999,0.581,0.999,0.045,0.994,0.965,1.0,0.12,0.006,0.997,1.0,0.006,0.004,1.0,0.341,0.752,0.934,0.115,1.0,0.001,0.999,0.708,0.004,0.001,0.987,0.002,0.334,1.0,0.957,0.99,1.0,1.0,0.999,0.013,0.979,0.991,0.436,0.988,1.0,0.999,0.987,0.999,0.997,1.0,0.74,0.542,0.98,0.029,0.991,0.878,0.001,1.0,0.953,0.996,0.998,0.136,0.115,0.999,0.074,0.001,0.981,0.008,0.999,0.886,1.0,0.999,0.995,0.996,1.0,0.849,0.999,0.966,0.986,0.995,0.993,0.013,0.003,1.0,0.985,0.017,1.0,0.999,1.0,1.0,0.021,1.0,0.044,0.911,0.003,0.778,0.604,0.102,1.0,0.927,1.0,0.084,0.092,1.0,0.998,1.0,0.003,0.026,0.493,0.05,1.0,0.99,0.673,0.03,1.0,0.479,0.627,0.066,1.0,0.006,0.002,0.988,1.0,0.999,0.96,0.098,0.006,0.075,0.018,0.821,0.72,0.023,0.019,0.999,0.022,0.001,0.068,0.898,0.426,0.013,1.0,1.0,0.005,0.976,1.0,0.998,0.81,0.999,0.983,0.007,0.998,0.946,0.96,0.172,0.999,0.15,0.029,0.002,0.997,1.0,0.999,1.0,0.235,0.999,1.0,0.001,0.994,0.66,1.0,0.006,0.126,1.0,1.0,1.0,0.992,0.999,0.992,0.001,1.0,0.762,0.0,1.0,0.966,0.999,1.0,0.999,1.0,0.192,1.0,0.553,0.01,0.997,0.997,0.999,0.998,0.019,0.999,0.005,0.644,0.001,0.001,1.0,1.0,0.916,0.996,0.994,0.996,0.025,1.0,0.929,0.942,0.117,0.984,0.022,0.991,0.891,0.997,1.0,0.988,1.0,0.987,0.995,0.025,1.0,0.661,0.951,0.968,0.996,0.999,0.985,0.089,1.0,0.999,0.992,0.776,0.007,0.994,0.989,0.09,1.0,0.923,0.947,0.984,0.014,0.228,1.0,0.012,1.0,0.001,0.991,0.031,0.999,0.0,0.455,0.01,0.03,0.168,0.993,1.0,0.993,0.994,0.175,1.0,0.044,0.791,0.003,0.921,0.963,0.001,0.03,0.007,0.322,0.001,0.002,0.994,0.999,0.73,0.999,1.0,0.288,0.99,0.989,0.974,0.986,1.0,0.002,0.671,0.615,0.15,0.139,1.0,0.999,0.766,1.0,0.999,0.998,0.989,0.65,0.999,0.048,0.999,0.999,0.749,0.99,0.991,1.0,0.669,0.999,0.126,0.16,0.02,0.144,0.287,0.974,0.066,0.234,0.819,0.021,0.002,0.225,0.953,0.003,0.007,0.245,0.986,0.999,0.676,1.0,0.711,0.001,0.032,0.402,0.894,0.187,0.531,0.144,0.005,0.32,0.129,1.0,0.998,0.978,0.189,0.582,0.024,0.994,0.024,0.706,0.007,1.0,0.051,0.001,0.956,0.991,0.008,0.68,0.991,0.106,0.017,0.998,0.008,1.0,0.102,0.978,0.906,0.983,1.0,0.058,0.024,0.972,0.769,0.062,1.0,0.086,0.494,0.003,0.948,0.988,0.902,0.034,0.957,0.986,0.999,0.118,0.997,1.0,0.028,0.003,0.006,0.973,0.969,0.996,0.905,0.989,0.831,0.986,0.942,0.352,0.319,1.0,0.014,1.0,0.125,1.0,0.016,0.868,0.003,0.995,0.991,0.007,0.004,0.026,0.998,0.971,0.973,0.004,1.0,0.003,0.001,1.0,0.22,0.011,0.801,0.009,0.99,0.79,0.783,0.04,0.98,0.536,0.956,0.015,0.925,0.006,0.722,0.92,0.969,0.992,0.999,0.008,0.995,0.988,1.0,0.463,1.0,0.987,0.918,0.96,0.997,1.0,0.763,0.037,0.004,0.002,0.322,0.015,0.682,0.989,0.003,0.16,0.978,0.117,1.0,0.999,0.013,1.0,0.002,0.349,0.001,0.01,0.999,0.997,0.002,0.974,0.998,1.0,1.0,0.99,0.686,0.999,0.995,0.101,0.979,0.005,0.999,0.197,0.942,0.271,1.0,0.879,0.998,0.048,0.004,0.001,0.012,1.0,1.0,0.043,0.999,0.001,0.704,0.997,0.999,0.958,0.001,0.999,0.018,1.0,1.0,0.021,0.973,0.998,0.001,0.549,0.834,0.049,0.991,0.966,0.417,0.016,0.012,0.0,0.002,0.989,0.006,0.771,0.996,0.777,0.995,0.363,0.001,0.997,0.003,1.0,0.002,0.998,0.001,0.629,0.061,0.997,0.001,0.006,0.953,0.127,0.991,1.0,0.115,0.005,0.744,0.991,1.0,0.018,0.977,0.04,0.007,0.307,0.154,0.999,0.008,0.99,0.086,0.002,0.125,0.995,0.016,0.469,0.305,0.004,0.002,1.0,0.01,0.565,0.009,0.841,0.998,0.999,1.0,0.007,1.0,0.683,0.148,0.948,0.222,0.017,0.612,0.986,0.249,0.989,1.0,0.003,0.036,0.037,0.95,0.615,0.058,0.999,0.001,0.111,0.994,1.0,0.788,0.999,0.966,0.038,0.027,0.004,0.848,0.023,0.061,0.039,0.011,0.12,0.011,1.0,0.002,0.017,0.021,0.139,0.335,0.998,0.794,0.557,0.548,0.124,0.131,0.002,0.635,0.035,0.957,0.007,0.014,0.997,0.004,0.742,0.029,0.104,0.01,0.995,0.354,0.972,0.315,0.021,0.845,0.052,0.758,0.043,0.724,0.76,0.065,0.334,0.001,0.951,0.931,0.17,0.954,0.982,0.028,0.034,0.004,0.105,0.293,0.892,1.0,0.968,0.324,0.033,0.025,0.031,0.857,0.977,0.446,0.039,0.005,0.999,0.408,0.0,0.005,0.959,0.987,0.103,0.034,0.507,0.98,0.022,0.016,0.999,0.019,1.0,0.946,0.004,0.004,0.943,0.106,0.833,0.05,0.999,0.987,0.101,0.068,0.922,0.026,0.629,1.0,0.193,0.01,0.999,0.029,0.036,0.996,0.999,0.996,0.982,0.791,0.996,0.997,0.029,0.879,0.452,0.999,0.011,0.993,0.988,0.21,0.079,0.943,0.999,0.081,0.95,0.052,0.996,1.0,1.0,0.822,1.0,0.604,0.999,0.994,0.998,0.112,0.524,0.007,0.998,0.012,0.998,0.013,0.994,0.115,0.99,0.039,0.137,1.0,0.964,0.94,1.0,1.0,0.607,0.016,0.998,0.009,0.995,1.0,0.0,1.0,0.017,0.016,0.935,0.026,0.979,1.0,0.115,0.996,0.995,0.001,0.0,0.006,0.928,0.045,0.001,0.019,0.994,0.125,0.056,1.0,0.011,0.999,0.98,1.0,1.0,0.071,0.022,1.0,0.001,0.816,0.778,1.0,0.997,0.056,0.43,0.965,0.999,0.956,0.993,0.992,0.004,0.848,0.001,0.004,0.992,0.476,0.959,0.007,0.577,0.987,0.003,0.996,0.098,0.981,0.998,1.0,0.944,0.265,1.0,0.127,0.987,0.346,0.014,0.995,0.987,0.999,0.778,0.994,0.99,0.001,0.027,0.001,0.002,0.996,0.065,0.001,0.999,0.995,0.134,0.991,1.0,0.004,0.075,0.989,0.004,0.668,0.029,0.999,0.039,0.999,0.001,0.002,0.0,0.048,0.82,0.074,0.004,0.013,0.018,0.77,0.998,0.998,0.999,0.999,0.022,0.962,0.999,0.007,0.067,0.009,0.995,0.993,0.984,0.999,0.999,0.967,0.993,0.999,0.993,0.001,0.995,0.993,0.949,0.997,0.001,0.003,0.999,0.998,0.004,0.984,0.881,0.999,0.995,0.998,0.989,0.998,0.995,0.997,0.981,0.599,0.979,0.661,0.153,0.348,0.279,0.896,0.006,0.999,0.986,0.002,0.238,0.992,0.028,1.0,0.974,0.975,0.307,0.996,0.035,0.029,0.997,0.992,0.003,0.932,0.003,0.104,0.2,0.005,0.064,0.998,0.999,0.022,1.0,0.996,0.874,0.66,0.977,0.035,0.995,1.0,0.49,0.999,0.999,0.981,0.707,0.991,0.737,0.985,0.515,0.778,0.019,0.001,0.001,0.018,0.555,1.0,0.999,0.0,0.977,0.991,0.016,0.732,0.971,0.134,0.001,0.002,0.12,0.391,0.999,0.995,0.008,0.982,0.107,0.009,0.062,0.063,0.287,0.074,0.995,0.517,0.092,0.998,1.0,0.005,0.001,0.035,0.976,0.01,0.167,0.215,0.996,0.971,0.009,0.03,0.045,0.036,0.999,0.895,0.148,0.165,0.974,0.966,0.794,0.01,0.761,0.116,0.994,0.96,0.998,0.997,0.095,0.826,0.996,0.043,0.952,0.01,0.997,0.977,0.958,0.94,0.022,0.0,0.0,0.017,0.998,0.011,0.004,0.982,0.046,0.977,0.033,0.003,0.184,0.005,0.0,0.94,0.997,0.054,0.997,0.006,0.112,0.017,0.186,0.576,0.89,0.991,0.03,0.33,0.879,0.331,0.932,0.364,0.955,0.867,0.02,0.212,0.742,0.955,0.025,0.669,0.053,0.963,0.098,0.567,0.0,0.785,0.235,0.96,0.074,0.778,0.582,0.86,0.941,0.0,0.716,0.513,0.981,0.001,0.241,0.019,0.201,0.015,0.499,0.236,0.182,0.865,0.078,0.908,0.03,0.087,0.528,0.978,0.994,0.9,0.074,0.967,0.896,0.003,0.584,0.007,0.93,0.024,0.001,0.969,0.992,0.967,0.995,0.999,0.833,0.988,0.143,0.682,0.978,0.786,0.998,0.991,0.98,0.596,0.823,0.754,0.929,0.477,0.997,0.079,0.319,0.957,0.431,0.122,0.04,0.082,0.999,0.95,0.998,0.026,0.988,0.776,1.0,0.999,0.995,0.992,0.835,0.007,0.94,0.003,0.141,0.976,0.205,0.082,0.017,0.988,0.937,0.01,0.963,0.077,0.989,0.979,0.013,0.518,0.437,0.015,0.228,0.002,0.809,0.954,0.311,0.046,0.258,0.959,0.026,0.747,0.907,0.002,0.72,0.008,0.953,0.032,0.005,0.994,0.878,0.015,0.694,0.618,0.028,0.798,0.996,0.822,0.808,0.935,0.963,0.991,0.999,0.019,0.819,0.859,0.104,0.072,0.06,0.001,0.763,0.498,0.01,0.016,0.002,0.106,0.913,0.993,0.0,0.009,0.047,0.007,0.221,0.098,0.0,0.01,0.73,0.003,0.151,0.975,0.398,0.147,0.097,0.079,0.752,0.212,0.999,0.029,0.07,0.091,0.94,0.836,0.684,0.336,0.002,0.317,0.871,0.021,0.008,0.004,0.01,0.137,0.236,0.983,0.996,0.858,0.345,0.999,0.981,0.635,0.992,0.031,0.443,0.005,0.033,0.309,0.988,0.93,0.402,0.008,0.003,0.006,0.002,0.955,0.096,0.068,0.006,0.218,0.413,0.783,0.128,0.8,0.584,0.288,0.89,0.989,0.286,0.012,0.92,0.967,0.639,0.988,0.503,0.009,0.021,0.65,0.095,0.031,0.038,0.327,0.015,0.001,0.007,0.347,0.196,0.549,1.0,0.991,0.031,0.191,0.134,0.956,0.741,0.015,0.017,0.951,0.07,0.173,0.112,0.469,0.107,0.028,0.575,0.492,0.123,0.004,0.139,0.178,0.082,0.67,0.015,0.134,1.0,0.259,0.036,0.893,0.074,0.989,0.481,0.03,0.092,0.336,0.991,0.511,0.004,0.019,0.874,0.981,0.045,0.026,0.007,0.0,0.01,0.011,0.013,0.176,0.951,0.106,0.825,0.002,0.027,0.002,0.04,0.915,0.991,0.002,0.692,0.001,0.997,0.949,0.978,0.993,0.281,0.028,0.0,0.008,0.018,0.145,0.552,0.087,0.906,0.784,0.102,0.283,0.971,0.619,0.52,0.068,0.004,0.635,0.492,0.109,0.998,0.055,0.61,0.743,0.07,0.988,0.641,0.038,0.257,0.003,0.008,0.972,0.853,0.002,0.772,0.016,0.991,0.393,0.171,0.293,0.019,0.68,0.012,0.005,0.039,0.617,0.806,0.214,0.484,0.437,0.98,0.005,0.754,0.926,0.243,0.031,0.022,0.084,0.857,0.011,0.964,0.062,0.057,0.982,0.04,0.071,0.076,0.923,0.055,0.851,0.086,0.574,0.303,0.423,0.018,0.612,0.122,0.004,0.001,0.046,0.477,0.039,0.033,0.192,0.992,0.004,0.946,0.401,0.991,0.009,0.336,0.002,0.243,0.976,0.961,0.007,0.995,0.97,0.655,0.958,0.927,0.829,0.841,0.75,0.004,0.374,0.004,0.022,0.108,0.877,0.578,0.089,0.005,0.02,0.001,0.016,0.017,0.009,0.004,0.08,0.515,0.743,0.781,0.026,0.349,0.486,0.38,0.773,0.006,0.059,0.008,0.961,0.172,0.01,0.2,0.358,0.009,0.69,0.015,0.108,0.993,0.003,0.0,0.062,0.377,0.312,0.014,0.905,0.024,0.586,0.061,0.227,0.014,0.974,0.812,0.965,0.715,0.763,0.015,0.858,0.153,0.17,0.291,0.235,0.019,0.074,0.03,0.001,0.963,0.004,0.019,0.013,0.673,0.272,0.711,0.018,0.003,0.194,0.996,0.085,0.162,0.03,0.067,0.561,0.749,0.999,0.002,0.313,0.985,0.001,0.064,0.034,0.037,0.987,0.493,0.005,0.049,0.997,0.015,0.029,0.509,0.797,0.895,0.002,0.154,0.014,0.055,0.976,0.007,0.009,0.249,0.72,0.051,0.128,0.947,0.898,0.244,0.021,0.008,0.645,0.106,0.007,0.001,0.373,0.581,0.852,0.349,0.025,0.186,0.001,0.953,0.001,0.049,0.02,0.002,0.484,0.027,0.266,0.786,0.0,0.039,0.053,0.158,0.385,0.023'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub27 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub27['pred'] = sub27['pred'].apply(lambda x: round(x,3))\n",
    "sub27_txt = ''\n",
    "for prob in list(sub27['pred'].values):\n",
    "    sub27_txt = sub27_txt+','+str(prob)\n",
    "sub27_txt = sub27_txt[1:]\n",
    "\n",
    "sub27_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c897d1",
   "metadata": {},
   "source": [
    "### Submission 22 no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048aa321",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_tr2, y_tr2 = oversample.fit_resample(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f844f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=100,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_tr2,y_tr2)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38093d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  1.0\n",
      "Val ROC:  0.8514544963694566\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33225cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17232</td>\n",
       "      <td>0.317829</td>\n",
       "      <td>0.49015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5              0.0              0.0             0.0        0.17232   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.317829       0.49015  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8ffb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.998,0.996,0.995,0.999,0.995,0.999,0.364,0.006,0.989,0.999,0.068,0.194,0.999,1.0,0.986,1.0,0.725,0.104,0.998,0.996,1.0,0.982,0.999,1.0,1.0,0.996,0.998,1.0,0.982,0.999,1.0,0.992,0.997,0.998,0.997,0.998,0.864,0.921,0.998,0.897,0.907,0.983,0.998,1.0,1.0,0.987,1.0,0.273,0.129,0.483,0.063,1.0,0.998,0.0,0.029,0.016,1.0,0.999,0.998,1.0,1.0,0.856,0.075,0.998,1.0,0.955,1.0,0.975,0.998,0.999,1.0,0.999,0.999,0.999,0.94,0.908,0.991,1.0,0.999,0.994,0.019,0.937,0.999,0.122,1.0,0.982,0.544,1.0,0.999,0.998,1.0,0.995,0.883,1.0,0.107,0.999,0.994,0.988,0.341,0.999,0.959,1.0,1.0,1.0,0.957,0.693,0.864,0.889,0.996,0.803,0.947,1.0,0.996,1.0,0.896,0.995,0.949,0.999,0.998,1.0,1.0,1.0,0.98,0.842,1.0,0.998,1.0,1.0,0.03,0.0,0.999,0.978,0.003,0.999,0.997,0.999,0.474,0.999,0.973,0.997,0.178,0.989,0.998,0.996,0.926,1.0,0.959,0.997,0.222,0.999,0.999,0.996,0.999,1.0,0.748,0.996,0.995,0.997,1.0,0.97,0.999,0.999,1.0,0.789,1.0,0.998,0.973,0.994,1.0,0.127,1.0,0.212,0.93,1.0,0.998,0.966,1.0,0.999,0.635,0.999,0.36,0.026,0.993,0.047,0.835,1.0,0.998,1.0,1.0,0.993,1.0,0.048,0.392,0.997,0.776,0.005,0.994,0.027,0.999,0.98,0.998,1.0,0.999,0.999,0.918,0.446,0.253,0.495,0.165,0.024,1.0,0.998,0.198,1.0,0.999,0.692,1.0,0.999,0.719,0.999,0.998,0.999,0.565,0.999,0.999,0.537,0.007,0.994,0.392,0.087,0.002,0.995,0.937,0.999,0.061,1.0,1.0,0.994,1.0,0.998,0.006,0.999,0.995,1.0,1.0,0.091,0.964,0.011,0.996,0.995,0.926,0.021,0.996,1.0,0.013,0.958,1.0,1.0,0.143,0.001,1.0,0.721,0.029,0.999,1.0,0.999,0.961,1.0,0.996,0.084,0.991,0.981,1.0,0.207,0.996,0.994,1.0,0.999,0.999,0.995,0.999,0.991,0.002,0.996,0.915,0.134,0.998,0.999,0.439,0.998,0.002,0.023,0.054,1.0,0.963,0.027,1.0,0.011,0.942,1.0,0.999,1.0,0.999,0.991,1.0,1.0,0.987,1.0,1.0,1.0,1.0,1.0,0.022,1.0,0.997,0.038,1.0,0.556,0.964,0.688,0.995,0.996,0.003,0.777,0.999,0.997,0.016,0.806,0.893,0.82,0.051,1.0,0.022,1.0,0.966,0.993,0.995,0.998,0.008,0.934,0.998,0.996,0.936,0.999,0.977,0.998,0.984,0.015,0.997,1.0,0.06,0.175,0.998,0.998,0.063,1.0,0.918,0.999,1.0,1.0,1.0,1.0,0.22,0.974,0.998,0.997,0.02,1.0,0.998,0.007,1.0,0.982,0.996,1.0,0.993,0.993,0.995,0.999,1.0,0.994,0.924,0.996,0.004,0.989,0.998,0.999,0.002,1.0,1.0,0.993,0.128,0.999,0.42,1.0,0.996,1.0,1.0,1.0,0.985,0.998,0.998,0.998,1.0,0.971,1.0,0.999,0.003,0.024,0.977,0.047,0.036,0.105,0.987,0.984,0.995,0.005,0.998,0.001,0.302,0.932,1.0,0.99,0.997,1.0,0.151,0.139,0.958,0.334,1.0,0.998,0.99,0.004,0.011,1.0,0.758,1.0,0.998,0.994,0.043,1.0,0.254,0.996,0.999,0.293,1.0,0.978,0.76,1.0,0.002,0.34,0.999,0.864,0.003,0.978,0.906,0.999,1.0,1.0,0.997,1.0,1.0,0.132,0.999,0.991,0.993,0.997,0.002,0.875,0.047,0.998,0.833,0.785,0.055,0.993,0.857,0.998,0.998,0.999,0.01,0.987,0.029,0.921,0.999,0.005,0.003,0.021,0.995,0.998,0.996,0.055,0.002,0.977,0.099,0.993,0.939,1.0,1.0,0.153,0.687,0.03,0.999,0.993,1.0,0.993,0.003,1.0,0.84,1.0,1.0,0.807,0.999,0.987,0.977,0.877,0.203,0.168,0.009,0.015,0.948,0.06,0.986,0.35,0.035,0.694,0.999,0.997,0.999,1.0,1.0,0.995,0.006,0.999,0.306,0.005,1.0,0.838,0.107,0.003,0.999,0.53,0.133,0.998,0.068,0.074,0.997,0.05,0.009,0.998,0.375,0.466,0.998,0.999,0.994,0.999,0.993,0.999,0.01,0.992,0.823,0.997,1.0,0.105,0.855,0.999,0.013,0.999,0.999,0.043,0.539,0.961,1.0,0.994,0.002,0.017,0.999,0.998,0.527,0.999,0.999,1.0,1.0,0.127,0.999,0.999,0.608,1.0,1.0,0.827,0.993,0.978,0.997,1.0,0.975,0.092,0.999,0.96,1.0,0.005,0.006,0.998,0.996,1.0,0.935,1.0,0.042,0.924,0.242,0.999,1.0,0.882,0.998,0.999,0.009,0.844,0.078,0.997,0.989,0.186,0.201,0.014,1.0,0.979,0.999,1.0,1.0,0.076,0.083,0.999,1.0,1.0,0.962,0.999,0.294,0.008,0.996,0.792,0.441,0.077,0.997,0.999,1.0,0.999,0.987,0.602,1.0,0.098,0.25,0.883,1.0,0.996,1.0,0.537,0.01,0.994,0.996,0.999,0.994,0.021,0.998,1.0,0.853,0.95,0.071,0.552,0.036,0.763,0.996,1.0,1.0,0.963,0.356,0.999,0.995,0.967,0.908,0.951,1.0,0.996,0.083,0.142,0.741,0.16,0.87,0.995,0.999,0.994,0.012,0.98,0.993,0.955,0.074,0.227,0.987,0.476,0.968,0.981,1.0,1.0,0.004,0.987,0.99,0.999,0.298,1.0,0.998,1.0,1.0,0.032,0.995,0.413,1.0,0.061,0.998,0.001,0.999,0.981,0.97,0.999,0.886,0.435,0.302,0.848,1.0,0.698,0.999,0.98,0.043,1.0,0.002,0.13,1.0,0.981,0.92,0.997,0.985,0.999,0.999,0.999,0.999,1.0,1.0,0.994,0.991,1.0,1.0,0.81,0.331,0.976,0.243,0.999,0.999,0.676,0.729,1.0,0.329,0.994,1.0,0.118,0.135,0.998,1.0,1.0,0.006,0.846,1.0,0.998,0.993,0.014,0.999,0.569,0.0,1.0,1.0,0.065,0.061,0.151,0.201,0.989,1.0,0.979,0.998,0.408,0.999,0.9,1.0,0.795,0.29,0.982,1.0,0.158,0.254,0.989,0.092,0.915,0.984,0.703,1.0,0.027,0.999,0.961,0.071,0.015,0.999,0.005,0.776,1.0,0.965,0.977,1.0,1.0,0.999,0.221,0.995,0.851,0.597,0.999,1.0,1.0,0.998,1.0,0.999,1.0,0.966,0.866,0.996,0.951,0.998,0.965,0.116,1.0,0.989,0.999,1.0,0.179,0.14,0.987,0.33,0.005,0.987,0.008,1.0,0.922,1.0,0.999,0.998,0.998,1.0,0.868,1.0,0.983,0.968,0.826,0.998,0.5,0.052,1.0,0.993,0.335,0.999,0.999,1.0,1.0,0.062,0.999,0.026,0.484,0.021,0.984,0.989,0.49,1.0,0.772,1.0,0.408,0.586,0.996,1.0,1.0,0.011,0.37,0.436,0.022,1.0,0.982,0.916,0.269,1.0,0.865,0.674,0.05,1.0,0.071,0.063,0.984,1.0,0.995,0.994,0.376,0.011,0.71,0.173,0.986,0.904,0.879,0.088,0.999,0.034,0.063,0.508,0.987,0.873,0.214,1.0,0.999,0.016,0.979,1.0,0.965,0.816,1.0,0.991,0.045,1.0,0.993,0.978,0.07,0.997,0.605,0.213,0.013,0.997,1.0,1.0,0.999,0.253,0.999,0.999,0.01,0.999,0.961,1.0,0.08,0.11,1.0,1.0,1.0,0.977,1.0,0.997,0.005,1.0,0.536,0.002,1.0,0.943,1.0,1.0,1.0,1.0,0.911,1.0,0.767,0.026,0.997,0.999,1.0,0.998,0.151,0.999,0.3,0.685,0.014,0.004,1.0,1.0,0.999,0.998,1.0,0.995,0.509,1.0,0.953,0.979,0.674,0.993,0.186,1.0,0.974,0.992,1.0,0.999,1.0,0.993,1.0,0.103,1.0,0.781,0.99,0.986,0.997,0.999,0.971,0.299,1.0,0.995,0.957,0.316,0.148,0.999,0.945,0.369,0.999,0.784,0.993,0.988,0.01,0.162,1.0,0.519,1.0,0.025,0.988,0.05,1.0,0.004,0.382,0.052,0.165,0.065,0.999,0.999,0.991,0.998,0.912,0.999,0.361,0.824,0.001,0.98,0.996,0.141,0.244,0.015,0.278,0.036,0.015,0.974,1.0,0.968,0.995,1.0,0.189,0.837,0.993,0.996,0.997,0.999,0.009,0.905,0.93,0.142,0.637,1.0,0.999,0.623,1.0,0.998,0.999,0.993,0.881,1.0,0.069,0.999,0.999,0.996,0.998,0.995,1.0,0.914,0.998,0.665,0.875,0.488,0.671,0.88,0.987,0.665,0.4,0.693,0.005,0.081,0.281,0.966,0.094,0.15,0.648,0.779,0.997,0.852,1.0,0.893,0.016,0.82,0.284,0.991,0.336,0.835,0.914,0.417,0.097,0.434,1.0,1.0,0.752,0.705,0.955,0.019,0.968,0.407,0.49,0.003,1.0,0.838,0.028,0.965,0.999,0.045,0.787,0.999,0.399,0.015,0.994,0.088,0.999,0.025,0.994,0.996,0.992,1.0,0.01,0.463,0.961,0.573,0.086,0.997,0.739,0.248,0.009,0.996,0.997,0.951,0.034,0.987,0.992,1.0,0.3,0.996,1.0,0.375,0.026,0.08,0.92,0.997,0.999,0.988,0.995,0.958,0.999,1.0,0.768,0.519,1.0,0.01,0.999,0.665,1.0,0.356,0.93,0.037,1.0,0.987,0.153,0.172,0.043,1.0,0.963,0.966,0.001,1.0,0.001,0.009,0.999,0.573,0.045,0.903,0.058,0.997,0.606,0.982,0.366,0.99,0.786,0.991,0.525,0.895,0.023,0.963,0.888,0.869,0.998,0.999,0.012,1.0,0.96,0.999,0.723,1.0,0.973,0.31,0.971,0.998,1.0,0.923,0.798,0.018,0.434,0.124,0.029,0.931,0.959,0.049,0.129,0.999,0.149,1.0,1.0,0.062,1.0,0.028,0.167,0.0,0.039,1.0,0.999,0.005,0.991,1.0,1.0,1.0,0.986,0.693,0.999,0.985,0.367,0.996,0.036,0.999,0.035,0.963,0.901,1.0,0.719,0.999,0.149,0.048,0.018,0.147,1.0,0.999,0.596,0.999,0.153,0.913,0.914,0.999,0.955,0.012,1.0,0.175,1.0,0.999,0.014,0.996,0.999,0.157,0.787,0.985,0.861,0.982,0.988,0.86,0.246,0.385,0.103,0.021,0.963,0.065,0.964,0.996,0.96,1.0,0.956,0.0,1.0,0.045,1.0,0.13,0.999,0.001,0.426,0.981,0.999,0.019,0.03,0.967,0.156,0.977,1.0,0.376,0.012,0.209,0.995,1.0,0.084,0.997,0.087,0.035,0.632,0.86,0.999,0.196,0.992,0.318,0.067,0.242,0.998,0.534,0.781,0.911,0.043,0.024,1.0,0.016,0.984,0.098,0.985,0.997,1.0,1.0,0.754,1.0,0.874,0.748,0.988,0.282,0.031,0.749,0.989,0.659,0.988,1.0,0.217,0.238,0.043,0.975,0.486,0.673,1.0,0.045,0.424,0.995,1.0,0.564,0.999,0.932,0.223,0.028,0.179,0.844,0.174,0.252,0.339,0.121,0.034,0.156,0.996,0.258,0.072,0.416,0.304,0.874,0.991,0.882,0.667,0.651,0.242,0.509,0.019,0.682,0.513,0.92,0.407,0.033,0.977,0.492,0.87,0.464,0.354,0.035,1.0,0.24,0.968,0.84,0.334,0.557,0.071,0.364,0.07,0.926,0.888,0.236,0.752,0.016,0.982,0.987,0.009,0.998,0.834,0.054,0.539,0.095,0.438,0.919,0.955,1.0,0.995,0.774,0.781,0.437,0.076,0.992,0.972,0.496,0.617,0.024,0.999,0.961,0.007,0.074,0.738,0.997,0.697,0.074,0.882,0.987,0.378,0.694,0.998,0.084,0.999,0.998,0.254,0.016,0.819,0.047,0.803,0.266,1.0,0.993,0.114,0.298,0.834,0.327,0.964,1.0,0.454,0.252,0.997,0.532,0.748,0.993,0.999,0.986,0.919,0.954,0.962,1.0,0.095,0.992,0.746,0.994,0.003,0.949,0.997,0.702,0.024,0.987,0.999,0.428,0.739,0.524,0.999,1.0,1.0,0.942,1.0,0.96,1.0,0.983,0.999,0.192,0.414,0.061,0.999,0.065,0.999,0.699,0.997,0.879,0.903,0.863,0.043,0.999,0.979,0.974,0.999,1.0,0.995,0.034,1.0,0.105,0.991,0.999,0.002,1.0,0.203,0.055,0.999,0.168,0.917,1.0,0.863,0.983,0.995,0.225,0.042,0.038,0.961,0.182,0.324,0.165,0.997,0.368,0.09,0.999,0.207,0.991,0.965,1.0,0.978,0.767,0.009,0.999,0.0,0.737,0.962,0.999,0.991,0.039,0.502,0.851,0.994,0.939,0.999,0.999,0.002,0.982,0.004,0.248,0.998,0.877,0.982,0.129,0.93,0.942,0.004,0.999,0.351,0.988,0.973,1.0,0.932,0.954,1.0,0.135,0.976,0.927,0.23,0.996,0.993,0.999,0.933,0.99,0.999,0.035,0.059,0.024,0.017,0.868,0.44,0.301,0.977,0.998,0.243,1.0,1.0,0.012,0.193,0.995,0.141,0.89,0.868,0.999,0.006,1.0,0.045,0.048,0.012,0.494,0.988,0.767,0.017,0.089,0.494,0.099,0.99,0.999,0.999,1.0,0.659,0.997,0.925,0.018,0.191,0.16,1.0,0.996,0.992,0.997,0.999,0.826,0.999,0.998,0.999,0.083,0.975,0.993,0.994,0.983,0.002,0.092,1.0,1.0,0.081,0.923,0.981,1.0,0.975,0.998,0.976,0.999,1.0,0.999,0.998,0.787,0.958,0.398,0.091,0.919,0.659,0.979,0.033,0.997,0.984,0.037,0.176,0.997,0.208,0.998,0.975,0.974,0.988,1.0,0.056,0.063,0.998,0.959,0.007,0.999,0.001,0.041,0.816,0.077,0.086,0.998,1.0,0.489,1.0,1.0,0.996,0.803,0.998,0.064,0.997,0.999,0.92,1.0,0.995,0.966,0.989,0.747,0.966,0.995,0.794,0.993,0.091,0.059,0.005,0.558,0.379,1.0,0.996,0.001,0.992,0.966,0.02,0.96,0.974,0.097,0.216,0.03,0.041,0.839,0.999,0.997,0.106,0.99,0.879,0.472,0.696,0.148,0.139,0.097,0.881,0.886,0.819,0.993,1.0,0.02,0.113,0.707,0.982,0.173,0.95,0.597,0.968,0.905,0.011,0.039,0.623,0.213,0.988,0.996,0.854,0.364,0.981,0.989,0.957,0.713,0.9,0.718,0.98,0.999,0.997,1.0,0.284,0.903,1.0,0.759,0.994,0.02,0.996,0.961,0.778,0.95,0.124,0.007,0.012,0.175,0.999,0.029,0.225,0.485,0.555,0.951,0.042,0.405,0.883,0.044,0.005,0.935,0.996,0.819,0.996,0.641,0.405,0.188,0.043,0.991,0.948,0.979,0.602,0.408,0.968,0.851,0.986,0.483,0.938,0.99,0.051,0.951,0.805,0.915,0.025,0.986,0.099,0.994,0.59,0.43,0.008,0.92,0.677,0.994,0.219,0.461,0.981,0.79,0.973,0.011,0.893,0.906,0.884,0.011,0.582,0.234,0.471,0.055,0.944,0.885,0.518,0.847,0.519,0.927,0.375,0.253,0.762,0.949,0.998,0.981,0.741,0.994,0.448,0.007,0.469,0.01,0.9,0.636,0.008,0.96,0.996,0.998,0.998,0.999,0.891,0.985,0.924,0.984,0.959,0.982,0.994,0.987,0.972,0.614,0.808,0.989,0.996,0.611,0.909,0.111,0.692,0.974,0.345,0.962,0.837,0.546,0.997,0.671,0.998,0.293,0.991,0.982,1.0,0.997,0.998,0.989,0.819,0.149,0.835,0.044,0.218,0.984,0.168,0.975,0.686,0.989,0.788,0.204,0.989,0.876,0.971,0.988,0.201,0.763,0.759,0.5,0.886,0.116,0.698,0.998,0.624,0.124,0.869,0.982,0.288,0.948,0.701,0.101,0.97,0.027,0.538,0.357,0.011,0.957,0.991,0.153,0.708,0.942,0.103,0.999,0.998,0.996,0.891,0.831,0.988,0.99,0.998,0.245,0.907,0.897,0.38,0.697,0.192,0.004,0.097,0.843,0.002,0.363,0.009,0.063,0.967,0.982,0.03,0.326,0.048,0.03,0.424,0.981,0.015,0.084,0.538,0.204,0.752,0.995,0.425,0.357,0.28,0.078,0.968,0.161,0.998,0.566,0.769,0.172,0.979,0.538,0.405,0.897,0.186,0.846,0.935,0.263,0.272,0.523,0.372,0.932,0.873,0.988,0.973,0.842,0.823,0.988,0.94,0.942,0.997,0.016,0.274,0.699,0.014,0.621,0.98,0.903,0.856,0.39,0.063,0.195,0.269,0.845,0.867,0.447,0.046,0.602,0.455,0.945,0.072,0.997,0.66,0.825,0.922,1.0,0.837,0.327,0.963,0.937,0.787,0.995,0.887,0.088,0.041,0.976,0.43,0.046,0.088,0.197,0.791,0.02,0.59,0.825,0.849,0.957,0.997,0.989,0.045,0.613,0.983,0.953,0.94,0.042,0.874,0.167,0.227,0.913,0.618,0.899,0.851,0.829,0.064,0.777,0.543,0.004,0.325,0.105,0.139,0.98,0.461,0.775,1.0,0.25,0.119,0.929,0.551,0.998,0.895,0.006,0.245,0.768,0.986,0.648,0.013,0.109,0.999,0.999,0.445,0.89,0.028,0.015,0.002,0.571,0.117,0.134,0.929,0.311,0.898,0.016,0.494,0.016,0.848,0.883,0.991,0.011,0.994,0.049,0.994,0.996,0.996,0.982,0.568,0.013,0.004,0.472,0.46,0.83,0.417,0.212,0.969,0.823,0.401,0.69,0.91,0.988,0.399,0.231,0.224,0.828,0.536,0.109,0.994,0.422,0.709,0.989,0.347,0.989,0.905,0.002,0.829,0.031,0.052,0.989,0.995,0.336,0.91,0.15,0.998,0.474,0.113,0.037,0.738,0.432,0.059,0.292,0.514,0.529,0.457,0.529,0.829,0.961,0.996,0.017,0.847,0.985,0.874,0.383,0.123,0.265,0.676,0.087,0.984,0.045,0.493,0.92,0.183,0.231,0.91,0.969,0.228,0.976,0.289,0.48,0.475,0.85,0.142,0.973,0.028,0.076,0.046,0.102,0.904,0.95,0.508,0.724,0.999,0.081,0.985,0.961,0.997,0.014,0.826,0.005,0.757,0.955,0.975,0.043,0.996,0.961,0.581,0.97,0.825,0.771,0.909,0.863,0.032,0.278,0.139,0.065,0.366,0.99,0.88,0.948,0.074,0.335,0.004,0.121,0.086,0.073,0.125,0.058,0.923,0.642,0.834,0.082,0.129,0.983,0.931,0.843,0.176,0.83,0.009,0.89,0.215,0.254,0.284,0.884,0.286,0.96,0.102,0.459,0.995,0.087,0.016,0.755,0.932,0.623,0.623,0.999,0.128,0.386,0.366,0.807,0.924,0.942,0.513,0.997,0.909,0.785,0.036,0.985,0.474,0.747,0.083,0.324,0.425,0.833,0.19,0.002,0.977,0.17,0.102,0.138,0.97,0.886,0.333,0.447,0.03,0.337,0.993,0.931,0.185,0.285,0.407,0.672,0.955,1.0,0.024,0.87,0.996,0.002,0.031,0.493,0.048,0.753,0.898,0.321,0.166,0.998,0.435,0.439,0.884,0.894,0.924,0.156,0.699,0.509,0.363,0.972,0.387,0.009,0.907,0.978,0.763,0.541,0.998,0.908,0.971,0.309,0.407,0.839,0.802,0.072,0.003,0.79,0.692,0.973,0.997,0.569,0.732,0.227,1.0,0.005,0.094,0.033,0.004,0.878,0.918,0.419,0.975,0.001,0.843,0.343,0.491,0.948,0.502'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub22 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub22['pred'] = sub22['pred'].apply(lambda x: round(x,3))\n",
    "sub22_txt = ''\n",
    "for prob in list(sub22['pred'].values):\n",
    "    sub22_txt = sub22_txt+','+str(prob)\n",
    "sub22_txt = sub22_txt[1:]\n",
    "\n",
    "sub22_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a231b40",
   "metadata": {},
   "source": [
    "### Submission 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_tr2, y_tr2 = oversample.fit_resample(X_train_experienced_imputed, y_train_experienced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66219e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:45:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=10,criterion='entropy',n_estimators=100,min_child_samples=20,colsample_bytree=0.8)\n",
    "rf.fit(X_tr2,y_tr2)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff3171f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.8949664519192359\n",
      "Val ROC:  0.8175559832970454\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "984a6034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.07172</td>\n",
       "      <td>0.46134</td>\n",
       "      <td>0.53306</td>\n",
       "      <td>0.126187</td>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.622311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5          0.07172          0.46134         0.53306       0.126187   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.496124      0.622311  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca3642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.998,0.996,0.995,0.999,0.995,0.999,0.364,0.006,0.989,0.999,0.068,0.194,0.999,1.0,0.986,1.0,0.725,0.104,0.998,0.996,1.0,0.982,0.999,1.0,1.0,0.996,0.998,1.0,0.982,0.999,1.0,0.992,0.997,0.998,0.997,0.998,0.864,0.921,0.998,0.897,0.907,0.983,0.998,1.0,1.0,0.987,1.0,0.273,0.129,0.483,0.063,1.0,0.998,0.0,0.029,0.016,1.0,0.999,0.998,1.0,1.0,0.856,0.075,0.998,1.0,0.955,1.0,0.975,0.998,0.999,1.0,0.999,0.999,0.999,0.94,0.908,0.991,1.0,0.999,0.994,0.019,0.937,0.999,0.122,1.0,0.982,0.544,1.0,0.999,0.998,1.0,0.995,0.883,1.0,0.107,0.999,0.994,0.988,0.341,0.999,0.959,1.0,1.0,1.0,0.957,0.693,0.864,0.889,0.996,0.803,0.947,1.0,0.996,1.0,0.896,0.995,0.949,0.999,0.998,1.0,1.0,1.0,0.98,0.842,1.0,0.998,1.0,1.0,0.03,0.0,0.999,0.978,0.003,0.999,0.997,0.999,0.474,0.999,0.973,0.997,0.178,0.989,0.998,0.996,0.926,1.0,0.959,0.997,0.222,0.999,0.999,0.996,0.999,1.0,0.748,0.996,0.995,0.997,1.0,0.97,0.999,0.999,1.0,0.789,1.0,0.998,0.973,0.994,1.0,0.127,1.0,0.212,0.93,1.0,0.998,0.966,1.0,0.999,0.635,0.999,0.36,0.026,0.993,0.047,0.835,1.0,0.998,1.0,1.0,0.993,1.0,0.048,0.392,0.997,0.776,0.005,0.994,0.027,0.999,0.98,0.998,1.0,0.999,0.999,0.918,0.446,0.253,0.495,0.165,0.024,1.0,0.998,0.198,1.0,0.999,0.692,1.0,0.999,0.719,0.999,0.998,0.999,0.565,0.999,0.999,0.537,0.007,0.994,0.392,0.087,0.002,0.995,0.937,0.999,0.061,1.0,1.0,0.994,1.0,0.998,0.006,0.999,0.995,1.0,1.0,0.091,0.964,0.011,0.996,0.995,0.926,0.021,0.996,1.0,0.013,0.958,1.0,1.0,0.143,0.001,1.0,0.721,0.029,0.999,1.0,0.999,0.961,1.0,0.996,0.084,0.991,0.981,1.0,0.207,0.996,0.994,1.0,0.999,0.999,0.995,0.999,0.991,0.002,0.996,0.915,0.134,0.998,0.999,0.439,0.998,0.002,0.023,0.054,1.0,0.963,0.027,1.0,0.011,0.942,1.0,0.999,1.0,0.999,0.991,1.0,1.0,0.987,1.0,1.0,1.0,1.0,1.0,0.022,1.0,0.997,0.038,1.0,0.556,0.964,0.688,0.995,0.996,0.003,0.777,0.999,0.997,0.016,0.806,0.893,0.82,0.051,1.0,0.022,1.0,0.966,0.993,0.995,0.998,0.008,0.934,0.998,0.996,0.936,0.999,0.977,0.998,0.984,0.015,0.997,1.0,0.06,0.175,0.998,0.998,0.063,1.0,0.918,0.999,1.0,1.0,1.0,1.0,0.22,0.974,0.998,0.997,0.02,1.0,0.998,0.007,1.0,0.982,0.996,1.0,0.993,0.993,0.995,0.999,1.0,0.994,0.924,0.996,0.004,0.989,0.998,0.999,0.002,1.0,1.0,0.993,0.128,0.999,0.42,1.0,0.996,1.0,1.0,1.0,0.985,0.998,0.998,0.998,1.0,0.971,1.0,0.999,0.003,0.024,0.977,0.047,0.036,0.105,0.987,0.984,0.995,0.005,0.998,0.001,0.302,0.932,1.0,0.99,0.997,1.0,0.151,0.139,0.958,0.334,1.0,0.998,0.99,0.004,0.011,1.0,0.758,1.0,0.998,0.994,0.043,1.0,0.254,0.996,0.999,0.293,1.0,0.978,0.76,1.0,0.002,0.34,0.999,0.864,0.003,0.978,0.906,0.999,1.0,1.0,0.997,1.0,1.0,0.132,0.999,0.991,0.993,0.997,0.002,0.875,0.047,0.998,0.833,0.785,0.055,0.993,0.857,0.998,0.998,0.999,0.01,0.987,0.029,0.921,0.999,0.005,0.003,0.021,0.995,0.998,0.996,0.055,0.002,0.977,0.099,0.993,0.939,1.0,1.0,0.153,0.687,0.03,0.999,0.993,1.0,0.993,0.003,1.0,0.84,1.0,1.0,0.807,0.999,0.987,0.977,0.877,0.203,0.168,0.009,0.015,0.948,0.06,0.986,0.35,0.035,0.694,0.999,0.997,0.999,1.0,1.0,0.995,0.006,0.999,0.306,0.005,1.0,0.838,0.107,0.003,0.999,0.53,0.133,0.998,0.068,0.074,0.997,0.05,0.009,0.998,0.375,0.466,0.998,0.999,0.994,0.999,0.993,0.999,0.01,0.992,0.823,0.997,1.0,0.105,0.855,0.999,0.013,0.999,0.999,0.043,0.539,0.961,1.0,0.994,0.002,0.017,0.999,0.998,0.527,0.999,0.999,1.0,1.0,0.127,0.999,0.999,0.608,1.0,1.0,0.827,0.993,0.978,0.997,1.0,0.975,0.092,0.999,0.96,1.0,0.005,0.006,0.998,0.996,1.0,0.935,1.0,0.042,0.924,0.242,0.999,1.0,0.882,0.998,0.999,0.009,0.844,0.078,0.997,0.989,0.186,0.201,0.014,1.0,0.979,0.999,1.0,1.0,0.076,0.083,0.999,1.0,1.0,0.962,0.999,0.294,0.008,0.996,0.792,0.441,0.077,0.997,0.999,1.0,0.999,0.987,0.602,1.0,0.098,0.25,0.883,1.0,0.996,1.0,0.537,0.01,0.994,0.996,0.999,0.994,0.021,0.998,1.0,0.853,0.95,0.071,0.552,0.036,0.763,0.996,1.0,1.0,0.963,0.356,0.999,0.995,0.967,0.908,0.951,1.0,0.996,0.083,0.142,0.741,0.16,0.87,0.995,0.999,0.994,0.012,0.98,0.993,0.955,0.074,0.227,0.987,0.476,0.968,0.981,1.0,1.0,0.004,0.987,0.99,0.999,0.298,1.0,0.998,1.0,1.0,0.032,0.995,0.413,1.0,0.061,0.998,0.001,0.999,0.981,0.97,0.999,0.886,0.435,0.302,0.848,1.0,0.698,0.999,0.98,0.043,1.0,0.002,0.13,1.0,0.981,0.92,0.997,0.985,0.999,0.999,0.999,0.999,1.0,1.0,0.994,0.991,1.0,1.0,0.81,0.331,0.976,0.243,0.999,0.999,0.676,0.729,1.0,0.329,0.994,1.0,0.118,0.135,0.998,1.0,1.0,0.006,0.846,1.0,0.998,0.993,0.014,0.999,0.569,0.0,1.0,1.0,0.065,0.061,0.151,0.201,0.989,1.0,0.979,0.998,0.408,0.999,0.9,1.0,0.795,0.29,0.982,1.0,0.158,0.254,0.989,0.092,0.915,0.984,0.703,1.0,0.027,0.999,0.961,0.071,0.015,0.999,0.005,0.776,1.0,0.965,0.977,1.0,1.0,0.999,0.221,0.995,0.851,0.597,0.999,1.0,1.0,0.998,1.0,0.999,1.0,0.966,0.866,0.996,0.951,0.998,0.965,0.116,1.0,0.989,0.999,1.0,0.179,0.14,0.987,0.33,0.005,0.987,0.008,1.0,0.922,1.0,0.999,0.998,0.998,1.0,0.868,1.0,0.983,0.968,0.826,0.998,0.5,0.052,1.0,0.993,0.335,0.999,0.999,1.0,1.0,0.062,0.999,0.026,0.484,0.021,0.984,0.989,0.49,1.0,0.772,1.0,0.408,0.586,0.996,1.0,1.0,0.011,0.37,0.436,0.022,1.0,0.982,0.916,0.269,1.0,0.865,0.674,0.05,1.0,0.071,0.063,0.984,1.0,0.995,0.994,0.376,0.011,0.71,0.173,0.986,0.904,0.879,0.088,0.999,0.034,0.063,0.508,0.987,0.873,0.214,1.0,0.999,0.016,0.979,1.0,0.965,0.816,1.0,0.991,0.045,1.0,0.993,0.978,0.07,0.997,0.605,0.213,0.013,0.997,1.0,1.0,0.999,0.253,0.999,0.999,0.01,0.999,0.961,1.0,0.08,0.11,1.0,1.0,1.0,0.977,1.0,0.997,0.005,1.0,0.536,0.002,1.0,0.943,1.0,1.0,1.0,1.0,0.911,1.0,0.767,0.026,0.997,0.999,1.0,0.998,0.151,0.999,0.3,0.685,0.014,0.004,1.0,1.0,0.999,0.998,1.0,0.995,0.509,1.0,0.953,0.979,0.674,0.993,0.186,1.0,0.974,0.992,1.0,0.999,1.0,0.993,1.0,0.103,1.0,0.781,0.99,0.986,0.997,0.999,0.971,0.299,1.0,0.995,0.957,0.316,0.148,0.999,0.945,0.369,0.999,0.784,0.993,0.988,0.01,0.162,1.0,0.519,1.0,0.025,0.988,0.05,1.0,0.004,0.382,0.052,0.165,0.065,0.999,0.999,0.991,0.998,0.912,0.999,0.361,0.824,0.001,0.98,0.996,0.141,0.244,0.015,0.278,0.036,0.015,0.974,1.0,0.968,0.995,1.0,0.189,0.837,0.993,0.996,0.997,0.999,0.009,0.905,0.93,0.142,0.637,1.0,0.999,0.623,1.0,0.998,0.999,0.993,0.881,1.0,0.069,0.999,0.999,0.996,0.998,0.995,1.0,0.914,0.998,0.665,0.875,0.488,0.671,0.88,0.987,0.665,0.4,0.693,0.005,0.081,0.281,0.966,0.094,0.15,0.648,0.779,0.997,0.852,1.0,0.893,0.016,0.82,0.284,0.991,0.336,0.835,0.914,0.417,0.097,0.434,1.0,1.0,0.752,0.705,0.955,0.019,0.968,0.407,0.49,0.003,1.0,0.838,0.028,0.965,0.999,0.045,0.787,0.999,0.399,0.015,0.994,0.088,0.999,0.025,0.994,0.996,0.992,1.0,0.01,0.463,0.961,0.573,0.086,0.997,0.739,0.248,0.009,0.996,0.997,0.951,0.034,0.987,0.992,1.0,0.3,0.996,1.0,0.375,0.026,0.08,0.92,0.997,0.999,0.988,0.995,0.958,0.999,1.0,0.768,0.519,1.0,0.01,0.999,0.665,1.0,0.356,0.93,0.037,1.0,0.987,0.153,0.172,0.043,1.0,0.963,0.966,0.001,1.0,0.001,0.009,0.999,0.573,0.045,0.903,0.058,0.997,0.606,0.982,0.366,0.99,0.786,0.991,0.525,0.895,0.023,0.963,0.888,0.869,0.998,0.999,0.012,1.0,0.96,0.999,0.723,1.0,0.973,0.31,0.971,0.998,1.0,0.923,0.798,0.018,0.434,0.124,0.029,0.931,0.959,0.049,0.129,0.999,0.149,1.0,1.0,0.062,1.0,0.028,0.167,0.0,0.039,1.0,0.999,0.005,0.991,1.0,1.0,1.0,0.986,0.693,0.999,0.985,0.367,0.996,0.036,0.999,0.035,0.963,0.901,1.0,0.719,0.999,0.149,0.048,0.018,0.147,1.0,0.999,0.596,0.999,0.153,0.913,0.914,0.999,0.955,0.012,1.0,0.175,1.0,0.999,0.014,0.996,0.999,0.157,0.787,0.985,0.861,0.982,0.988,0.86,0.246,0.385,0.103,0.021,0.963,0.065,0.964,0.996,0.96,1.0,0.956,0.0,1.0,0.045,1.0,0.13,0.999,0.001,0.426,0.981,0.999,0.019,0.03,0.967,0.156,0.977,1.0,0.376,0.012,0.209,0.995,1.0,0.084,0.997,0.087,0.035,0.632,0.86,0.999,0.196,0.992,0.318,0.067,0.242,0.998,0.534,0.781,0.911,0.043,0.024,1.0,0.016,0.984,0.098,0.985,0.997,1.0,1.0,0.754,1.0,0.874,0.748,0.988,0.282,0.031,0.749,0.989,0.659,0.988,1.0,0.217,0.238,0.043,0.975,0.486,0.673,1.0,0.045,0.424,0.995,1.0,0.564,0.999,0.932,0.223,0.028,0.179,0.844,0.174,0.252,0.339,0.121,0.034,0.156,0.996,0.258,0.072,0.416,0.304,0.874,0.991,0.882,0.667,0.651,0.242,0.509,0.019,0.682,0.513,0.92,0.407,0.033,0.977,0.492,0.87,0.464,0.354,0.035,1.0,0.24,0.968,0.84,0.334,0.557,0.071,0.364,0.07,0.926,0.888,0.236,0.752,0.016,0.982,0.987,0.009,0.998,0.834,0.054,0.539,0.095,0.438,0.919,0.955,1.0,0.995,0.774,0.781,0.437,0.076,0.992,0.972,0.496,0.617,0.024,0.999,0.961,0.007,0.074,0.738,0.997,0.697,0.074,0.882,0.987,0.378,0.694,0.998,0.084,0.999,0.998,0.254,0.016,0.819,0.047,0.803,0.266,1.0,0.993,0.114,0.298,0.834,0.327,0.964,1.0,0.454,0.252,0.997,0.532,0.748,0.993,0.999,0.986,0.919,0.954,0.962,1.0,0.095,0.992,0.746,0.994,0.003,0.949,0.997,0.702,0.024,0.987,0.999,0.428,0.739,0.524,0.999,1.0,1.0,0.942,1.0,0.96,1.0,0.983,0.999,0.192,0.414,0.061,0.999,0.065,0.999,0.699,0.997,0.879,0.903,0.863,0.043,0.999,0.979,0.974,0.999,1.0,0.995,0.034,1.0,0.105,0.991,0.999,0.002,1.0,0.203,0.055,0.999,0.168,0.917,1.0,0.863,0.983,0.995,0.225,0.042,0.038,0.961,0.182,0.324,0.165,0.997,0.368,0.09,0.999,0.207,0.991,0.965,1.0,0.978,0.767,0.009,0.999,0.0,0.737,0.962,0.999,0.991,0.039,0.502,0.851,0.994,0.939,0.999,0.999,0.002,0.982,0.004,0.248,0.998,0.877,0.982,0.129,0.93,0.942,0.004,0.999,0.351,0.988,0.973,1.0,0.932,0.954,1.0,0.135,0.976,0.927,0.23,0.996,0.993,0.999,0.933,0.99,0.999,0.035,0.059,0.024,0.017,0.868,0.44,0.301,0.977,0.998,0.243,1.0,1.0,0.012,0.193,0.995,0.141,0.89,0.868,0.999,0.006,1.0,0.045,0.048,0.012,0.494,0.988,0.767,0.017,0.089,0.494,0.099,0.99,0.999,0.999,1.0,0.659,0.997,0.925,0.018,0.191,0.16,1.0,0.996,0.992,0.997,0.999,0.826,0.999,0.998,0.999,0.083,0.975,0.993,0.994,0.983,0.002,0.092,1.0,1.0,0.081,0.923,0.981,1.0,0.975,0.998,0.976,0.999,1.0,0.999,0.998,0.787,0.958,0.398,0.091,0.919,0.659,0.979,0.033,0.997,0.984,0.037,0.176,0.997,0.208,0.998,0.975,0.974,0.988,1.0,0.056,0.063,0.998,0.959,0.007,0.999,0.001,0.041,0.816,0.077,0.086,0.998,1.0,0.489,1.0,1.0,0.996,0.803,0.998,0.064,0.997,0.999,0.92,1.0,0.995,0.966,0.989,0.747,0.966,0.995,0.794,0.993,0.091,0.059,0.005,0.558,0.379,1.0,0.996,0.001,0.992,0.966,0.02,0.96,0.974,0.097,0.216,0.03,0.041,0.839,0.999,0.997,0.106,0.99,0.879,0.472,0.696,0.148,0.139,0.097,0.881,0.886,0.819,0.993,1.0,0.02,0.113,0.707,0.982,0.173,0.95,0.597,0.968,0.905,0.011,0.039,0.623,0.213,0.988,0.996,0.854,0.364,0.981,0.989,0.957,0.713,0.9,0.718,0.98,0.999,0.997,1.0,0.284,0.903,1.0,0.759,0.994,0.02,0.996,0.961,0.778,0.95,0.124,0.007,0.012,0.175,0.999,0.029,0.225,0.485,0.555,0.951,0.042,0.405,0.883,0.044,0.005,0.935,0.996,0.819,0.996,0.641,0.405,0.188,0.043,0.991,0.948,0.979,0.602,0.408,0.968,0.851,0.986,0.483,0.938,0.99,0.051,0.951,0.805,0.915,0.025,0.986,0.099,0.994,0.59,0.43,0.008,0.92,0.677,0.994,0.219,0.461,0.981,0.79,0.973,0.011,0.893,0.906,0.884,0.011,0.582,0.234,0.471,0.055,0.944,0.885,0.518,0.847,0.519,0.927,0.375,0.253,0.762,0.949,0.998,0.981,0.741,0.994,0.448,0.007,0.469,0.01,0.9,0.636,0.008,0.96,0.996,0.998,0.998,0.999,0.891,0.985,0.924,0.984,0.959,0.982,0.994,0.987,0.972,0.614,0.808,0.989,0.996,0.611,0.909,0.111,0.692,0.974,0.345,0.962,0.837,0.546,0.997,0.671,0.998,0.293,0.991,0.982,1.0,0.997,0.998,0.989,0.819,0.149,0.835,0.044,0.218,0.984,0.168,0.975,0.686,0.989,0.788,0.204,0.989,0.876,0.971,0.988,0.201,0.763,0.759,0.5,0.886,0.116,0.698,0.998,0.624,0.124,0.869,0.982,0.288,0.948,0.701,0.101,0.97,0.027,0.538,0.357,0.011,0.957,0.991,0.153,0.708,0.942,0.103,0.999,0.998,0.996,0.891,0.831,0.988,0.99,0.998,0.245,0.907,0.897,0.38,0.697,0.192,0.004,0.097,0.843,0.002,0.363,0.009,0.063,0.967,0.982,0.03,0.326,0.048,0.03,0.424,0.981,0.015,0.084,0.538,0.204,0.752,0.995,0.425,0.357,0.28,0.078,0.968,0.161,0.998,0.566,0.769,0.172,0.979,0.538,0.405,0.897,0.186,0.846,0.935,0.263,0.272,0.523,0.372,0.932,0.873,0.988,0.973,0.842,0.823,0.988,0.94,0.942,0.997,0.016,0.274,0.699,0.014,0.621,0.98,0.903,0.856,0.39,0.063,0.195,0.269,0.845,0.867,0.447,0.046,0.602,0.455,0.945,0.072,0.997,0.66,0.825,0.922,1.0,0.837,0.327,0.963,0.937,0.787,0.995,0.887,0.088,0.041,0.976,0.43,0.046,0.088,0.197,0.791,0.02,0.59,0.825,0.849,0.957,0.997,0.989,0.045,0.613,0.983,0.953,0.94,0.042,0.874,0.167,0.227,0.913,0.618,0.899,0.851,0.829,0.064,0.777,0.543,0.004,0.325,0.105,0.139,0.98,0.461,0.775,1.0,0.25,0.119,0.929,0.551,0.998,0.895,0.006,0.245,0.768,0.986,0.648,0.013,0.109,0.999,0.999,0.445,0.89,0.028,0.015,0.002,0.571,0.117,0.134,0.929,0.311,0.898,0.016,0.494,0.016,0.848,0.883,0.991,0.011,0.994,0.049,0.994,0.996,0.996,0.982,0.568,0.013,0.004,0.472,0.46,0.83,0.417,0.212,0.969,0.823,0.401,0.69,0.91,0.988,0.399,0.231,0.224,0.828,0.536,0.109,0.994,0.422,0.709,0.989,0.347,0.989,0.905,0.002,0.829,0.031,0.052,0.989,0.995,0.336,0.91,0.15,0.998,0.474,0.113,0.037,0.738,0.432,0.059,0.292,0.514,0.529,0.457,0.529,0.829,0.961,0.996,0.017,0.847,0.985,0.874,0.383,0.123,0.265,0.676,0.087,0.984,0.045,0.493,0.92,0.183,0.231,0.91,0.969,0.228,0.976,0.289,0.48,0.475,0.85,0.142,0.973,0.028,0.076,0.046,0.102,0.904,0.95,0.508,0.724,0.999,0.081,0.985,0.961,0.997,0.014,0.826,0.005,0.757,0.955,0.975,0.043,0.996,0.961,0.581,0.97,0.825,0.771,0.909,0.863,0.032,0.278,0.139,0.065,0.366,0.99,0.88,0.948,0.074,0.335,0.004,0.121,0.086,0.073,0.125,0.058,0.923,0.642,0.834,0.082,0.129,0.983,0.931,0.843,0.176,0.83,0.009,0.89,0.215,0.254,0.284,0.884,0.286,0.96,0.102,0.459,0.995,0.087,0.016,0.755,0.932,0.623,0.623,0.999,0.128,0.386,0.366,0.807,0.924,0.942,0.513,0.997,0.909,0.785,0.036,0.985,0.474,0.747,0.083,0.324,0.425,0.833,0.19,0.002,0.977,0.17,0.102,0.138,0.97,0.886,0.333,0.447,0.03,0.337,0.993,0.931,0.185,0.285,0.407,0.672,0.955,1.0,0.024,0.87,0.996,0.002,0.031,0.493,0.048,0.753,0.898,0.321,0.166,0.998,0.435,0.439,0.884,0.894,0.924,0.156,0.699,0.509,0.363,0.972,0.387,0.009,0.907,0.978,0.763,0.541,0.998,0.908,0.971,0.309,0.407,0.839,0.802,0.072,0.003,0.79,0.692,0.973,0.997,0.569,0.732,0.227,1.0,0.005,0.094,0.033,0.004,0.878,0.918,0.419,0.975,0.001,0.843,0.343,0.491,0.948,0.502'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub22 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub22['pred'] = sub22['pred'].apply(lambda x: round(x,3))\n",
    "sub22_txt = ''\n",
    "for prob in list(sub22['pred'].values):\n",
    "    sub22_txt = sub22_txt+','+str(prob)\n",
    "sub22_txt = sub22_txt[1:]\n",
    "\n",
    "sub22_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f97a4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.995,0.998,0.957,1.0,0.997,1.0,0.619,0.008,0.998,0.997,0.44,0.005,0.999,1.0,0.988,0.999,0.014,0.045,0.999,0.999,1.0,0.99,0.999,0.999,0.999,0.995,0.999,1.0,0.996,0.998,1.0,0.995,0.996,0.999,0.999,0.999,0.938,0.978,0.997,0.965,0.987,0.938,0.98,0.994,1.0,0.937,1.0,0.345,0.321,0.658,0.008,1.0,0.998,0.003,0.942,0.011,1.0,0.999,0.999,0.999,1.0,0.201,0.033,0.998,0.999,0.972,1.0,0.984,0.998,1.0,1.0,0.982,1.0,1.0,0.999,0.943,0.993,1.0,0.999,0.998,0.025,0.991,1.0,0.011,1.0,0.994,0.432,1.0,0.998,0.997,1.0,0.998,0.846,0.999,0.02,0.997,0.999,0.981,0.493,0.997,0.997,1.0,0.996,1.0,0.92,0.494,0.997,0.51,0.998,0.204,0.977,1.0,0.999,0.999,0.732,1.0,0.696,0.998,0.999,0.998,1.0,1.0,0.766,0.977,0.999,0.998,0.999,1.0,0.01,0.012,1.0,0.942,0.014,0.997,0.999,0.999,0.91,0.999,0.947,0.992,0.215,0.89,0.997,0.998,0.846,1.0,0.937,0.992,0.68,1.0,1.0,0.999,0.999,1.0,0.346,0.99,0.992,0.997,1.0,0.897,0.993,0.989,1.0,0.969,1.0,0.996,0.852,0.999,0.998,0.167,1.0,0.385,0.987,1.0,0.985,0.901,0.999,0.999,0.075,1.0,0.076,0.001,0.988,0.949,0.187,1.0,0.998,1.0,1.0,0.999,1.0,0.132,0.66,0.992,0.997,0.016,1.0,0.036,0.997,0.997,0.999,1.0,1.0,0.999,0.914,0.216,0.323,0.199,0.273,0.002,1.0,0.999,0.159,1.0,0.998,0.292,1.0,0.999,0.989,0.999,0.93,1.0,0.627,1.0,1.0,0.413,0.016,0.999,0.572,0.506,0.0,0.996,0.979,1.0,0.007,1.0,1.0,0.993,1.0,1.0,0.007,0.997,0.998,1.0,1.0,0.087,0.962,0.004,0.999,0.987,0.996,0.0,1.0,1.0,0.001,0.777,1.0,0.998,0.017,0.004,0.999,0.916,0.761,1.0,1.0,1.0,0.734,1.0,0.997,0.015,0.998,0.973,1.0,0.231,0.994,0.992,0.999,1.0,1.0,0.993,1.0,0.998,0.004,0.999,0.975,0.015,0.997,0.998,0.704,0.999,0.094,0.021,0.086,1.0,0.993,0.004,0.994,0.001,0.594,1.0,1.0,0.998,0.996,0.998,1.0,1.0,0.998,1.0,1.0,1.0,1.0,1.0,0.105,0.994,0.991,0.044,1.0,0.999,0.916,0.903,0.998,0.998,0.109,0.467,1.0,0.999,0.758,0.888,0.92,0.98,0.07,0.998,0.004,1.0,0.906,0.307,1.0,0.999,0.013,0.989,0.999,0.99,0.917,1.0,0.994,0.999,0.995,0.004,0.999,1.0,0.034,0.462,1.0,0.997,0.699,0.999,0.957,0.999,1.0,1.0,1.0,0.998,0.82,0.977,0.964,0.981,0.413,1.0,0.998,0.042,0.999,0.973,0.998,1.0,0.987,0.998,0.999,1.0,0.998,0.987,0.886,0.999,0.026,0.923,0.998,0.997,0.002,0.999,1.0,0.999,0.116,1.0,0.49,1.0,0.985,0.999,1.0,1.0,0.985,0.998,0.992,1.0,1.0,0.83,1.0,0.975,0.134,0.007,0.997,0.129,0.012,0.845,0.991,0.528,0.999,0.011,0.998,0.003,0.112,0.968,1.0,0.96,0.988,1.0,0.02,0.032,0.998,0.831,1.0,0.965,0.414,0.033,0.001,1.0,0.256,0.999,0.999,0.991,0.005,0.997,0.02,1.0,0.999,0.356,0.999,0.71,0.492,1.0,0.008,0.551,0.998,0.991,0.616,1.0,0.899,1.0,1.0,1.0,0.968,0.999,1.0,0.012,0.99,0.997,0.996,0.999,0.27,0.683,0.011,0.993,0.988,0.105,0.012,0.91,0.977,0.982,0.997,1.0,0.955,0.989,0.02,0.975,1.0,0.005,0.009,0.011,0.993,0.999,0.999,0.043,0.0,0.927,0.981,0.97,0.929,1.0,1.0,0.918,0.136,0.435,0.996,0.991,1.0,0.993,0.018,1.0,0.036,1.0,0.999,0.111,0.992,0.744,0.972,0.265,0.123,0.012,0.002,0.479,0.917,0.004,0.932,0.38,0.009,0.471,0.999,1.0,0.998,1.0,1.0,0.998,0.013,1.0,0.781,0.02,1.0,0.928,0.628,0.017,0.997,0.949,0.051,1.0,0.861,0.051,0.992,0.873,0.005,0.988,0.213,0.849,1.0,0.998,0.999,0.999,0.988,0.998,0.001,0.984,0.861,1.0,0.999,0.002,0.834,1.0,0.032,0.999,1.0,0.051,0.055,0.996,1.0,0.888,0.004,0.029,1.0,0.996,0.832,0.974,0.999,1.0,1.0,0.595,0.999,1.0,0.531,1.0,1.0,0.91,0.997,0.998,0.981,1.0,0.998,0.023,0.999,0.941,1.0,0.347,0.827,0.998,0.999,1.0,0.236,1.0,0.049,0.921,0.702,1.0,0.998,0.982,1.0,0.998,0.003,0.621,0.002,1.0,0.985,0.018,0.057,0.159,1.0,0.991,1.0,1.0,0.999,0.044,0.207,1.0,0.993,1.0,0.982,0.999,0.638,0.004,1.0,0.933,0.929,0.001,0.999,1.0,1.0,0.999,0.865,0.829,1.0,0.072,0.963,0.991,1.0,0.988,0.999,0.741,0.034,0.996,0.995,0.996,0.993,0.029,0.999,0.999,0.942,0.966,0.004,0.918,0.204,0.932,1.0,1.0,1.0,0.987,0.327,0.999,0.999,0.996,0.974,0.987,1.0,0.998,0.011,0.92,0.631,0.009,0.444,1.0,0.983,0.999,0.005,0.852,0.995,0.769,0.013,0.141,0.772,0.954,0.98,0.954,1.0,1.0,0.088,0.992,0.999,0.988,0.108,1.0,0.938,0.998,0.999,0.939,1.0,0.227,1.0,0.827,0.998,0.055,0.999,0.997,0.997,0.999,0.985,0.051,0.554,0.961,1.0,0.181,1.0,0.984,0.439,1.0,0.005,0.06,1.0,0.793,0.885,1.0,0.915,0.996,1.0,1.0,0.999,0.999,1.0,0.999,0.996,1.0,1.0,0.85,0.195,0.998,0.403,0.996,0.999,0.893,0.545,1.0,0.869,0.999,1.0,0.025,0.985,0.999,1.0,1.0,0.008,0.997,1.0,0.989,0.992,0.155,0.998,0.857,0.001,0.998,1.0,0.903,0.232,0.973,0.31,0.982,0.999,0.916,0.999,0.952,0.999,0.845,1.0,0.786,0.013,0.977,1.0,0.039,0.06,0.999,0.784,0.349,0.758,0.577,0.999,0.004,0.999,0.939,0.873,0.002,0.994,0.001,0.772,1.0,0.988,0.809,1.0,1.0,1.0,0.901,0.998,0.999,0.961,0.997,1.0,1.0,0.996,1.0,1.0,1.0,0.962,0.993,0.992,0.724,0.979,0.99,0.959,1.0,0.99,0.996,0.999,0.946,0.923,0.995,0.992,0.0,0.945,0.79,1.0,0.814,1.0,1.0,0.998,0.997,0.999,0.627,1.0,0.973,0.992,0.985,0.947,0.874,0.975,1.0,0.993,0.263,1.0,0.995,0.999,1.0,0.996,1.0,0.159,0.975,0.022,0.998,0.991,0.205,1.0,0.973,1.0,0.433,0.76,0.999,1.0,1.0,0.001,0.98,0.09,0.932,0.999,0.996,0.867,0.11,1.0,0.992,0.757,0.838,1.0,0.002,0.932,0.926,1.0,1.0,0.992,0.266,0.015,0.966,0.932,0.994,0.945,0.52,0.206,0.996,0.015,0.01,0.427,0.995,0.879,0.056,1.0,1.0,0.568,0.998,1.0,0.963,0.928,1.0,0.995,0.001,1.0,0.975,0.99,0.103,0.999,0.989,0.38,0.232,0.996,1.0,0.999,0.999,0.281,1.0,0.999,0.0,0.998,0.922,1.0,0.8,0.109,1.0,1.0,1.0,0.98,0.999,1.0,0.002,1.0,0.836,0.001,1.0,0.99,1.0,1.0,1.0,0.998,0.783,1.0,0.991,0.267,0.981,0.997,1.0,0.999,0.093,1.0,0.131,0.405,0.265,0.017,1.0,1.0,0.99,0.998,0.999,0.998,0.562,1.0,0.994,0.964,0.439,0.951,0.493,0.999,0.999,0.97,1.0,0.998,1.0,0.986,0.998,0.036,1.0,0.991,0.989,0.976,0.993,1.0,0.992,0.59,1.0,0.999,0.937,0.332,0.025,0.997,0.999,0.13,1.0,0.895,0.991,0.997,0.746,0.146,1.0,0.991,1.0,0.019,0.999,0.947,0.993,0.019,0.97,0.659,0.917,0.98,0.998,0.998,0.999,0.998,0.939,0.999,0.878,0.956,0.003,0.885,0.968,0.021,0.041,0.861,0.15,0.001,0.321,0.996,1.0,0.969,0.999,1.0,0.149,0.97,0.99,0.985,0.995,0.999,0.132,0.97,0.996,0.556,0.998,1.0,0.999,0.836,1.0,0.996,0.995,0.999,0.574,0.995,0.063,1.0,0.999,0.991,1.0,0.997,1.0,0.951,0.993,0.914,0.432,0.996,0.605,0.98,0.995,0.981,0.269,0.281,0.01,0.855,0.445,0.946,0.03,0.998,0.034,0.857,0.999,0.824,1.0,0.369,0.379,0.961,0.952,0.993,0.348,0.682,0.637,0.412,0.334,0.202,0.999,1.0,0.88,0.04,0.886,0.009,0.998,0.992,0.178,0.15,1.0,0.752,0.065,0.991,0.998,0.973,0.869,0.978,0.99,0.006,0.996,0.026,1.0,0.936,0.978,0.973,0.997,1.0,0.007,0.989,0.981,0.989,0.993,1.0,0.975,0.953,0.012,0.999,0.985,0.991,0.012,0.993,0.993,0.998,0.033,0.998,0.998,0.303,0.015,0.466,0.941,0.999,0.997,0.988,0.986,0.782,0.997,0.973,0.945,0.897,1.0,0.0,0.999,0.798,1.0,0.875,0.747,0.015,0.998,0.927,0.984,0.056,0.468,0.998,0.968,0.998,0.051,1.0,0.001,0.004,0.999,0.423,0.045,0.952,0.115,0.988,0.838,0.724,0.056,0.842,0.979,0.975,0.748,0.83,0.22,0.547,0.974,0.984,0.998,0.999,0.39,0.999,0.997,0.998,0.913,1.0,0.922,0.26,0.996,0.991,1.0,0.963,0.058,0.002,0.096,0.754,0.012,0.986,0.971,0.004,0.791,0.873,0.407,0.994,0.999,0.961,1.0,0.85,0.956,0.003,0.867,0.999,0.999,0.204,0.935,0.999,1.0,1.0,0.985,0.96,0.999,0.832,0.18,0.997,0.04,1.0,0.967,0.949,0.993,0.999,0.91,1.0,0.027,0.287,0.896,0.979,1.0,0.999,0.686,0.977,0.1,0.969,0.984,1.0,0.983,0.002,1.0,0.982,1.0,1.0,0.192,0.961,0.999,0.179,0.79,0.901,0.312,0.998,0.961,0.994,0.797,0.867,0.353,0.006,0.807,0.006,0.971,0.993,0.988,1.0,0.965,0.004,0.859,0.957,1.0,0.348,1.0,0.63,0.837,0.867,0.999,0.387,0.495,0.721,0.832,0.986,1.0,0.763,0.018,0.177,0.991,1.0,0.198,0.999,0.786,0.029,0.816,0.887,0.997,0.4,0.996,0.596,0.569,0.863,1.0,0.068,0.899,0.91,0.404,0.002,1.0,0.028,0.312,0.303,0.965,0.99,1.0,1.0,0.293,1.0,0.978,0.9,0.455,0.458,0.04,0.954,0.985,0.527,0.94,1.0,0.625,0.057,0.523,0.915,0.076,0.067,0.999,0.029,0.785,0.946,0.999,0.991,0.996,0.992,0.904,0.963,0.64,0.677,0.841,0.072,0.592,0.864,0.054,0.829,1.0,0.959,0.801,0.083,0.994,0.502,0.998,0.907,0.991,0.938,0.298,0.579,0.969,0.817,0.06,0.254,0.939,0.785,0.998,0.113,0.97,0.818,0.986,0.607,1.0,0.759,0.755,0.895,0.898,0.981,0.116,0.973,0.04,0.963,0.86,0.17,0.301,0.711,0.999,0.969,0.808,0.999,0.997,0.771,0.519,0.006,0.948,0.018,0.963,1.0,0.911,0.976,0.155,0.7,0.02,0.931,0.219,0.234,0.454,0.39,1.0,0.102,0.035,0.018,0.706,0.934,0.558,0.399,0.732,0.95,0.024,0.727,0.995,0.338,1.0,0.999,0.434,0.809,0.944,0.015,0.77,0.014,0.999,0.977,0.044,0.967,0.937,0.322,0.478,0.999,0.725,0.653,0.999,0.005,0.081,0.984,0.999,0.973,0.955,0.933,0.995,1.0,0.022,0.964,0.201,0.676,0.112,0.981,0.998,0.607,0.117,0.797,1.0,0.292,0.755,0.71,0.815,1.0,1.0,0.674,1.0,0.99,0.999,0.927,0.999,0.171,0.889,0.042,0.998,0.046,0.976,0.673,0.999,0.382,0.927,0.974,0.413,1.0,0.847,0.836,1.0,1.0,0.964,0.96,1.0,0.061,0.97,0.999,0.008,1.0,0.603,0.089,0.984,0.263,0.735,1.0,0.992,0.999,0.997,0.069,0.26,0.002,0.963,0.396,0.228,0.382,0.996,0.048,0.049,0.999,0.233,0.981,0.913,1.0,0.995,0.69,0.337,0.984,0.0,0.958,0.906,1.0,0.909,0.169,0.811,0.794,0.999,0.988,0.997,0.977,0.016,0.954,0.002,0.834,0.994,0.723,0.999,0.123,0.041,0.999,0.022,0.995,0.309,0.997,0.995,1.0,0.994,0.61,1.0,0.092,0.993,0.828,0.01,0.999,0.986,1.0,0.873,0.864,0.988,0.006,0.107,0.492,0.198,0.977,0.065,0.082,0.997,0.985,0.114,0.999,1.0,0.557,0.951,0.995,0.037,0.975,0.956,0.999,0.016,1.0,0.013,0.757,0.69,0.034,0.953,0.538,0.027,0.273,0.392,0.844,0.949,0.991,0.995,0.999,0.965,0.972,0.999,0.076,0.124,0.563,0.999,0.997,0.913,0.982,0.999,0.969,0.995,0.998,0.997,0.393,0.97,0.919,0.811,0.991,0.011,0.082,1.0,0.987,0.442,0.992,0.91,1.0,0.993,0.998,0.992,0.974,0.999,0.991,0.998,0.39,0.093,0.078,0.007,0.898,0.97,0.975,0.011,0.975,0.988,0.391,0.256,0.926,0.203,0.993,0.742,0.967,0.85,0.998,0.943,0.302,0.999,0.999,0.796,0.997,0.124,0.799,0.949,0.839,0.427,0.995,0.984,0.187,0.999,0.998,0.976,0.666,0.993,0.742,0.901,0.998,0.555,1.0,0.998,0.795,0.931,0.999,0.843,0.958,0.848,0.929,0.942,0.685,0.002,0.049,0.373,0.999,0.97,0.077,0.991,0.956,0.089,0.242,0.606,0.64,0.944,0.049,0.259,0.41,0.991,0.993,0.19,0.931,0.491,0.885,0.218,0.084,0.238,0.639,0.936,0.891,0.862,0.93,0.999,0.121,0.824,0.293,0.971,0.443,0.77,0.909,0.998,0.777,0.172,0.706,0.173,0.115,0.998,0.99,0.5,0.851,0.923,0.978,0.757,0.871,0.883,0.857,0.97,0.907,0.995,0.99,0.74,0.944,0.934,0.903,0.998,0.772,0.806,0.917,0.897,0.883,0.817,0.268,0.2,0.641,0.997,0.438,0.877,0.829,0.89,0.918,0.7,0.794,0.948,0.945,0.818,0.946,0.983,0.983,0.983,0.884,0.87,0.087,0.321,0.765,0.99,0.99,0.288,0.124,0.998,0.437,0.999,0.631,0.997,0.997,0.046,0.626,0.989,0.935,0.032,0.994,0.969,0.994,0.988,0.751,0.029,0.92,0.794,0.943,0.96,0.992,0.919,0.998,0.79,0.015,0.676,0.072,0.993,0.689,0.741,0.94,0.706,0.841,0.988,0.994,0.791,0.913,0.99,0.525,0.91,0.473,0.945,0.97,0.994,0.988,0.936,0.998,0.995,0.207,0.7,0.538,0.969,0.227,0.057,0.997,0.984,0.999,0.996,1.0,0.741,0.98,0.978,0.986,0.916,0.956,1.0,0.915,0.912,0.911,0.97,0.388,0.847,0.978,0.574,0.11,0.436,0.995,0.918,0.662,0.997,0.958,0.998,0.915,0.998,0.881,0.556,0.898,1.0,0.999,0.962,0.926,0.985,0.903,0.993,0.969,0.921,0.98,0.93,0.26,0.96,0.955,0.958,0.058,0.234,0.986,0.885,0.85,0.96,0.923,0.99,0.994,0.71,0.692,0.876,0.994,0.547,0.968,0.335,0.976,0.964,0.999,0.114,0.869,0.917,0.633,0.956,0.983,0.194,0.985,0.992,0.918,0.971,0.87,0.954,0.955,0.995,0.925,0.849,0.869,0.972,0.939,0.985,0.57,0.668,0.992,0.995,0.762,0.875,0.53,0.995,0.983,0.001,0.896,0.674,0.384,0.993,0.977,0.968,0.118,0.596,0.1,0.968,0.985,0.295,0.618,0.65,0.843,0.915,0.984,0.842,0.585,0.802,0.869,0.506,0.922,0.981,0.191,0.131,0.973,0.895,0.97,0.997,0.984,0.965,0.989,0.747,0.977,0.552,0.955,0.924,0.722,0.983,0.861,0.952,0.824,0.994,0.978,0.953,0.091,0.89,0.94,0.973,0.945,0.878,0.97,0.987,0.954,0.952,0.997,0.776,0.852,0.994,0.87,0.849,0.982,0.531,0.479,0.931,0.931,0.865,0.995,0.963,0.915,0.982,0.991,0.616,0.118,0.984,0.665,0.95,0.617,0.999,0.394,0.786,0.736,0.531,0.959,0.99,0.977,0.772,0.959,0.896,0.978,0.979,0.805,0.998,0.946,0.609,0.221,0.856,0.952,0.987,0.426,0.207,0.981,0.982,0.793,0.409,0.994,0.982,0.248,0.989,0.95,0.926,0.202,0.868,0.963,0.983,0.956,0.913,0.568,0.981,0.956,0.927,0.846,0.976,0.605,0.585,0.166,0.469,0.948,0.979,0.165,0.95,0.938,0.844,0.778,0.994,0.991,0.426,0.824,0.952,0.877,0.461,0.845,0.987,0.261,0.773,0.174,0.521,0.228,0.977,0.289,0.883,0.105,0.99,0.928,0.95,0.571,0.963,0.859,0.983,0.251,0.163,0.885,0.452,0.444,0.916,0.965,0.928,0.853,0.994,0.99,0.886,0.392,0.988,0.861,0.961,0.947,0.891,0.762,0.974,0.911,0.874,0.605,0.661,0.956,0.92,0.461,0.983,0.875,0.102,0.933,0.987,0.956,0.992,0.921,0.988,0.79,0.711,0.782,0.901,0.902,0.916,0.968,0.979,0.975,0.65,0.903,0.858,0.866,0.972,0.101,0.116,0.604,0.93,0.95,0.998,0.724,0.349,0.88,0.947,0.932,0.982,0.989,0.973,0.946,0.876,0.918,0.734,0.988,0.526,0.983,0.955,0.908,0.867,0.193,0.526,0.552,0.851,0.973,0.653,0.983,0.91,0.981,0.884,0.411,0.687,0.752,0.062,0.251,0.107,0.069,0.463,0.619,0.625,0.122,0.584,0.973,0.968,0.954,0.786,0.987,0.966,0.968,0.949,0.998,0.377,0.197,0.928,0.54,0.463,0.977,0.859,0.864,0.093,0.787,0.548,0.938,0.907,0.866,0.93,0.907,0.572,0.741,0.719,0.975,0.954,0.786,0.952,0.853,0.439,0.44,0.916,0.974,0.937,0.998,0.95,0.889,0.901,0.925,0.916,0.757,0.679,0.636,0.776,0.977,0.786,0.996,0.586,0.942,0.99,0.907,0.979,0.438,0.147,0.274,0.995,0.987,0.829,0.885,0.897,0.948,0.951,0.857,0.903,0.722,0.146,0.088,0.06,0.962,0.432,0.837,0.992,0.985,0.965,0.076,0.346,0.996,0.987,0.968,0.943,0.57,0.513,0.983,0.947,0.035,0.774,0.97,0.157,0.855,0.977,0.414,0.991,0.549,0.242,0.821,0.988,0.844,0.963,0.898,0.909,0.98,0.685,0.757,0.893,0.84,0.022,0.764,0.937,0.945,0.551,0.991,0.89,0.705,0.94,0.596,0.876,0.597,0.979,0.942,0.972,0.359,0.874,0.07,0.921,0.853,0.971,0.981,0.602,0.963,0.575,0.08,0.162,0.211,0.17,0.993,0.857,0.941,0.463,0.796,0.525,0.273,0.758,0.969,0.914'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub23 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub23['pred'] = sub23['pred'].apply(lambda x: round(x,3))\n",
    "sub23_txt = ''\n",
    "for prob in list(sub23['pred'].values):\n",
    "    sub23_txt = sub23_txt+','+str(prob)\n",
    "sub23_txt = sub23_txt[1:]\n",
    "\n",
    "sub23_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b2b8f",
   "metadata": {},
   "source": [
    "### Submission 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98724b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_tr2, y_tr2 = oversample.fit_resample(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307c4b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=20,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5)\n",
    "rf.fit(X_tr2,y_tr2)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cc2fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  1.0\n",
      "Val ROC:  0.8582983602074197\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "463921de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17232</td>\n",
       "      <td>0.258398</td>\n",
       "      <td>0.430718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5              0.0              0.0             0.0        0.17232   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.258398      0.430718  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c39bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.999,0.998,0.984,1.0,0.997,0.999,0.702,0.013,0.999,0.997,0.011,0.009,1.0,1.0,0.994,1.0,0.147,0.011,0.995,0.998,1.0,0.998,1.0,1.0,1.0,0.998,1.0,1.0,0.999,1.0,1.0,0.996,0.999,0.991,1.0,0.999,0.688,0.974,0.999,0.976,0.974,0.998,1.0,0.999,1.0,0.998,1.0,0.884,0.034,0.521,0.019,1.0,0.998,0.001,0.025,0.001,1.0,0.996,0.997,1.0,1.0,0.957,0.583,0.998,1.0,0.942,1.0,0.997,0.999,1.0,1.0,0.979,1.0,0.999,0.984,0.992,0.998,1.0,0.999,0.999,0.098,0.989,1.0,0.019,1.0,0.993,0.93,1.0,0.998,0.995,1.0,0.999,0.884,1.0,0.031,0.999,0.999,0.963,0.641,0.999,0.996,0.999,1.0,1.0,0.982,0.005,0.195,0.726,0.999,0.838,0.996,1.0,0.996,1.0,0.865,0.999,0.788,0.999,0.993,0.999,1.0,1.0,0.897,0.937,0.998,0.999,1.0,1.0,0.017,0.001,0.999,0.988,0.012,0.998,0.999,1.0,0.61,1.0,0.992,0.998,0.069,0.557,0.997,0.998,0.978,0.999,0.972,0.999,0.189,1.0,1.0,1.0,1.0,1.0,0.873,0.999,0.99,0.993,1.0,0.951,1.0,0.995,0.999,0.977,1.0,0.998,0.968,0.995,1.0,0.206,1.0,0.43,0.788,1.0,0.999,0.985,1.0,0.998,0.824,0.997,0.094,0.014,0.989,0.029,0.595,1.0,1.0,1.0,1.0,0.998,1.0,0.05,0.899,0.995,0.973,0.002,0.997,0.33,1.0,0.913,0.999,1.0,1.0,1.0,0.957,0.096,0.635,0.288,0.243,0.006,1.0,1.0,0.196,1.0,1.0,0.764,1.0,0.999,0.964,1.0,0.985,0.999,0.051,1.0,1.0,0.147,0.003,1.0,0.174,0.239,0.031,0.999,0.886,1.0,0.117,1.0,1.0,0.988,1.0,1.0,0.007,1.0,0.999,0.999,1.0,0.01,0.925,0.006,0.999,0.995,0.994,0.001,1.0,1.0,0.021,0.966,1.0,1.0,0.15,0.006,1.0,0.918,0.05,0.999,1.0,0.999,0.98,1.0,0.999,0.03,0.998,0.998,1.0,0.5,0.998,0.984,1.0,1.0,1.0,0.998,1.0,1.0,0.012,1.0,0.957,0.148,0.999,0.996,0.816,0.999,0.007,0.017,0.03,1.0,0.931,0.003,0.999,0.034,0.993,1.0,1.0,1.0,0.985,0.987,1.0,1.0,0.999,1.0,1.0,0.999,1.0,0.994,0.002,1.0,0.997,0.004,1.0,0.603,0.981,0.415,1.0,1.0,0.01,0.939,0.999,0.981,0.004,0.415,0.986,0.327,0.038,1.0,0.077,1.0,0.994,0.979,1.0,1.0,0.001,0.998,0.999,0.998,0.863,0.999,0.999,0.997,0.989,0.01,0.999,1.0,0.059,0.056,1.0,0.994,0.055,1.0,0.974,1.0,1.0,1.0,1.0,1.0,0.111,0.946,0.992,0.987,0.101,1.0,1.0,0.027,1.0,0.999,0.997,1.0,0.998,0.997,0.998,1.0,1.0,0.985,0.973,0.997,0.004,0.992,0.998,1.0,0.002,1.0,1.0,0.999,0.387,1.0,0.956,1.0,0.992,1.0,1.0,1.0,0.999,0.999,0.998,0.999,0.999,0.995,1.0,0.991,0.001,0.009,0.994,0.01,0.032,0.174,0.996,0.986,0.982,0.005,0.999,0.001,0.387,0.995,1.0,0.998,0.993,1.0,0.157,0.448,0.97,0.074,1.0,0.995,0.997,0.001,0.006,1.0,0.753,1.0,1.0,1.0,0.008,1.0,0.635,0.998,1.0,0.489,1.0,0.994,0.415,1.0,0.015,0.167,1.0,0.905,0.052,1.0,0.971,1.0,1.0,1.0,0.993,0.999,1.0,0.182,0.998,0.999,0.989,0.998,0.003,0.851,0.026,0.993,0.886,0.264,0.012,0.999,0.983,0.998,0.995,1.0,0.042,0.994,0.005,0.995,0.999,0.002,0.004,0.008,0.999,1.0,1.0,0.003,0.001,0.977,0.194,0.986,0.827,1.0,1.0,0.7,0.872,0.056,0.999,0.96,1.0,0.996,0.001,1.0,0.262,1.0,1.0,0.849,0.996,0.956,0.996,0.887,0.057,0.119,0.017,0.023,0.978,0.005,0.951,0.306,0.008,0.245,1.0,0.999,0.999,1.0,1.0,0.997,0.011,1.0,0.554,0.007,0.999,0.987,0.033,0.011,0.999,0.761,0.175,1.0,0.084,0.095,1.0,0.104,0.023,0.998,0.19,0.412,1.0,0.999,0.999,0.999,0.995,0.999,0.005,0.99,0.85,1.0,0.999,0.004,0.941,1.0,0.009,1.0,0.999,0.014,0.276,0.998,1.0,0.967,0.002,0.001,1.0,0.999,0.684,0.995,0.999,1.0,1.0,0.049,1.0,0.999,0.82,1.0,0.999,0.789,0.998,0.993,0.995,1.0,0.995,0.27,0.999,0.982,1.0,0.045,0.007,1.0,0.991,1.0,0.884,1.0,0.005,0.959,0.515,1.0,0.996,0.965,1.0,0.999,0.005,0.335,0.048,0.999,0.968,0.021,0.009,0.006,1.0,0.996,1.0,1.0,1.0,0.5,0.021,1.0,1.0,1.0,0.94,0.998,0.967,0.002,1.0,0.219,0.315,0.003,0.999,1.0,1.0,1.0,0.99,0.08,1.0,0.077,0.013,0.955,0.998,0.999,1.0,0.687,0.003,0.829,0.995,0.999,0.999,0.009,1.0,1.0,0.861,0.965,0.047,0.121,0.091,0.105,1.0,1.0,1.0,0.864,0.584,0.998,0.949,0.963,0.98,0.992,1.0,0.999,0.023,0.005,0.512,0.076,0.863,0.999,0.995,0.975,0.008,0.999,0.999,0.803,0.02,0.107,0.948,0.953,0.998,0.992,1.0,1.0,0.01,0.999,1.0,0.999,0.15,1.0,0.998,1.0,1.0,0.457,1.0,0.217,1.0,0.098,0.992,0.001,0.996,0.988,0.998,1.0,0.978,0.219,0.133,0.863,1.0,0.178,1.0,0.987,0.138,1.0,0.033,0.087,1.0,0.953,0.996,1.0,0.902,0.998,1.0,0.998,1.0,0.999,0.999,0.997,0.981,1.0,1.0,0.536,0.814,0.996,0.074,0.994,1.0,0.493,0.604,1.0,0.036,0.999,1.0,0.067,0.038,1.0,1.0,1.0,0.001,0.971,1.0,0.999,0.998,0.054,0.999,0.714,0.001,1.0,1.0,0.028,0.057,0.098,0.045,0.991,1.0,0.996,1.0,0.477,0.999,0.981,1.0,0.344,0.328,0.999,1.0,0.022,0.004,0.998,0.801,0.844,0.863,0.352,1.0,0.005,0.999,0.648,0.004,0.002,0.996,0.002,0.867,1.0,0.957,0.971,1.0,1.0,1.0,0.149,0.999,0.971,0.506,0.999,1.0,1.0,0.998,1.0,1.0,1.0,0.707,0.847,0.993,0.444,0.999,0.976,0.345,1.0,0.984,0.999,1.0,0.285,0.12,0.999,0.435,0.001,0.991,0.014,1.0,0.982,1.0,1.0,0.99,1.0,1.0,0.936,1.0,0.982,0.997,0.974,0.993,0.158,0.011,1.0,0.966,0.261,1.0,0.996,1.0,1.0,0.055,1.0,0.264,0.838,0.034,0.972,0.991,0.127,1.0,0.997,1.0,0.449,0.389,1.0,1.0,1.0,0.003,0.17,0.801,0.016,1.0,0.998,0.913,0.181,1.0,0.628,0.39,0.067,1.0,0.14,0.008,0.998,1.0,1.0,0.995,0.711,0.014,0.783,0.123,0.981,0.915,0.755,0.3,0.997,0.026,0.01,0.616,0.986,0.667,0.244,1.0,1.0,0.012,0.994,1.0,0.998,0.815,1.0,0.99,0.008,1.0,0.995,0.992,0.094,0.999,0.254,0.569,0.134,0.997,1.0,1.0,1.0,0.087,0.998,1.0,0.006,1.0,0.974,1.0,0.03,0.084,1.0,1.0,1.0,0.976,1.0,0.999,0.004,1.0,0.98,0.001,1.0,0.973,1.0,1.0,1.0,1.0,0.81,1.0,0.596,0.093,0.999,1.0,1.0,1.0,0.124,1.0,0.152,0.492,0.031,0.033,1.0,1.0,0.996,1.0,1.0,0.995,0.076,1.0,0.991,0.972,0.795,0.996,0.117,0.998,0.986,0.982,1.0,1.0,1.0,0.999,1.0,0.31,1.0,0.913,0.99,0.999,0.999,1.0,0.994,0.292,1.0,0.999,0.988,0.461,0.355,0.999,0.998,0.139,1.0,0.981,0.998,0.993,0.003,0.238,1.0,0.154,1.0,0.007,0.998,0.908,0.999,0.0,0.948,0.016,0.32,0.188,0.996,1.0,0.998,1.0,0.788,1.0,0.803,0.906,0.007,0.949,0.985,0.004,0.02,0.035,0.61,0.004,0.015,0.985,1.0,0.953,0.998,1.0,0.526,0.988,0.997,0.995,0.998,0.999,0.03,0.876,0.833,0.269,0.078,1.0,0.997,0.925,1.0,0.999,1.0,0.988,0.692,1.0,0.041,1.0,1.0,0.997,0.999,0.999,1.0,0.354,0.996,0.039,0.905,0.247,0.154,0.2,0.995,0.511,0.112,0.975,0.0,0.024,0.014,0.996,0.02,0.155,0.8,0.934,1.0,0.865,1.0,0.631,0.017,0.725,0.563,0.997,0.299,0.248,0.85,0.781,0.46,0.304,1.0,1.0,0.965,0.068,0.837,0.037,0.996,0.167,0.435,0.013,1.0,0.582,0.154,0.975,1.0,0.014,0.92,0.999,0.796,0.012,0.998,0.028,1.0,0.403,0.996,0.992,0.997,1.0,0.005,0.356,0.966,0.857,0.16,1.0,0.279,0.216,0.018,0.995,0.997,0.952,0.124,0.994,0.999,0.999,0.438,0.998,1.0,0.008,0.007,0.127,0.988,0.999,0.999,0.997,0.998,0.938,0.998,0.998,0.981,0.816,1.0,0.003,0.996,0.815,1.0,0.009,0.968,0.064,1.0,0.999,0.461,0.022,0.532,1.0,0.986,0.991,0.001,1.0,0.003,0.002,1.0,0.356,0.034,0.965,0.006,0.999,0.917,0.998,0.698,0.995,0.799,0.996,0.08,0.93,0.412,0.949,0.983,0.978,0.996,1.0,0.006,1.0,0.999,0.998,0.919,1.0,0.981,0.703,0.997,1.0,1.0,0.944,0.078,0.076,0.131,0.304,0.019,0.937,0.994,0.004,0.016,0.999,0.408,0.999,1.0,0.23,1.0,0.104,0.239,0.003,0.019,1.0,1.0,0.012,0.984,0.999,1.0,0.999,0.989,0.987,1.0,0.989,0.126,0.999,0.21,1.0,0.282,0.968,0.711,1.0,0.949,1.0,0.049,0.043,0.001,0.341,1.0,1.0,0.255,0.999,0.279,0.989,0.978,0.999,0.941,0.013,1.0,0.474,1.0,1.0,0.014,0.986,1.0,0.267,0.848,0.872,0.414,0.988,0.998,0.298,0.116,0.267,0.021,0.003,0.707,0.096,0.857,0.999,0.906,1.0,0.923,0.001,0.998,0.014,1.0,0.073,1.0,0.0,0.896,0.961,0.998,0.002,0.055,0.97,0.149,0.992,1.0,0.4,0.002,0.693,0.999,1.0,0.023,0.998,0.013,0.004,0.454,0.325,1.0,0.266,0.997,0.255,0.094,0.777,1.0,0.057,0.914,0.849,0.007,0.005,1.0,0.02,0.988,0.236,0.948,1.0,1.0,1.0,0.083,1.0,0.983,0.744,0.985,0.26,0.062,0.84,0.987,0.222,0.948,1.0,0.241,0.124,0.284,0.981,0.266,0.356,1.0,0.008,0.576,0.999,0.999,0.524,1.0,0.938,0.194,0.122,0.001,0.906,0.102,0.165,0.044,0.077,0.005,0.279,1.0,0.052,0.369,0.091,0.609,0.987,0.994,0.939,0.21,0.422,0.251,0.684,0.022,0.98,0.038,0.988,0.039,0.153,0.998,0.155,0.891,0.643,0.15,0.036,1.0,0.4,0.974,0.491,0.507,0.348,0.005,0.936,0.009,0.887,0.556,0.091,0.697,0.111,0.996,0.99,0.333,1.0,0.995,0.188,0.436,0.006,0.597,0.767,0.994,1.0,0.973,0.705,0.806,0.713,0.007,0.998,0.99,0.105,0.307,0.02,1.0,0.926,0.016,0.065,0.825,0.998,0.245,0.117,0.836,0.912,0.089,0.682,1.0,0.032,0.998,0.994,0.338,0.028,0.985,0.041,0.769,0.206,0.999,0.993,0.277,0.672,0.334,0.424,0.983,1.0,0.024,0.376,0.996,0.383,0.499,0.999,0.999,0.998,0.916,0.883,0.992,1.0,0.025,0.992,0.938,0.985,0.036,0.999,0.994,0.603,0.117,0.977,1.0,0.302,0.934,0.172,1.0,1.0,1.0,0.686,1.0,0.989,1.0,0.997,1.0,0.352,0.732,0.004,1.0,0.056,1.0,0.974,1.0,0.925,0.931,0.773,0.278,1.0,0.998,0.975,1.0,1.0,0.983,0.064,1.0,0.058,0.997,0.999,0.008,1.0,0.033,0.013,0.996,0.055,0.978,1.0,0.895,0.977,0.995,0.152,0.06,0.045,0.92,0.662,0.023,0.036,0.997,0.172,0.257,1.0,0.232,0.995,0.988,1.0,0.999,0.979,0.029,0.999,0.0,0.901,0.979,1.0,0.994,0.038,0.938,0.97,1.0,0.995,0.999,0.995,0.0,0.995,0.002,0.365,0.999,0.785,0.998,0.071,0.977,0.986,0.002,0.998,0.129,0.993,0.999,1.0,0.948,0.72,1.0,0.064,0.998,0.602,0.052,0.99,0.999,1.0,0.989,0.999,1.0,0.001,0.068,0.044,0.013,0.996,0.174,0.015,0.996,0.999,0.116,1.0,1.0,0.007,0.007,0.999,0.472,0.989,0.451,1.0,0.01,1.0,0.004,0.005,0.006,0.24,0.974,0.759,0.003,0.016,0.475,0.074,0.995,0.991,0.997,1.0,0.636,0.99,0.999,0.013,0.107,0.022,0.999,0.993,0.959,0.999,0.998,0.993,1.0,1.0,0.999,0.014,1.0,0.999,0.918,0.999,0.008,0.169,1.0,1.0,0.088,0.991,0.934,1.0,0.997,0.999,0.993,0.998,0.999,1.0,0.998,0.739,0.983,0.818,0.297,0.758,0.825,0.989,0.005,0.999,1.0,0.025,0.126,0.999,0.488,1.0,0.997,0.996,0.64,1.0,0.188,0.004,1.0,0.988,0.015,0.999,0.002,0.254,0.958,0.126,0.777,0.998,0.999,0.208,1.0,1.0,0.997,0.962,0.989,0.034,1.0,1.0,0.9,1.0,1.0,0.995,0.931,0.992,0.932,0.999,0.846,0.972,0.12,0.176,0.0,0.065,0.773,1.0,0.999,0.001,0.962,0.753,0.016,0.98,0.953,0.387,0.07,0.031,0.075,0.837,1.0,0.998,0.014,0.996,0.639,0.514,0.406,0.002,0.167,0.027,0.963,0.833,0.486,0.997,1.0,0.036,0.021,0.17,0.852,0.024,0.672,0.584,0.999,0.899,0.011,0.04,0.051,0.034,0.999,0.997,0.269,0.33,0.998,0.997,0.797,0.497,0.994,0.772,0.999,0.997,1.0,0.999,0.391,0.897,1.0,0.356,0.998,0.012,0.993,0.938,0.989,0.989,0.063,0.001,0.001,0.317,1.0,0.029,0.033,0.955,0.788,0.996,0.037,0.072,0.864,0.06,0.002,0.976,0.999,0.711,0.999,0.438,0.432,0.019,0.683,0.994,0.957,0.957,0.586,0.907,0.834,0.743,0.999,0.289,0.919,0.983,0.017,0.938,0.622,0.716,0.033,0.84,0.098,0.997,0.942,0.738,0.002,0.632,0.725,0.997,0.065,0.953,0.968,0.827,0.935,0.001,0.997,0.78,0.95,0.034,0.795,0.032,0.433,0.041,0.989,0.905,0.93,0.846,0.071,0.989,0.495,0.119,0.87,0.947,0.996,0.996,0.889,0.998,0.65,0.002,0.326,0.029,0.981,0.659,0.008,0.924,1.0,0.99,0.999,0.999,0.84,1.0,0.962,0.432,0.994,0.993,1.0,1.0,0.621,0.994,0.985,0.996,0.997,0.546,0.964,0.131,0.701,0.991,0.868,0.938,0.753,0.117,1.0,0.996,0.997,0.408,0.987,0.956,1.0,0.996,0.999,0.962,0.893,0.034,0.897,0.128,0.141,0.998,0.183,0.412,0.24,0.998,0.911,0.011,0.999,0.513,0.981,0.993,0.31,0.74,0.977,0.096,0.919,0.066,0.961,0.992,0.754,0.403,0.226,0.999,0.023,0.922,0.149,0.036,0.983,0.023,0.861,0.317,0.005,0.997,0.995,0.044,0.804,0.964,0.004,0.958,0.999,0.993,0.843,0.942,0.999,0.998,1.0,0.031,0.87,0.985,0.753,0.865,0.301,0.005,0.577,0.65,0.009,0.155,0.003,0.026,0.999,0.993,0.011,0.14,0.175,0.275,0.61,0.906,0.003,0.024,0.93,0.256,0.918,0.997,0.96,0.4,0.068,0.419,0.996,0.564,0.999,0.873,0.709,0.445,0.976,0.989,0.373,0.979,0.049,0.931,0.927,0.211,0.022,0.035,0.025,0.985,0.848,0.967,0.994,0.738,0.878,0.999,0.964,0.963,0.999,0.832,0.237,0.576,0.005,0.685,0.958,0.975,0.956,0.349,0.015,0.347,0.141,0.888,0.833,0.492,0.005,0.45,0.402,0.971,0.015,0.999,0.727,0.642,0.911,0.999,0.954,0.046,0.987,0.978,0.747,0.999,0.959,0.026,0.033,0.996,0.315,0.358,0.129,0.715,0.283,0.018,0.127,0.615,0.953,0.901,1.0,0.998,0.003,0.833,0.828,0.952,0.659,0.068,0.442,0.818,0.811,0.83,0.629,0.752,0.986,0.095,0.645,0.628,0.198,0.008,0.265,0.053,0.193,0.98,0.454,0.267,1.0,0.565,0.052,0.959,0.157,0.997,0.841,0.017,0.449,0.619,0.994,0.911,0.007,0.071,0.996,0.997,0.713,0.887,0.017,0.008,0.006,0.928,0.39,0.447,0.983,0.558,0.948,0.02,0.062,0.009,0.732,0.954,0.999,0.003,0.991,0.038,0.999,0.969,0.989,0.998,0.124,0.043,0.0,0.52,0.651,0.964,0.703,0.423,0.8,0.961,0.751,0.674,0.987,0.99,0.878,0.245,0.035,0.952,0.814,0.847,0.999,0.103,0.749,0.977,0.587,0.926,0.976,0.012,0.703,0.015,0.056,0.964,1.0,0.127,0.96,0.139,0.99,0.835,0.615,0.006,0.107,0.191,0.124,0.112,0.703,0.849,0.375,0.71,0.874,0.849,0.97,0.011,0.964,0.997,0.853,0.411,0.135,0.304,0.878,0.086,0.991,0.012,0.311,0.951,0.157,0.613,0.806,0.992,0.31,0.909,0.041,0.592,0.889,0.474,0.049,0.967,0.025,0.154,0.094,0.006,0.959,0.899,0.071,0.725,0.999,0.019,0.997,0.9,0.999,0.036,0.951,0.018,0.775,0.994,0.921,0.016,0.999,0.943,0.53,0.956,0.33,0.646,0.979,0.707,0.007,0.057,0.259,0.123,0.112,0.995,0.96,0.945,0.016,0.049,0.001,0.163,0.083,0.069,0.057,0.149,0.951,0.974,0.977,0.152,0.24,0.823,0.981,0.909,0.505,0.928,0.017,0.982,0.73,0.029,0.876,0.835,0.547,0.982,0.393,0.166,0.999,0.135,0.005,0.824,0.991,0.866,0.62,0.997,0.172,0.984,0.669,0.638,0.217,0.998,0.963,0.993,0.938,0.735,0.046,0.841,0.491,0.575,0.261,0.334,0.157,0.781,0.805,0.002,0.987,0.02,0.01,0.089,0.959,0.885,0.843,0.111,0.017,0.538,0.996,0.618,0.074,0.478,0.414,0.75,0.994,0.999,0.006,0.596,0.999,0.002,0.014,0.403,0.145,0.995,0.971,0.321,0.668,0.997,0.165,0.408,0.873,0.883,0.568,0.169,0.805,0.258,0.292,0.994,0.015,0.023,0.369,0.993,0.765,0.623,0.982,0.918,0.965,0.481,0.093,0.912,0.893,0.098,0.0,0.874,0.969,0.994,0.993,0.443,0.133,0.434,1.0,0.011,0.084,0.055,0.009,0.648,0.565,0.43,0.979,0.001,0.502,0.195,0.473,0.974,0.416'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub24 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub24['pred'] = sub24['pred'].apply(lambda x: round(x,3))\n",
    "sub24_txt = ''\n",
    "for prob in list(sub24['pred'].values):\n",
    "    sub24_txt = sub24_txt+','+str(prob)\n",
    "sub24_txt = sub24_txt[1:]\n",
    "\n",
    "sub24_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dd07d",
   "metadata": {},
   "source": [
    "### Submission 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382c0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = pd.read_csv('variable_list.csv')\n",
    "var_list.columns=['index','feature']\n",
    "var_list = [x[2:] for x in list(var_list['feature'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c251aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sellingprice_median_basket',\n",
       " 'all_favorite_Level3_Category_Id_cnt',\n",
       " 'sellingprice_min_order',\n",
       " 'sellingprice_median_search',\n",
       " 'weekly_visit_contentid_cnt',\n",
       " 'weekly_favorite_secs_btw_first_last',\n",
       " 'sellingprice_mean_ALTINYILDIZ CLASSICS',\n",
       " 'sellingprice_sum_Addax',\n",
       " 'sellingprice_median_Apple',\n",
       " 'sellingprice_mean_Aqua Di Polo 1987',\n",
       " 'sellingprice_max_Avva',\n",
       " 'sellingprice_max_Bambi',\n",
       " 'sellingprice_sum_Bershka',\n",
       " 'sellingprice_mean_Cool & Sexy',\n",
       " 'sellingprice_max_DeFacto',\n",
       " 'sellingprice_max_Dilvin',\n",
       " 'sellingprice_max_Elle Shoes',\n",
       " 'sellingprice_sum_English Home',\n",
       " 'sellingprice_min_HUMMEL',\n",
       " 'sellingprice_sum_Happiness Ä°st.',\n",
       " 'sellingprice_sum_Karaca',\n",
       " 'sellingprice_sum_Kitchen Life',\n",
       " 'sellingprice_mean_Koton',\n",
       " 'sellingprice_mean_Koton Kids',\n",
       " 'sellingprice_sum_LC Waikiki',\n",
       " 'sellingprice_sum_MANGO Woman',\n",
       " 'sellingprice_min_MUGGO',\n",
       " 'sellingprice_sum_Madame Coco',\n",
       " 'sellingprice_count_Mavi',\n",
       " 'sellingprice_min_Nike',\n",
       " 'sellingprice_sum_Olalook',\n",
       " 'sellingprice_mean_Oysho',\n",
       " 'sellingprice_sum_Penti',\n",
       " 'sellingprice_count_Pierre Cardin',\n",
       " 'sellingprice_min_Pull & Bear',\n",
       " 'sellingprice_mean_Puma',\n",
       " 'sellingprice_mean_SOHO',\n",
       " 'sellingprice_sum_Sateen',\n",
       " 'sellingprice_sum_Stradivarius',\n",
       " 'sellingprice_min_TRENDYOL MAN',\n",
       " 'sellingprice_sum_PL Woman',\n",
       " 'sellingprice_mean_Terapi Men',\n",
       " 'sellingprice_mean_Tonny Black',\n",
       " 'sellingprice_mean_Trend AlaÃ§atÄ± Stili',\n",
       " 'sellingprice_count_U.S. Polo Assn.',\n",
       " 'sellingprice_mean_XENA',\n",
       " 'sellingprice_max_Xiaomi',\n",
       " 'sellingprice_mean_Yaya  by HotiÃ§',\n",
       " 'sellingprice_min_adidas',\n",
       " 'sellingprice_mean_armonika',\n",
       " 'sellingprice_count_other_brand_name',\n",
       " 'sellingprice_sum_DeFacto',\n",
       " 'sellingprice_sum_Koton',\n",
       " 'sellingprice_max_Mavi',\n",
       " 'sellingprice_min_Pierre Cardin',\n",
       " 'sellingprice_mean_U.S. Polo Assn.',\n",
       " 'sellingprice_min_Yaya  by HotiÃ§',\n",
       " 'sellingprice_count_Branded Shoes A',\n",
       " 'sellingprice_count_Branded Shoes B',\n",
       " 'sellingprice_max_Cep Telefonu & AksesuarlarÄ±',\n",
       " 'sellingprice_sum_Cilt BakÄ±m_x',\n",
       " 'sellingprice_max_Erkek A',\n",
       " 'sellingprice_max_Erkek B',\n",
       " 'sellingprice_mean_Ev Dekorasyon_x',\n",
       " 'sellingprice_sum_Ev Giyim_x',\n",
       " 'sellingprice_sum_Ev Tekstil',\n",
       " 'sellingprice_max_GAS Denim',\n",
       " 'sellingprice_count_Hobi',\n",
       " 'sellingprice_count_KadÄ±n A',\n",
       " 'sellingprice_sum_KadÄ±n B',\n",
       " 'sellingprice_count_Kids & Baby Fashion',\n",
       " 'sellingprice_median_Makyaj_x',\n",
       " 'sellingprice_mean_Salon &Mutfak Mobilya',\n",
       " 'sellingprice_count_SaÄŸlÄ±k_y',\n",
       " 'sellingprice_count_Sofra & Mutfak_x',\n",
       " 'sellingprice_mean_Sportswear',\n",
       " 'sellingprice_median_TakÄ± & MÃ¼cevher',\n",
       " 'sellingprice_median_other_businessunit',\n",
       " 'sellingprice_min_Ã‡anta & Valiz',\n",
       " 'sellingprice_min_Ä°Ã§ Giyim_x',\n",
       " 'sellingprice_median_Branded Shoes A',\n",
       " 'sellingprice_min_Branded Shoes B',\n",
       " 'sellingprice_median_Hobi',\n",
       " 'sellingprice_sum_KadÄ±n A',\n",
       " 'sellingprice_sum_Kids & Baby Fashion',\n",
       " 'sellingprice_sum_Salon &Mutfak Mobilya',\n",
       " 'sellingprice_median_SaÄŸlÄ±k_y',\n",
       " 'sellingprice_sum_Sofra & Mutfak_x',\n",
       " 'sellingprice_sum_TakÄ±',\n",
       " 'sellingprice_max_Ä°Ã§ Giyim_x',\n",
       " 'sellingprice_min_Ev Dekorasyon_y',\n",
       " 'sellingprice_min_KadÄ±n B',\n",
       " 'sellingprice_sum_Erkek',\n",
       " 'sellingprice_count_KadÄ±n',\n",
       " 'sellingprice_median_Unisex',\n",
       " 'sellingprice_sum_KadÄ±n',\n",
       " 'sellingprice_sum_Elbise_y',\n",
       " 'sellingprice_sum_1186.0',\n",
       " 'sellingprice_count_407.0',\n",
       " 'sellingprice_count_529.0',\n",
       " 'sellingprice_median_535.0',\n",
       " 'sellingprice_mean_Ev Giyim_y',\n",
       " 'sellingprice_count_Kazak & HÄ±rka',\n",
       " 'sellingprice_count_601.0',\n",
       " 'sellingprice_median_975.0',\n",
       " 'sellingprice_count_other_category_id',\n",
       " 'sellingprice_min_407.0',\n",
       " 'sellingprice_median_529.0',\n",
       " 'sellingprice_mean_Pijama',\n",
       " 'sellingprice_mean_599.0',\n",
       " 'sellingprice_mean_601.0',\n",
       " 'sellingprice_mean_1186.0',\n",
       " 'sellingprice_min_Aksesuar',\n",
       " 'sellingprice_count_Anne & Bebek & Ã‡ocuk',\n",
       " 'sellingprice_min_AyakkabÄ±',\n",
       " 'sellingprice_max_Elektronik',\n",
       " 'sellingprice_count_Ev & Mobilya',\n",
       " 'sellingprice_count_Giyim',\n",
       " 'sellingprice_count_Kozmetik & KiÅŸisel BakÄ±m',\n",
       " 'sellingprice_mean_Spor & Outdoor',\n",
       " 'sellingprice_count_SÃ¼permarket',\n",
       " 'sellingprice_mean_YaÅŸam',\n",
       " 'sellingprice_mean_Anne & Bebek & Ã‡ocuk',\n",
       " 'sellingprice_sum_Ev & Mobilya',\n",
       " 'sellingprice_sum_Kozmetik & KiÅŸisel BakÄ±m',\n",
       " 'sellingprice_sum_SÃ¼permarket',\n",
       " 'sellingprice_count_Alt Giyim',\n",
       " 'sellingprice_median_DÄ±ÅŸ Giyim',\n",
       " 'sellingprice_count_Elektrikli Ev Aletleri',\n",
       " 'sellingprice_sum_Ev Tekstili',\n",
       " 'sellingprice_max_Mobilya',\n",
       " 'sellingprice_min_Spor Giyim',\n",
       " 'sellingprice_median_Telefon',\n",
       " 'sellingprice_mean_other_2',\n",
       " 'sellingprice_max_Ã‡anta',\n",
       " 'sellingprice_mean_Ãœst Giyim',\n",
       " 'sellingprice_max_Ä°Ã§ Giyim_y',\n",
       " 'sellingprice_mean_Alt Giyim',\n",
       " 'sellingprice_mean_Elektrikli Ev Aletleri',\n",
       " 'sellingprice_mean_Mobilya',\n",
       " 'sellingprice_sum_Ã‡anta',\n",
       " 'sellingprice_min_Ä°Ã§ Giyim_y',\n",
       " 'sellingprice_max_Alt-Ãœst TakÄ±m',\n",
       " 'sellingprice_sum_Bluz & Tunik',\n",
       " 'sellingprice_count_Ceket & Yelek',\n",
       " 'sellingprice_mean_Cep Telefonu Aksesuar',\n",
       " 'sellingprice_max_Etek',\n",
       " 'sellingprice_count_EÅŸofman',\n",
       " 'sellingprice_count_GÃ¶mlek',\n",
       " 'sellingprice_median_HalÄ± & Kilim & Paspas',\n",
       " 'sellingprice_count_Kaban & Mont',\n",
       " 'sellingprice_mean_Kolye',\n",
       " 'sellingprice_count_Medikal Maske',\n",
       " 'sellingprice_min_Mutfak Saklama ve DÃ¼zenleme',\n",
       " 'sellingprice_sum_Omuz Ã‡antasÄ±',\n",
       " 'sellingprice_sum_PiÅŸirme',\n",
       " 'sellingprice_max_Saat',\n",
       " 'sellingprice_mean_Salon & Oturma OdasÄ±',\n",
       " 'sellingprice_count_Sofra',\n",
       " 'sellingprice_sum_SÃ¼tyen',\n",
       " 'sellingprice_count_T-Shirt',\n",
       " 'sellingprice_median_Tablo',\n",
       " 'sellingprice_sum_Tayt',\n",
       " 'sellingprice_mean_YapÄ± Market',\n",
       " 'sellingprice_sum_Yatak OdasÄ± Tekstili',\n",
       " 'sellingprice_median_other_3',\n",
       " 'sellingprice_sum_Ã‡izme',\n",
       " 'sellingprice_max_Ã‡orap',\n",
       " 'sellingprice_min_Ceket & Yelek',\n",
       " 'sellingprice_max_EÅŸofman',\n",
       " 'sellingprice_mean_GÃ¶mlek',\n",
       " 'sellingprice_sum_Medikal Maske',\n",
       " 'sellingprice_sum_Sofra',\n",
       " 'sellingprice_median_T-Shirt',\n",
       " 'sellingprice_mean_Tablo',\n",
       " 'sellingprice_mean_Ã‡orap',\n",
       " 'sellingprice_median_Kolye',\n",
       " 'sellingprice_min_Alt-Ãœst TakÄ±m',\n",
       " 'weekend_sellingprice_count_0',\n",
       " 'weekend_sellingprice_median_1',\n",
       " 'campaign_sellingprice_count_0',\n",
       " 'campaign_sellingprice_median_1',\n",
       " 'day_sellingprice_count_Friday',\n",
       " 'day_sellingprice_median_Monday',\n",
       " 'day_sellingprice_median_Saturday',\n",
       " 'day_sellingprice_median_Sunday',\n",
       " 'day_sellingprice_median_Thursday',\n",
       " 'day_sellingprice_count_Tuesday',\n",
       " 'day_sellingprice_median_Wednesday',\n",
       " 'hour_sellingprice_median_0',\n",
       " 'hour_sellingprice_median_1',\n",
       " 'hour_sellingprice_median_2',\n",
       " 'hour_sellingprice_mean_3',\n",
       " 'hour_sellingprice_median_4',\n",
       " 'hour_sellingprice_median_5',\n",
       " 'hour_sellingprice_count_6',\n",
       " 'hour_sellingprice_mean_7',\n",
       " 'hour_sellingprice_median_8',\n",
       " 'hour_sellingprice_median_9',\n",
       " 'hour_sellingprice_median_10',\n",
       " 'hour_sellingprice_median_11',\n",
       " 'hour_sellingprice_median_12',\n",
       " 'hour_sellingprice_median_13',\n",
       " 'hour_sellingprice_count_14',\n",
       " 'hour_sellingprice_count_15',\n",
       " 'hour_sellingprice_median_16',\n",
       " 'hour_sellingprice_median_17',\n",
       " 'hour_sellingprice_count_18',\n",
       " 'hour_sellingprice_median_19',\n",
       " 'hour_sellingprice_median_20',\n",
       " 'hour_sellingprice_median_21',\n",
       " 'hour_sellingprice_median_22',\n",
       " 'hour_sellingprice_mean_23',\n",
       " 'hour_sellingprice_median_6',\n",
       " 'hour_sellingprice_median_14',\n",
       " 'hour_sellingprice_max_13',\n",
       " 'hour4_sellingprice_median_1',\n",
       " 'hour4_sellingprice_median_2',\n",
       " 'hour4_sellingprice_median_3',\n",
       " 'hour4_sellingprice_median_4',\n",
       " 'hour4_sellingprice_median_5',\n",
       " 'hour4_sellingprice_median_6',\n",
       " 'hour8_sellingprice_mean_1',\n",
       " 'hour8_sellingprice_count_2',\n",
       " 'hour8_sellingprice_median_3',\n",
       " 'all_operation_cnt',\n",
       " 'all_secs_btw_consecutives_mean',\n",
       " 'all_contentid_cnt',\n",
       " 'all_brand_id_cnt',\n",
       " 'all_Level1_Category_Id_cnt',\n",
       " 'all_Level2_Category_Id_cnt',\n",
       " 'all_Level3_Category_Id_cnt',\n",
       " 'all_day_name_cnt',\n",
       " 'all_hour_cnt',\n",
       " 'all_4_hour_interval_cnt',\n",
       " 'all_8_hour_interval_cnt',\n",
       " 'all_secs_btw_first_last',\n",
       " 'daily_operation_cnt',\n",
       " 'weekly_secs_btw_consecutives_mean',\n",
       " 'daily_contentid_cnt',\n",
       " 'daily_brand_id_cnt',\n",
       " 'daily_Level1_Category_Id_cnt',\n",
       " 'daily_Level2_Category_Id_cnt',\n",
       " 'daily_Level3_Category_Id_cnt',\n",
       " 'daily_hour_cnt',\n",
       " 'daily_4_hour_interval_cnt',\n",
       " 'daily_8_hour_interval_cnt',\n",
       " 'daily_secs_btw_first_last',\n",
       " 'weekly_operation_cnt',\n",
       " 'weekly_contentid_cnt',\n",
       " 'weekly_brand_id_cnt',\n",
       " 'weekly_Level1_Category_Id_cnt',\n",
       " 'weekly_Level2_Category_Id_cnt',\n",
       " 'weekly_Level3_Category_Id_cnt',\n",
       " 'weekly_day_name_cnt',\n",
       " 'weekly_hour_cnt',\n",
       " 'weekly_4_hour_interval_cnt',\n",
       " 'weekly_8_hour_interval_cnt',\n",
       " 'weekly_secs_btw_first_last',\n",
       " 'hourly_operation_cnt',\n",
       " 'hourly_contentid_cnt',\n",
       " 'hourly_brand_id_cnt',\n",
       " 'hourly_Level1_Category_Id_cnt',\n",
       " 'hourly_Level2_Category_Id_cnt',\n",
       " 'hourly_Level3_Category_Id_cnt',\n",
       " 'hourly_secs_btw_first_last',\n",
       " 'all_order_secs_btw_consecutives_mean',\n",
       " 'hourly_order_Level1_Category_Id_cnt',\n",
       " 'weekly_order_secs_btw_consecutives_mean',\n",
       " 'weekly_favorite_secs_btw_consecutives_max',\n",
       " 'daily_favorite_secs_btw_consecutives_mean',\n",
       " 'all_basket_secs_btw_consecutives_mean',\n",
       " 'daily_basket_secs_btw_consecutives_max',\n",
       " 'all_search_secs_btw_consecutives_mean',\n",
       " 'all_search_Level1_Category_Id_cnt',\n",
       " 'all_visit_secs_btw_consecutives_mean',\n",
       " 'all_visit_4_hour_interval_cnt',\n",
       " 'weekly_visit_secs_btw_consecutives_min',\n",
       " 'time_btw_basketand_order_max',\n",
       " 'search_cnt_before_order_24hour',\n",
       " 'visit_cnt_before_order_24hour',\n",
       " 'favorite_cnt_before_order_3hour',\n",
       " 'favorite_cnt_before_order_12hour',\n",
       " 'favorite_cnt_before_order_24hour',\n",
       " 'transaction_count',\n",
       " 'experience_flag']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf637063",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_tr2, y_tr2 = oversample.fit_resample(X_train_imputed[var_list], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89be0c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:14:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=20,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=1)\n",
    "rf.fit(X_tr2,y_tr2)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed[var_list])[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed[var_list])[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed[var_list])[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "537f95db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  1.0\n",
      "Val ROC:  0.841115072979009\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26defabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185889</td>\n",
       "      <td>0.30491</td>\n",
       "      <td>0.490798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5              0.0              0.0             0.0       0.185889   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0        0.30491      0.490798  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e53c814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.995,0.991,0.99,1.0,0.999,1.0,0.646,0.299,0.998,0.999,0.191,0.045,1.0,1.0,0.999,1.0,0.262,0.02,0.999,0.998,1.0,0.999,1.0,1.0,1.0,0.999,1.0,1.0,0.994,1.0,1.0,0.999,0.989,0.999,0.999,1.0,0.737,0.904,0.998,0.883,0.998,0.989,0.999,0.998,1.0,0.996,1.0,0.459,0.867,0.131,0.014,1.0,0.997,0.0,0.006,0.04,1.0,0.999,0.999,1.0,1.0,0.832,0.15,0.996,1.0,0.991,0.999,0.992,0.969,0.999,1.0,0.999,0.999,0.996,0.998,0.973,1.0,1.0,1.0,0.989,0.104,0.924,0.999,0.295,1.0,0.909,0.906,1.0,1.0,0.976,1.0,0.998,0.98,1.0,0.036,0.995,1.0,0.996,0.733,0.999,1.0,0.999,1.0,1.0,0.983,0.025,0.977,0.99,1.0,0.272,0.984,1.0,1.0,0.999,0.993,0.999,0.968,0.999,1.0,0.999,1.0,1.0,0.461,0.975,0.999,1.0,1.0,1.0,0.003,0.003,0.999,0.995,0.002,1.0,0.988,1.0,0.721,1.0,0.991,1.0,0.556,0.691,0.998,0.998,0.995,0.999,0.983,0.994,0.0,1.0,1.0,1.0,1.0,0.999,0.857,0.994,0.999,0.994,1.0,0.998,1.0,0.972,0.998,0.972,1.0,1.0,0.46,0.998,0.998,0.197,1.0,0.339,0.375,1.0,0.997,0.995,1.0,0.994,0.232,0.999,0.622,0.278,1.0,0.119,0.251,1.0,1.0,1.0,1.0,1.0,1.0,0.015,0.791,0.999,0.996,0.31,0.998,0.122,1.0,0.994,0.997,1.0,1.0,1.0,0.987,0.124,0.814,0.165,0.396,0.001,1.0,1.0,0.283,1.0,0.999,0.955,1.0,0.999,0.925,1.0,0.975,0.999,0.707,0.86,1.0,0.567,0.011,1.0,0.555,0.02,0.002,0.997,0.958,1.0,0.026,1.0,1.0,0.624,1.0,1.0,0.006,1.0,0.996,0.999,1.0,0.079,0.995,0.002,0.998,1.0,0.988,0.008,1.0,1.0,0.003,0.975,1.0,1.0,0.899,0.001,1.0,0.823,0.088,0.999,1.0,0.998,0.986,1.0,0.971,0.006,0.999,0.999,1.0,0.32,0.996,0.998,1.0,1.0,1.0,1.0,1.0,1.0,0.001,0.999,0.888,0.152,0.997,0.999,0.857,1.0,0.101,0.029,0.01,1.0,0.391,0.028,0.996,0.232,0.514,1.0,1.0,1.0,0.994,1.0,1.0,1.0,1.0,0.999,1.0,0.998,1.0,0.999,0.011,1.0,0.985,0.009,1.0,0.891,0.992,0.609,0.997,0.996,0.019,0.584,0.998,0.959,0.001,0.606,0.999,0.424,0.004,1.0,0.032,1.0,0.982,0.996,0.999,0.999,0.014,0.969,0.999,0.999,0.976,1.0,0.988,1.0,0.994,0.012,0.999,1.0,0.005,0.012,0.999,1.0,0.021,1.0,0.985,1.0,1.0,1.0,1.0,0.999,0.07,0.983,0.995,0.998,0.038,1.0,1.0,0.003,1.0,1.0,0.999,1.0,0.988,0.999,0.996,1.0,1.0,0.995,0.977,1.0,0.078,0.991,0.999,1.0,0.001,1.0,1.0,0.999,0.011,1.0,0.65,1.0,0.999,0.999,1.0,1.0,1.0,1.0,0.999,0.999,1.0,0.974,1.0,0.998,0.011,0.002,0.983,0.021,0.035,0.003,0.97,0.981,1.0,0.032,0.991,0.014,0.077,0.97,1.0,0.993,0.977,1.0,0.092,0.063,0.651,0.036,1.0,0.984,0.997,0.011,0.013,1.0,0.952,1.0,1.0,1.0,0.01,1.0,0.194,0.998,1.0,0.772,1.0,0.992,0.063,1.0,0.002,0.243,0.984,0.992,0.014,1.0,0.991,1.0,1.0,1.0,0.999,0.999,1.0,0.89,0.998,0.951,0.999,1.0,0.001,0.916,0.06,1.0,0.713,0.522,0.022,1.0,0.994,0.999,0.999,1.0,0.015,0.993,0.002,0.999,1.0,0.003,0.026,0.005,1.0,0.996,0.998,0.017,0.0,0.889,0.735,0.999,0.128,1.0,1.0,0.867,0.242,0.09,1.0,0.998,1.0,0.979,0.008,1.0,0.781,1.0,0.999,0.786,0.996,0.998,0.993,0.531,0.031,0.078,0.019,0.004,0.987,0.02,0.537,0.106,0.021,0.697,1.0,1.0,0.999,1.0,1.0,0.989,0.03,1.0,0.633,0.011,1.0,0.999,0.23,0.002,0.945,0.95,0.005,1.0,0.375,0.109,0.999,0.018,0.015,0.99,0.503,0.921,1.0,0.998,0.993,1.0,1.0,0.999,0.006,0.689,0.015,0.999,1.0,0.03,0.957,1.0,0.447,0.999,0.999,0.079,0.097,0.995,1.0,0.613,0.006,0.007,1.0,0.999,0.985,0.999,1.0,1.0,1.0,0.142,0.999,1.0,0.377,1.0,1.0,0.664,1.0,1.0,0.997,1.0,0.999,0.036,0.999,0.762,1.0,0.061,0.002,1.0,0.999,1.0,0.924,0.995,0.026,0.941,0.895,0.999,0.993,0.945,0.998,0.997,0.105,0.751,0.107,0.994,0.765,0.017,0.024,0.022,1.0,0.999,1.0,1.0,1.0,0.331,0.004,1.0,0.999,1.0,0.465,0.966,0.724,0.004,1.0,0.974,0.543,0.021,0.984,0.998,1.0,0.999,0.953,0.557,1.0,0.005,0.178,0.997,1.0,0.956,1.0,0.869,0.071,0.996,0.999,1.0,1.0,0.024,1.0,1.0,0.938,0.975,0.026,0.168,0.059,0.458,1.0,1.0,1.0,0.992,0.27,1.0,0.995,0.808,0.957,0.963,1.0,0.999,0.009,0.002,0.381,0.01,0.813,1.0,0.997,0.998,0.001,0.998,0.934,0.985,0.013,0.247,0.983,0.869,0.999,0.962,1.0,1.0,0.116,0.999,0.998,0.999,0.126,1.0,0.997,0.999,1.0,0.168,1.0,0.083,1.0,0.005,0.998,0.037,1.0,1.0,0.998,1.0,0.958,0.374,0.729,0.935,1.0,0.032,0.999,0.998,0.337,1.0,0.202,0.306,0.999,0.997,0.997,1.0,0.981,0.995,1.0,0.999,1.0,0.999,0.993,0.995,0.988,1.0,1.0,0.646,0.948,1.0,0.452,1.0,1.0,0.169,0.753,0.998,0.761,0.998,1.0,0.337,0.047,0.998,1.0,0.999,0.012,0.731,1.0,0.999,0.998,0.011,0.998,0.958,0.007,1.0,1.0,0.01,0.067,0.166,0.085,0.99,1.0,0.997,0.999,0.328,0.992,0.904,1.0,0.889,0.347,0.973,1.0,0.045,0.046,0.997,0.241,0.912,0.961,0.007,1.0,0.08,1.0,0.828,0.013,0.004,0.997,0.001,0.755,1.0,0.999,0.997,1.0,1.0,1.0,0.676,0.999,0.581,0.665,0.992,1.0,1.0,1.0,0.999,1.0,1.0,0.909,0.972,0.999,0.927,0.999,0.995,0.527,1.0,0.992,1.0,1.0,0.872,0.027,0.999,0.865,0.012,0.982,0.147,1.0,0.967,1.0,0.999,0.999,0.999,0.996,0.889,1.0,0.454,0.984,0.993,0.993,0.462,0.016,1.0,0.994,0.074,1.0,0.988,1.0,1.0,0.076,1.0,0.067,0.035,0.04,1.0,0.981,0.64,1.0,0.986,1.0,0.034,0.856,0.996,1.0,1.0,0.002,0.085,0.327,0.018,1.0,0.999,0.989,0.602,1.0,0.963,0.929,0.044,1.0,0.383,0.505,0.998,1.0,0.999,0.992,0.64,0.012,0.997,0.252,0.967,0.865,0.853,0.011,0.993,0.025,0.014,0.724,0.999,0.154,0.091,1.0,1.0,0.01,0.992,1.0,1.0,0.869,1.0,1.0,0.045,0.999,0.998,0.996,0.006,0.994,0.405,0.543,0.073,0.999,1.0,0.999,1.0,0.153,0.998,1.0,0.007,1.0,0.88,1.0,0.035,0.762,1.0,1.0,0.998,0.613,1.0,1.0,0.001,1.0,0.964,0.008,1.0,0.947,1.0,1.0,1.0,0.999,0.99,1.0,0.733,0.007,1.0,0.999,1.0,0.85,0.023,1.0,0.089,0.949,0.014,0.407,1.0,1.0,0.975,0.999,0.997,0.999,0.54,1.0,0.831,0.974,0.932,0.992,0.045,1.0,0.996,0.998,1.0,1.0,1.0,0.992,0.999,0.11,1.0,0.21,0.878,0.995,0.988,1.0,0.986,0.294,1.0,0.999,0.981,0.141,0.33,1.0,0.988,0.039,1.0,0.92,0.995,1.0,0.006,0.001,1.0,0.372,1.0,0.044,0.996,0.006,0.998,0.0,0.127,0.005,0.202,0.058,0.999,1.0,0.999,1.0,0.829,1.0,0.88,0.668,0.009,0.988,0.999,0.002,0.126,0.003,0.575,0.004,0.034,0.998,1.0,0.978,0.994,0.999,0.77,0.916,0.999,0.998,0.986,0.993,0.249,0.993,0.993,0.047,0.014,1.0,0.997,0.715,1.0,0.989,0.997,0.994,0.647,0.998,0.004,1.0,1.0,0.996,0.999,0.999,1.0,0.142,0.989,0.064,0.559,0.525,0.532,0.779,0.998,0.048,0.991,0.977,0.009,0.254,0.332,0.997,0.011,0.048,0.128,0.962,1.0,0.733,1.0,0.944,0.005,0.658,0.523,0.994,0.037,0.34,0.967,0.281,0.47,0.06,0.993,0.999,0.974,0.17,0.506,0.001,0.998,0.181,0.896,0.014,1.0,0.952,0.727,0.941,0.999,0.064,0.572,1.0,0.616,0.011,0.997,0.008,1.0,0.087,0.991,0.996,0.997,1.0,0.003,0.183,0.991,0.564,0.609,1.0,0.321,0.652,0.036,0.985,0.998,0.901,0.011,0.989,0.997,1.0,0.064,1.0,1.0,0.189,0.032,0.312,0.951,0.996,0.995,0.995,0.98,0.975,1.0,0.997,0.681,0.283,1.0,0.001,0.995,0.229,1.0,0.066,0.156,0.261,0.999,0.991,0.123,0.029,0.022,0.993,0.99,0.998,0.003,1.0,0.02,0.001,1.0,0.417,0.026,0.998,0.024,0.998,0.887,0.992,0.456,0.962,0.539,0.889,0.028,0.741,0.038,0.978,0.951,0.459,1.0,1.0,0.008,1.0,0.993,0.995,0.961,1.0,0.948,0.487,0.996,0.999,1.0,0.99,0.188,0.022,0.27,0.83,0.01,0.642,0.996,0.001,0.097,0.999,0.108,1.0,1.0,0.248,1.0,0.036,0.323,0.007,0.034,1.0,0.999,0.004,0.979,0.999,1.0,0.991,0.959,0.967,1.0,0.915,0.277,0.999,0.158,1.0,0.38,0.877,0.521,1.0,0.994,1.0,0.553,0.882,0.113,0.09,1.0,1.0,0.08,0.999,0.003,0.676,1.0,0.999,0.993,0.026,1.0,0.126,1.0,1.0,0.055,0.991,0.999,0.093,0.367,0.924,0.086,0.998,0.992,0.712,0.691,0.002,0.002,0.005,0.958,0.061,0.656,1.0,0.997,0.997,0.933,0.005,0.999,0.052,1.0,0.109,1.0,0.0,0.986,0.983,1.0,0.023,0.165,0.987,0.887,0.993,1.0,0.031,0.027,0.081,0.994,1.0,0.047,0.994,0.015,0.103,0.331,0.942,1.0,0.936,0.999,0.2,0.039,0.714,0.999,0.443,0.863,0.983,0.033,0.356,1.0,0.035,0.992,0.773,0.959,0.998,1.0,1.0,0.571,1.0,0.987,0.856,0.831,0.44,0.049,0.901,0.997,0.122,0.946,1.0,0.054,0.265,0.638,0.999,0.737,0.467,1.0,0.193,0.475,0.993,1.0,0.401,0.998,0.995,0.449,0.036,0.157,0.945,0.792,0.89,0.065,0.005,0.019,0.496,0.999,0.018,0.112,0.272,0.283,0.387,0.991,0.98,0.375,0.211,0.951,0.772,0.083,0.937,0.007,0.999,0.303,0.003,0.998,0.617,0.876,0.292,0.251,0.034,1.0,0.055,0.984,0.033,0.006,0.493,0.205,0.895,0.084,0.923,0.987,0.049,0.539,0.063,0.994,0.973,0.15,0.998,0.991,0.014,0.657,0.0,0.516,0.149,0.871,1.0,0.985,0.797,0.849,0.02,0.047,0.98,0.995,0.529,0.445,0.07,1.0,0.352,0.019,0.31,0.94,0.989,0.022,0.215,0.428,0.77,0.185,0.406,0.996,0.163,0.998,1.0,0.031,0.006,0.993,0.397,0.91,0.184,1.0,0.69,0.166,0.049,0.888,0.824,0.974,0.999,0.874,0.003,0.996,0.008,0.262,0.999,0.994,0.999,0.991,0.334,0.999,1.0,0.028,0.994,0.506,0.985,0.001,0.995,0.998,0.896,0.035,0.91,1.0,0.966,0.606,0.179,0.999,1.0,1.0,0.985,1.0,0.996,1.0,0.986,0.999,0.575,0.984,0.005,0.999,0.248,0.994,0.475,0.998,0.403,0.991,0.934,0.022,1.0,0.999,0.909,1.0,1.0,0.927,0.005,0.998,0.003,0.976,1.0,0.002,1.0,0.099,0.091,0.905,0.328,0.962,1.0,0.992,0.997,1.0,0.468,0.021,0.025,0.991,0.045,0.002,0.386,0.991,0.001,0.043,1.0,0.193,0.998,0.982,0.999,1.0,0.984,0.357,1.0,0.0,0.847,0.994,1.0,0.995,0.401,0.854,0.996,1.0,0.931,0.997,0.994,0.184,0.987,0.001,0.006,1.0,0.872,0.985,0.01,0.11,0.96,0.052,0.999,0.948,0.986,0.995,1.0,0.919,0.753,1.0,0.002,0.993,0.027,0.044,0.936,0.999,0.998,0.533,0.987,0.999,0.008,0.029,0.671,0.014,0.996,0.165,0.024,1.0,0.998,0.399,0.999,0.999,0.046,0.013,0.997,0.523,0.995,0.206,0.998,0.013,1.0,0.006,0.001,0.085,0.608,1.0,0.319,0.046,0.057,0.774,0.976,0.998,0.997,0.995,1.0,0.355,0.997,0.991,0.196,0.358,0.002,0.999,0.999,0.995,0.998,1.0,0.95,0.999,1.0,0.975,0.001,0.998,0.998,0.866,0.999,0.001,0.045,1.0,0.998,0.009,1.0,0.985,1.0,0.996,0.999,0.993,0.992,0.997,1.0,0.999,0.993,0.988,0.507,0.468,0.986,0.17,0.996,0.13,0.999,0.992,0.001,0.22,1.0,0.225,0.986,0.999,0.993,0.576,1.0,0.552,0.368,1.0,0.998,0.033,0.998,0.004,0.401,0.943,0.238,0.816,1.0,0.998,0.587,0.999,1.0,0.891,0.974,0.992,0.07,0.989,0.998,0.966,0.998,0.999,0.996,0.995,0.997,0.617,0.998,0.866,0.722,0.094,0.035,0.002,0.011,0.712,1.0,1.0,0.002,0.997,0.998,0.049,0.994,0.957,0.654,0.199,0.003,0.064,0.141,1.0,1.0,0.129,0.997,0.234,0.273,0.358,0.531,0.028,0.076,0.914,0.557,0.438,0.997,0.999,0.097,0.04,0.607,0.92,0.098,0.956,0.29,0.946,0.76,0.009,0.06,0.028,0.408,0.997,0.999,0.009,0.341,0.996,0.99,0.996,0.021,0.796,0.479,1.0,0.935,1.0,0.997,0.471,0.745,0.998,0.668,0.999,0.004,0.996,0.994,0.97,0.987,0.014,0.001,0.0,0.16,0.998,0.1,0.013,0.994,0.826,0.98,0.061,0.063,0.964,0.009,0.0,0.859,0.996,0.823,0.996,0.323,0.109,0.03,0.051,0.985,0.801,0.989,0.186,0.947,0.782,0.488,0.627,0.11,0.924,0.981,0.012,0.937,0.753,0.899,0.002,0.88,0.334,0.984,0.746,0.926,0.005,0.803,0.986,0.876,0.234,0.718,0.997,0.808,0.948,0.001,0.982,0.752,0.833,0.006,0.689,0.591,0.655,0.014,0.772,0.95,0.96,0.315,0.555,0.976,0.027,0.005,0.992,0.977,0.997,0.983,0.941,0.977,0.781,0.062,0.055,0.009,0.994,0.166,0.01,0.969,0.998,0.999,0.998,0.994,0.978,1.0,0.99,0.799,0.962,0.988,0.99,0.988,0.925,0.995,0.99,0.961,0.97,0.367,0.985,0.052,0.318,0.987,0.477,0.984,0.094,0.506,1.0,0.995,0.944,0.339,0.998,0.982,1.0,0.999,0.99,0.997,0.972,0.09,0.858,0.012,0.7,0.915,0.819,0.046,0.019,0.911,0.993,0.262,0.999,0.423,0.876,0.997,0.106,0.988,0.975,0.164,0.949,0.013,0.7,1.0,0.975,0.06,0.034,0.993,0.02,0.993,0.031,0.025,0.839,0.454,0.925,0.037,0.215,0.971,0.995,0.298,0.33,0.999,0.039,0.986,1.0,0.997,0.892,0.557,1.0,0.999,0.999,0.008,0.972,0.999,0.98,0.88,0.624,0.062,0.439,0.991,0.036,0.424,0.003,0.002,0.993,0.996,0.007,0.093,0.835,0.311,0.042,0.142,0.001,0.172,0.982,0.282,0.941,0.993,0.902,0.351,0.192,0.215,0.909,0.044,0.997,0.198,0.462,0.439,0.956,0.999,0.966,0.858,0.06,0.933,0.621,0.845,0.11,0.798,0.013,0.993,0.956,0.907,0.936,0.862,0.97,0.999,0.952,0.994,0.93,0.181,0.036,0.486,0.022,0.221,0.973,0.946,0.977,0.311,0.027,0.654,0.121,0.998,0.094,0.426,0.004,0.474,0.514,0.972,0.388,0.993,0.968,0.639,0.984,0.998,0.991,0.033,0.991,1.0,0.961,0.99,0.891,0.008,0.111,0.96,0.185,0.133,0.801,0.903,0.138,0.093,0.185,0.281,0.97,0.971,1.0,1.0,0.019,0.525,0.996,0.992,0.691,0.497,0.761,0.854,0.612,0.936,0.044,0.823,0.992,0.019,0.752,0.118,0.625,0.047,0.197,0.074,0.477,0.985,0.897,0.593,0.997,0.983,0.034,0.97,0.918,0.999,0.9,0.056,0.129,0.902,0.992,0.996,0.066,0.016,0.973,0.997,0.97,0.171,0.677,0.004,0.01,0.605,0.487,0.242,0.987,0.257,0.943,0.128,0.028,0.005,0.944,0.995,0.991,0.008,0.998,0.007,0.999,0.974,0.949,0.962,0.068,0.033,0.002,0.335,0.748,0.99,0.617,0.332,0.941,0.978,0.268,0.341,0.992,0.715,0.633,0.134,0.021,0.541,0.383,0.136,0.993,0.311,0.733,0.998,0.381,0.993,0.987,0.072,0.122,0.088,0.017,0.977,0.989,0.013,0.995,0.008,0.999,0.818,0.82,0.009,0.018,0.338,0.233,0.591,0.583,0.357,0.896,0.418,0.783,0.996,0.989,0.168,0.827,0.734,0.803,0.815,0.112,0.453,0.998,0.004,0.925,0.046,0.816,0.828,0.301,0.928,0.546,0.993,0.271,0.988,0.412,0.91,0.788,0.972,0.145,0.929,0.408,0.057,0.286,0.019,0.979,0.245,0.386,0.889,1.0,0.469,0.985,0.94,0.999,0.052,0.96,0.035,0.786,0.994,0.965,0.009,0.99,0.97,0.651,0.97,0.544,0.406,0.436,0.446,0.024,0.076,0.013,0.033,0.587,0.997,0.982,0.297,0.032,0.051,0.001,0.072,0.203,0.094,0.045,0.026,0.975,0.995,0.978,0.041,0.627,0.966,0.97,0.971,0.982,0.957,0.024,0.98,0.922,0.006,0.794,0.832,0.369,0.996,0.024,0.506,0.991,0.624,0.005,0.958,0.995,0.467,0.989,0.988,0.521,0.594,0.39,0.697,0.542,0.963,0.997,0.944,0.396,0.949,0.048,0.891,0.783,0.658,0.041,0.076,0.036,0.862,0.416,0.024,0.999,0.02,0.003,0.11,0.632,0.82,0.804,0.031,0.049,0.49,0.997,0.484,0.547,0.664,0.134,0.56,0.955,0.999,0.006,0.921,0.992,0.002,0.013,0.015,0.01,0.999,0.885,0.064,0.733,0.997,0.182,0.243,0.989,0.988,0.844,0.756,0.95,0.697,0.635,0.965,0.008,0.019,0.3,0.996,0.637,0.184,0.943,0.937,0.88,0.91,0.043,0.697,0.887,0.06,0.0,0.925,0.66,0.99,0.998,0.461,0.697,0.447,0.999,0.02,0.096,0.159,0.001,0.733,0.09,0.586,0.945,0.005,0.589,0.863,0.695,0.987,0.286'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub28 = pd.merge(id_test_df,pred_test.reset_index(),how='left',on='unique_id')\n",
    "sub28['pred'] = sub28['pred'].apply(lambda x: round(x,3))\n",
    "sub28_txt = ''\n",
    "for prob in list(sub28['pred'].values):\n",
    "    sub28_txt = sub28_txt+','+str(prob)\n",
    "sub28_txt = sub28_txt[1:]\n",
    "\n",
    "sub28_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248ee4f",
   "metadata": {},
   "source": [
    "### Submission 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e14227c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_and_val = pd.concat([X_train,X_val])\n",
    "y_train_and_val = pd.concat([y_train,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc7f28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(max_depth=2,learning_rate=0.1,n_estimators=200,min_child_samples=20,colsample_bytree=1)\n",
    "lgbm.fit(X_train_and_val,y_train_and_val)\n",
    "\n",
    "pred_train = pd.DataFrame(lgbm.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(lgbm.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(lgbm.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf5055e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9322050843804972\n",
      "Val ROC:  0.9282218225293547\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdce1580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.103331</td>\n",
       "      <td>0.243557</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.103331         0.243557        0.346888       0.111262   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.222222      0.333484  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2486c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_lgbm_cold = pred_train.copy()\n",
    "pred_val_lgbm_cold = pred_val.copy()\n",
    "pred_test_lgbm_cold = pred_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e46d1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=50,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5)\n",
    "rf.fit(X_train_experienced_under_imputed,y_train_experienced_under)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7a4403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9922903803429884\n",
      "Val ROC:  0.8491632745364088\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "775c7898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.408062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.126785              0.0        0.126785       0.265943   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.142119      0.408062  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c579c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = pred_train.copy()\n",
    "pred_val_rf = pred_val.copy()\n",
    "pred_test_rf = pred_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8005017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>female_label</th>\n",
       "      <th>pred_binary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999889</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999861</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998537</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954260</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998548</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>0.996615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>0.007574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>0.000132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>0.006232</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.985598</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4494 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred  female_label  pred_binary\n",
       "unique_id                                     \n",
       "1          0.999889             1            1\n",
       "2          0.999861             1            1\n",
       "3          0.998537             1            1\n",
       "4          0.954260             1            1\n",
       "5          0.998548             1            1\n",
       "...             ...           ...          ...\n",
       "7987       0.996615             1            1\n",
       "7988       0.007574             0            0\n",
       "7989       0.000132             0            0\n",
       "7992       0.006232             1            0\n",
       "7996       0.985598             1            1\n",
       "\n",
       "[4494 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98e809e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.986,0.971,0.905,0.997,0.964,0.996,0.483,0.158,0.959,0.98,0.182,0.029,0.992,0.997,0.921,0.993,0.094,0.078,0.988,0.968,0.994,0.935,0.981,0.989,0.998,0.977,0.987,0.997,0.899,0.932,0.996,0.914,0.972,0.982,0.995,0.995,0.729,0.606,0.993,0.875,0.958,0.921,0.956,0.96,0.991,0.975,0.996,0.331,0.519,0.792,0.087,0.995,0.967,0.052,0.162,0.044,0.984,0.987,0.991,0.993,0.99,0.538,0.12,0.99,0.996,0.873,0.993,0.959,0.977,0.994,0.998,0.983,0.977,0.986,0.936,0.572,0.972,0.997,0.99,0.927,0.115,0.89,0.996,0.122,0.994,0.945,0.723,0.995,0.979,0.976,0.998,0.99,0.574,0.99,0.248,0.974,0.964,0.888,0.347,0.98,0.976,0.993,0.985,0.993,0.806,0.409,0.651,0.598,0.986,0.619,0.984,0.999,0.976,0.997,0.673,0.936,0.857,0.978,0.974,0.973,0.998,0.998,0.341,0.924,0.976,0.972,0.988,0.988,0.109,0.073,0.995,0.8,0.054,0.977,0.892,0.993,0.632,0.989,0.886,0.989,0.218,0.622,0.987,0.957,0.79,0.993,0.767,0.986,0.076,0.985,0.995,0.994,0.985,0.996,0.291,0.923,0.961,0.976,0.988,0.862,0.996,0.867,0.992,0.873,0.993,0.98,0.825,0.957,0.977,0.248,0.99,0.626,0.742,0.994,0.996,0.888,0.993,0.926,0.244,0.975,0.337,0.103,0.972,0.516,0.628,0.992,0.988,0.996,0.993,0.955,0.993,0.112,0.331,0.961,0.966,0.084,0.986,0.244,0.991,0.845,0.959,0.994,0.998,0.995,0.879,0.532,0.116,0.156,0.164,0.062,0.99,0.982,0.188,0.998,0.987,0.39,0.993,0.991,0.664,0.99,0.91,0.991,0.238,0.985,0.996,0.275,0.087,0.991,0.327,0.088,0.025,0.94,0.758,0.996,0.172,0.988,0.995,0.933,0.996,0.992,0.027,0.991,0.975,0.983,0.995,0.17,0.766,0.056,0.988,0.972,0.722,0.105,0.989,0.996,0.029,0.896,0.997,0.991,0.173,0.074,0.995,0.868,0.094,0.995,0.995,0.997,0.906,0.997,0.985,0.076,0.954,0.944,0.992,0.286,0.99,0.675,0.982,0.99,0.993,0.928,0.995,0.982,0.097,0.99,0.923,0.072,0.995,0.968,0.337,0.993,0.067,0.174,0.175,0.986,0.702,0.112,0.981,0.035,0.483,0.997,0.995,0.987,0.978,0.983,0.999,0.99,0.962,0.993,0.997,0.978,0.998,0.956,0.119,0.972,0.986,0.116,0.998,0.347,0.802,0.442,0.953,0.96,0.064,0.594,0.995,0.946,0.162,0.753,0.663,0.675,0.078,0.994,0.07,0.996,0.906,0.682,0.966,0.997,0.06,0.887,0.956,0.962,0.963,0.997,0.964,0.994,0.958,0.073,0.96,0.996,0.111,0.146,0.987,0.975,0.23,0.99,0.661,0.991,0.998,0.999,0.993,0.98,0.159,0.947,0.937,0.991,0.06,0.992,0.977,0.131,0.989,0.966,0.971,0.996,0.915,0.975,0.99,0.989,0.987,0.895,0.93,0.99,0.088,0.827,0.991,0.988,0.04,0.996,0.996,0.963,0.123,0.995,0.535,0.997,0.954,0.993,0.998,1.0,0.99,0.974,0.934,0.995,0.993,0.908,0.995,0.946,0.126,0.087,0.897,0.054,0.145,0.149,0.909,0.544,0.97,0.086,0.979,0.113,0.247,0.85,0.994,0.948,0.917,0.986,0.103,0.092,0.356,0.115,0.993,0.951,0.774,0.117,0.056,0.996,0.36,0.969,0.989,0.958,0.129,0.976,0.117,0.986,0.994,0.31,0.987,0.922,0.222,0.997,0.052,0.292,0.955,0.813,0.098,0.985,0.925,0.974,0.998,0.994,0.971,0.991,0.993,0.26,0.957,0.929,0.818,0.984,0.097,0.587,0.138,0.968,0.629,0.305,0.109,0.975,0.93,0.968,0.955,0.99,0.181,0.928,0.086,0.979,0.995,0.045,0.074,0.122,0.975,0.985,0.986,0.075,0.048,0.918,0.159,0.962,0.563,0.997,0.992,0.469,0.292,0.067,0.993,0.941,0.996,0.81,0.044,0.995,0.16,0.995,0.994,0.631,0.961,0.953,0.954,0.759,0.152,0.061,0.059,0.234,0.724,0.088,0.608,0.245,0.085,0.206,0.986,0.987,0.988,0.995,0.997,0.978,0.035,0.99,0.699,0.026,0.995,0.905,0.293,0.111,0.977,0.927,0.248,0.992,0.227,0.226,0.97,0.145,0.099,0.989,0.335,0.297,0.993,0.987,0.971,0.99,0.946,0.987,0.046,0.894,0.214,0.972,0.992,0.161,0.786,0.992,0.08,0.978,0.956,0.223,0.166,0.963,0.991,0.872,0.065,0.087,0.989,0.991,0.514,0.962,0.99,0.997,0.989,0.15,0.978,0.992,0.376,0.997,0.994,0.68,0.991,0.945,0.952,0.994,0.951,0.151,0.969,0.872,0.997,0.123,0.07,0.965,0.977,0.998,0.719,0.992,0.137,0.682,0.299,0.996,0.992,0.956,0.996,0.982,0.064,0.478,0.137,0.992,0.919,0.08,0.104,0.077,0.997,0.976,0.973,0.982,0.991,0.207,0.045,0.993,0.989,0.987,0.556,0.941,0.295,0.046,0.985,0.631,0.149,0.045,0.975,0.997,0.994,0.993,0.925,0.435,0.991,0.099,0.243,0.96,0.998,0.967,0.99,0.248,0.048,0.948,0.976,0.981,0.953,0.113,0.989,0.989,0.755,0.955,0.142,0.232,0.086,0.245,0.995,0.997,0.991,0.971,0.28,0.987,0.941,0.353,0.845,0.826,0.984,0.967,0.094,0.209,0.218,0.069,0.862,0.991,0.958,0.983,0.06,0.845,0.923,0.826,0.132,0.169,0.76,0.9,0.961,0.559,0.998,0.999,0.083,0.962,0.977,0.983,0.233,0.993,0.948,0.959,0.994,0.218,0.99,0.161,0.998,0.179,0.978,0.078,0.972,0.971,0.974,0.977,0.9,0.213,0.888,0.824,0.995,0.165,0.986,0.922,0.122,0.998,0.16,0.097,0.999,0.871,0.939,0.986,0.933,0.943,0.997,0.997,0.993,0.996,0.98,0.987,0.962,0.997,0.995,0.22,0.279,0.982,0.26,0.978,0.988,0.111,0.529,0.988,0.239,0.921,0.991,0.14,0.217,0.986,0.995,0.993,0.047,0.895,0.998,0.919,0.935,0.252,0.989,0.592,0.073,0.976,0.997,0.11,0.428,0.162,0.199,0.862,0.994,0.72,0.989,0.241,0.978,0.96,0.997,0.241,0.168,0.931,0.996,0.071,0.115,0.985,0.503,0.743,0.859,0.273,0.987,0.071,0.997,0.775,0.129,0.054,0.951,0.028,0.573,0.999,0.946,0.941,0.994,0.995,0.998,0.203,0.976,0.875,0.648,0.966,0.994,0.995,0.971,0.996,0.991,0.987,0.797,0.731,0.885,0.264,0.966,0.882,0.293,0.997,0.946,0.983,0.933,0.344,0.232,0.964,0.263,0.044,0.922,0.215,0.991,0.871,0.998,0.993,0.972,0.985,0.99,0.821,0.993,0.925,0.925,0.98,0.937,0.245,0.222,0.995,0.967,0.197,0.997,0.968,0.977,0.995,0.323,0.998,0.143,0.589,0.128,0.878,0.734,0.219,0.997,0.796,0.991,0.322,0.47,0.99,0.983,0.99,0.037,0.202,0.517,0.21,0.978,0.935,0.723,0.136,0.997,0.582,0.646,0.144,0.996,0.155,0.135,0.796,0.996,0.983,0.924,0.303,0.081,0.337,0.14,0.886,0.777,0.244,0.158,0.951,0.105,0.068,0.386,0.911,0.384,0.191,0.999,0.991,0.261,0.976,0.997,0.975,0.65,0.988,0.956,0.051,0.99,0.931,0.936,0.202,0.983,0.337,0.264,0.05,0.985,0.998,0.99,0.992,0.264,0.986,0.996,0.066,0.979,0.701,0.996,0.109,0.347,0.999,0.988,0.989,0.973,0.994,0.985,0.057,0.994,0.817,0.048,0.996,0.91,0.993,0.993,0.996,0.995,0.295,0.992,0.624,0.231,0.938,0.984,0.991,0.98,0.172,0.987,0.135,0.609,0.07,0.052,0.992,0.995,0.935,0.982,0.975,0.972,0.178,0.996,0.782,0.888,0.24,0.948,0.13,0.988,0.79,0.939,0.994,0.98,0.997,0.941,0.992,0.094,0.992,0.778,0.935,0.952,0.987,0.996,0.833,0.333,0.974,0.989,0.931,0.678,0.1,0.991,0.905,0.229,0.996,0.813,0.93,0.97,0.13,0.274,0.997,0.264,0.996,0.11,0.961,0.325,0.995,0.059,0.569,0.169,0.303,0.275,0.986,0.986,0.989,0.983,0.385,0.993,0.275,0.778,0.069,0.856,0.961,0.098,0.172,0.086,0.391,0.056,0.154,0.983,0.993,0.815,0.995,0.995,0.344,0.936,0.935,0.938,0.973,0.99,0.103,0.688,0.721,0.42,0.326,0.997,0.994,0.805,0.997,0.98,0.972,0.974,0.579,0.974,0.155,0.973,0.98,0.767,0.982,0.93,0.998,0.666,0.985,0.268,0.302,0.216,0.222,0.453,0.946,0.244,0.522,0.583,0.166,0.153,0.368,0.905,0.084,0.25,0.277,0.924,0.986,0.725,0.995,0.649,0.066,0.188,0.523,0.857,0.328,0.529,0.501,0.189,0.315,0.21,0.993,0.993,0.899,0.17,0.539,0.08,0.985,0.414,0.706,0.101,0.998,0.23,0.147,0.884,0.969,0.184,0.571,0.949,0.359,0.064,0.968,0.038,0.996,0.278,0.904,0.921,0.856,0.994,0.061,0.249,0.9,0.671,0.32,0.987,0.238,0.456,0.12,0.929,0.907,0.84,0.121,0.959,0.978,0.989,0.358,0.992,0.996,0.193,0.047,0.122,0.819,0.98,0.991,0.915,0.965,0.776,0.981,0.92,0.493,0.468,0.996,0.08,0.956,0.211,0.996,0.268,0.832,0.064,0.985,0.938,0.251,0.116,0.082,0.981,0.841,0.96,0.063,0.992,0.096,0.09,0.981,0.229,0.057,0.778,0.112,0.949,0.728,0.683,0.278,0.843,0.609,0.89,0.191,0.895,0.097,0.706,0.877,0.765,0.975,0.989,0.189,0.99,0.974,0.991,0.637,0.998,0.895,0.84,0.926,0.958,0.997,0.803,0.193,0.078,0.089,0.525,0.085,0.7,0.823,0.075,0.178,0.966,0.331,0.987,0.992,0.189,0.997,0.175,0.399,0.021,0.147,0.994,0.966,0.045,0.82,0.988,0.996,0.982,0.941,0.75,0.996,0.98,0.246,0.976,0.309,0.994,0.287,0.837,0.422,0.995,0.839,0.995,0.25,0.108,0.138,0.245,0.995,0.995,0.233,0.961,0.083,0.687,0.971,0.984,0.867,0.084,0.985,0.216,0.997,0.992,0.066,0.94,0.993,0.144,0.623,0.781,0.234,0.976,0.902,0.543,0.203,0.144,0.081,0.159,0.915,0.162,0.714,0.982,0.863,0.992,0.559,0.04,0.967,0.151,0.994,0.151,0.978,0.075,0.676,0.412,0.977,0.084,0.083,0.93,0.304,0.863,0.998,0.268,0.058,0.483,0.96,0.993,0.317,0.977,0.164,0.088,0.375,0.363,0.995,0.185,0.983,0.382,0.169,0.381,0.992,0.267,0.634,0.428,0.256,0.168,0.997,0.235,0.655,0.147,0.829,0.948,0.983,0.998,0.185,0.996,0.633,0.23,0.825,0.415,0.17,0.541,0.932,0.424,0.927,0.998,0.185,0.165,0.202,0.922,0.532,0.166,0.988,0.058,0.216,0.957,0.985,0.69,0.972,0.897,0.184,0.284,0.118,0.684,0.262,0.272,0.285,0.194,0.323,0.18,0.98,0.14,0.18,0.131,0.412,0.485,0.986,0.614,0.486,0.565,0.248,0.26,0.175,0.65,0.164,0.825,0.102,0.173,0.982,0.119,0.732,0.321,0.355,0.212,0.983,0.345,0.798,0.446,0.187,0.712,0.118,0.649,0.071,0.751,0.866,0.156,0.369,0.125,0.938,0.885,0.266,0.96,0.965,0.195,0.147,0.095,0.349,0.286,0.763,0.996,0.88,0.538,0.309,0.18,0.182,0.846,0.888,0.581,0.141,0.23,0.994,0.435,0.105,0.09,0.788,0.869,0.225,0.148,0.558,0.925,0.115,0.322,0.957,0.156,0.994,0.939,0.166,0.186,0.955,0.137,0.758,0.137,0.99,0.931,0.275,0.248,0.803,0.313,0.68,0.983,0.456,0.185,0.992,0.163,0.235,0.961,0.996,0.979,0.955,0.71,0.953,0.99,0.228,0.863,0.498,0.909,0.086,0.964,0.963,0.33,0.185,0.935,0.996,0.306,0.878,0.245,0.985,0.996,0.989,0.726,0.988,0.726,0.991,0.938,0.974,0.292,0.53,0.102,0.984,0.141,0.959,0.151,0.98,0.306,0.925,0.404,0.388,0.996,0.933,0.776,0.991,0.999,0.719,0.26,0.99,0.057,0.973,0.991,0.051,0.995,0.117,0.13,0.902,0.229,0.943,0.998,0.452,0.977,0.974,0.11,0.078,0.073,0.866,0.233,0.131,0.125,0.945,0.169,0.203,0.993,0.155,0.975,0.936,0.994,0.981,0.278,0.143,0.987,0.02,0.808,0.852,0.992,0.974,0.209,0.542,0.948,0.992,0.933,0.956,0.966,0.068,0.838,0.133,0.189,0.97,0.54,0.954,0.126,0.606,0.978,0.078,0.956,0.293,0.93,0.978,0.993,0.898,0.273,0.995,0.121,0.941,0.457,0.194,0.959,0.984,0.991,0.761,0.942,0.975,0.081,0.164,0.184,0.098,0.888,0.186,0.143,0.989,0.965,0.306,0.968,0.994,0.095,0.115,0.97,0.173,0.798,0.347,0.969,0.116,0.984,0.065,0.252,0.126,0.191,0.81,0.221,0.083,0.182,0.157,0.839,0.955,0.935,0.965,0.978,0.191,0.924,0.983,0.14,0.146,0.116,0.993,0.971,0.957,0.972,0.99,0.903,0.97,0.985,0.938,0.101,0.963,0.961,0.898,0.983,0.075,0.111,0.993,0.943,0.114,0.947,0.756,0.994,0.987,0.982,0.929,0.936,0.99,0.975,0.962,0.707,0.857,0.534,0.24,0.573,0.43,0.888,0.089,0.972,0.977,0.074,0.306,0.93,0.254,0.985,0.896,0.946,0.53,0.973,0.152,0.116,0.984,0.979,0.206,0.944,0.09,0.213,0.437,0.171,0.36,0.98,0.95,0.304,0.987,0.985,0.889,0.65,0.961,0.163,0.928,0.962,0.659,0.988,0.983,0.846,0.772,0.97,0.729,0.94,0.634,0.75,0.171,0.135,0.159,0.077,0.563,0.99,0.958,0.043,0.972,0.935,0.059,0.788,0.868,0.373,0.118,0.118,0.143,0.504,0.979,0.971,0.056,0.972,0.33,0.281,0.119,0.219,0.29,0.201,0.934,0.446,0.341,0.921,0.997,0.102,0.225,0.187,0.901,0.13,0.376,0.424,0.962,0.79,0.098,0.199,0.268,0.263,0.984,0.879,0.218,0.291,0.887,0.922,0.844,0.173,0.747,0.406,0.9,0.887,0.98,0.93,0.402,0.787,0.954,0.412,0.916,0.305,0.911,0.913,0.926,0.909,0.264,0.057,0.051,0.16,0.967,0.213,0.077,0.878,0.205,0.878,0.113,0.22,0.519,0.082,0.137,0.831,0.824,0.409,0.824,0.273,0.318,0.081,0.182,0.635,0.81,0.862,0.201,0.534,0.827,0.494,0.914,0.436,0.865,0.835,0.065,0.422,0.736,0.905,0.081,0.674,0.201,0.912,0.373,0.638,0.04,0.758,0.513,0.865,0.341,0.736,0.626,0.786,0.812,0.061,0.722,0.648,0.855,0.131,0.383,0.21,0.373,0.153,0.537,0.407,0.402,0.799,0.293,0.623,0.156,0.118,0.697,0.871,0.854,0.763,0.277,0.866,0.765,0.09,0.502,0.137,0.85,0.236,0.101,0.936,0.937,0.949,0.937,0.984,0.744,0.912,0.536,0.654,0.912,0.738,0.941,0.946,0.912,0.686,0.811,0.746,0.854,0.429,0.898,0.379,0.517,0.911,0.487,0.364,0.316,0.273,0.979,0.91,0.946,0.153,0.88,0.823,0.995,0.972,0.926,0.948,0.823,0.103,0.822,0.204,0.321,0.912,0.378,0.318,0.262,0.928,0.858,0.116,0.899,0.254,0.917,0.88,0.194,0.535,0.61,0.289,0.462,0.202,0.788,0.891,0.429,0.138,0.262,0.888,0.171,0.778,0.711,0.077,0.722,0.143,0.925,0.227,0.081,0.889,0.824,0.144,0.676,0.72,0.203,0.829,0.979,0.839,0.773,0.823,0.929,0.931,0.956,0.168,0.794,0.811,0.35,0.265,0.225,0.126,0.68,0.639,0.101,0.159,0.126,0.139,0.9,0.932,0.184,0.219,0.198,0.307,0.385,0.317,0.064,0.204,0.765,0.142,0.383,0.936,0.529,0.42,0.241,0.328,0.804,0.343,0.933,0.335,0.355,0.372,0.782,0.792,0.69,0.566,0.231,0.468,0.747,0.248,0.138,0.317,0.182,0.395,0.496,0.924,0.928,0.827,0.454,0.917,0.881,0.693,0.956,0.289,0.487,0.178,0.165,0.429,0.887,0.862,0.437,0.224,0.133,0.189,0.227,0.875,0.446,0.332,0.193,0.333,0.428,0.676,0.243,0.804,0.738,0.504,0.816,0.949,0.456,0.244,0.691,0.813,0.591,0.886,0.574,0.179,0.198,0.689,0.31,0.3,0.26,0.415,0.217,0.17,0.183,0.436,0.494,0.547,0.961,0.93,0.151,0.433,0.5,0.889,0.707,0.184,0.281,0.752,0.229,0.437,0.227,0.575,0.369,0.08,0.588,0.512,0.36,0.145,0.369,0.236,0.279,0.751,0.237,0.338,0.955,0.464,0.213,0.825,0.349,0.903,0.443,0.121,0.348,0.423,0.87,0.595,0.117,0.174,0.874,0.912,0.328,0.286,0.238,0.135,0.154,0.28,0.172,0.273,0.884,0.279,0.781,0.091,0.187,0.101,0.389,0.82,0.932,0.094,0.721,0.244,0.903,0.772,0.93,0.91,0.323,0.217,0.127,0.28,0.295,0.391,0.579,0.287,0.806,0.73,0.383,0.42,0.877,0.63,0.564,0.346,0.254,0.597,0.421,0.261,0.939,0.295,0.608,0.739,0.371,0.889,0.706,0.23,0.446,0.107,0.29,0.927,0.785,0.145,0.741,0.149,0.935,0.554,0.293,0.351,0.17,0.56,0.179,0.291,0.319,0.648,0.653,0.396,0.558,0.576,0.941,0.253,0.758,0.862,0.357,0.22,0.303,0.32,0.725,0.215,0.837,0.253,0.284,0.891,0.204,0.269,0.32,0.797,0.213,0.797,0.26,0.572,0.456,0.558,0.28,0.674,0.371,0.091,0.132,0.303,0.575,0.334,0.294,0.384,0.913,0.173,0.875,0.494,0.884,0.26,0.505,0.266,0.452,0.846,0.858,0.308,0.891,0.818,0.6,0.805,0.733,0.785,0.771,0.691,0.221,0.51,0.257,0.155,0.289,0.78,0.658,0.32,0.188,0.198,0.249,0.19,0.13,0.256,0.248,0.261,0.592,0.77,0.751,0.205,0.519,0.549,0.524,0.668,0.297,0.333,0.268,0.807,0.348,0.252,0.382,0.471,0.196,0.633,0.268,0.336,0.857,0.266,0.193,0.178,0.589,0.466,0.281,0.814,0.276,0.549,0.393,0.442,0.255,0.888,0.739,0.878,0.663,0.669,0.238,0.795,0.343,0.339,0.419,0.396,0.23,0.282,0.335,0.093,0.842,0.224,0.291,0.215,0.714,0.449,0.694,0.336,0.145,0.472,0.867,0.297,0.334,0.315,0.319,0.573,0.726,0.9,0.237,0.417,0.847,0.232,0.284,0.195,0.241,0.862,0.624,0.312,0.336,0.943,0.237,0.287,0.5,0.667,0.747,0.261,0.337,0.202,0.394,0.803,0.244,0.199,0.471,0.726,0.327,0.399,0.792,0.713,0.436,0.281,0.186,0.619,0.297,0.198,0.188,0.503,0.629,0.689,0.475,0.293,0.421,0.268,0.888,0.252,0.331,0.293,0.23,0.519,0.224,0.395,0.743,0.129,0.345,0.196,0.373,0.456,0.315'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.5\n",
    "sub30 = pd.merge(pred_test_rf,pred_test_lgbm_cold,how='left',left_index=True,right_index=True)\n",
    "sub30['pred'] = th*sub30['pred_x']+(1-th)*sub30['pred_y']\n",
    "sub30['pred'] = sub30['pred'].apply(lambda x: round(x,3))\n",
    "sub30_txt = ''\n",
    "for prob in list(sub30['pred'].values):\n",
    "    sub30_txt = sub30_txt+','+str(prob)\n",
    "sub30_txt = sub30_txt[1:]\n",
    "\n",
    "sub30_txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce25703",
   "metadata": {},
   "source": [
    "### Submission 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de792821",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_and_val = pd.concat([X_train,X_val])\n",
    "y_train_and_val = pd.concat([y_train,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d71d2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(max_depth=2,learning_rate=0.1,n_estimators=200,min_child_samples=20,colsample_bytree=1)\n",
    "lgbm.fit(X_train_and_val,y_train_and_val)\n",
    "\n",
    "pred_train = pd.DataFrame(lgbm.predict_proba(X_train)[:,1],columns=['pred'],index=X_train.index)\n",
    "pred_val = pd.DataFrame(lgbm.predict_proba(X_val)[:,1],columns=['pred'],index=X_val.index)\n",
    "pred_test = pd.DataFrame(lgbm.predict_proba(X_test)[:,1],columns=['pred'],index=X_test.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f98a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.9322050843804972\n",
      "Val ROC:  0.9282218225293547\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a88d0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.103331</td>\n",
       "      <td>0.243557</td>\n",
       "      <td>0.346888</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5         0.103331         0.243557        0.346888       0.111262   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.222222      0.333484  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58b5bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_lgbm_cold = pred_train.copy()\n",
    "pred_val_lgbm_cold = pred_val.copy()\n",
    "pred_test_lgbm_cold = pred_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03a535ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\cidem\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:46:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"class_weight\", \"criterion\", \"min_child_samples\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:46:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rf = XGBClassifier(max_depth=50,criterion='gini',n_estimators=200,min_child_samples=20,colsample_bytree=0.5,class_weight= 'balanced')\n",
    "rf.fit(X_train_and_val,y_train_and_val)\n",
    "\n",
    "pred_train = pd.DataFrame(rf.predict_proba(X_train_imputed)[:,1],columns=['pred'],index=X_train_imputed.index)\n",
    "pred_val = pd.DataFrame(rf.predict_proba(X_val_imputed)[:,1],columns=['pred'],index=X_val_imputed.index)\n",
    "pred_test = pd.DataFrame(rf.predict_proba(X_test_imputed)[:,1],columns=['pred'],index=X_test_imputed.index)\n",
    "\n",
    "pred_train = pd.merge(pred_train,y_train,how='left',left_index=True, right_index=True)\n",
    "pred_val = pd.merge(pred_val,y_val,how='left',left_index=True, right_index=True)\n",
    "\n",
    "roc_train = roc_auc_score(pred_train['female_label'],pred_train['pred'])\n",
    "roc_val = roc_auc_score(pred_val['female_label'],pred_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1b9735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC:  0.8046013739864177\n",
      "Val ROC:  0.821288203100074\n"
     ]
    }
   ],
   "source": [
    "print('Train ROC: ',roc_train)\n",
    "print('Val ROC: ',roc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6872c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>err_rate1_train</th>\n",
       "      <th>err_rate0_train</th>\n",
       "      <th>err_rate_train</th>\n",
       "      <th>err_rate1_val</th>\n",
       "      <th>err_rate0_val</th>\n",
       "      <th>err_rate_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.947165</td>\n",
       "      <td>0.948864</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.943152</td>\n",
       "      <td>0.945866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  err_rate1_train  err_rate0_train  err_rate_train  err_rate1_val  \\\n",
       "0        0.5           0.0017         0.947165        0.948864       0.002714   \n",
       "\n",
       "   err_rate0_val  err_rate_val  \n",
       "0       0.943152      0.945866  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_rate_df = pd.DataFrame()\n",
    "\n",
    "for th in [0.5]:\n",
    "\n",
    "    pred_train['pred_binary'] = np.where(pred_train['pred']>=th,1,0)\n",
    "    pred_val['pred_binary'] = np.where(pred_val['pred']>=th,1,0)\n",
    "\n",
    "    err_rate1_train = 1-(pred_train[pred_train['female_label']==1]['pred_binary'].sum()/pred_train[pred_train['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_train = pred_train[pred_train['female_label']==0]['pred_binary'].sum()/pred_train[pred_train['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_train = err_rate1_train+err_rate0_train\n",
    "\n",
    "    err_rate1_val = 1-(pred_val[pred_val['female_label']==1]['pred_binary'].sum()/pred_val[pred_val['female_label']==1]['pred_binary'].count())\n",
    "    err_rate0_val = pred_val[pred_val['female_label']==0]['pred_binary'].sum()/pred_val[pred_val['female_label']==0]['pred_binary'].count()\n",
    "    err_rate_val = err_rate1_val+err_rate0_val\n",
    "\n",
    "    err_rate_df_tmp = pd.DataFrame({'threshold':[th],\n",
    "                                    'err_rate1_train':[err_rate1_train],\n",
    "                                    'err_rate0_train':[err_rate0_train],\n",
    "                                    'err_rate_train':[err_rate_train],\n",
    "                                    'err_rate1_val':[err_rate1_val],\n",
    "                                    'err_rate0_val':[err_rate0_val],\n",
    "                                    'err_rate_val':[err_rate_val]\n",
    "                                   })\n",
    "    err_rate_df = pd.concat([err_rate_df,err_rate_df_tmp])\n",
    "\n",
    "err_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dab35121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = pred_train.copy()\n",
    "pred_val_rf = pred_val.copy()\n",
    "pred_test_rf = pred_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a39bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>female_label</th>\n",
       "      <th>pred_binary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999894</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994198</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992091</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999467</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>0.933481</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>0.961368</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>0.942432</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>0.989745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.965862</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4494 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred  female_label  pred_binary\n",
       "unique_id                                     \n",
       "1          0.999894             1            1\n",
       "2          0.999960             1            1\n",
       "3          0.994198             1            1\n",
       "4          0.992091             1            1\n",
       "5          0.999467             1            1\n",
       "...             ...           ...          ...\n",
       "7987       0.933481             1            1\n",
       "7988       0.961368             0            1\n",
       "7989       0.942432             0            1\n",
       "7992       0.989745             1            1\n",
       "7996       0.965862             1            1\n",
       "\n",
       "[4494 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5465f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.982,0.962,0.972,0.997,0.964,0.995,0.731,0.605,0.961,0.984,0.652,0.482,0.992,0.997,0.93,0.993,0.449,0.525,0.987,0.969,0.994,0.934,0.981,0.988,0.998,0.971,0.99,0.997,0.977,0.937,0.996,0.905,0.971,0.992,0.994,0.995,0.834,0.897,0.993,0.959,0.964,0.931,0.896,0.935,0.991,0.975,0.996,0.724,0.86,0.811,0.571,0.995,0.966,0.483,0.655,0.529,0.984,0.987,0.976,0.992,0.99,0.888,0.434,0.989,0.995,0.917,0.995,0.959,0.978,0.988,0.998,0.983,0.978,0.986,0.954,0.92,0.971,0.997,0.993,0.926,0.49,0.929,0.988,0.435,0.994,0.962,0.896,0.995,0.982,0.95,0.998,0.993,0.913,0.99,0.667,0.974,0.974,0.906,0.653,0.978,0.978,0.988,0.984,0.993,0.824,0.822,0.802,0.757,0.987,0.78,0.98,0.999,0.975,0.997,0.725,0.925,0.813,0.98,0.974,0.975,0.998,0.998,0.718,0.903,0.965,0.988,0.988,0.988,0.545,0.571,0.996,0.935,0.523,0.978,0.932,0.993,0.856,0.986,0.825,0.953,0.7,0.859,0.988,0.978,0.801,0.993,0.763,0.986,0.45,0.985,0.994,0.994,0.985,0.996,0.69,0.936,0.961,0.979,0.988,0.857,0.996,0.843,0.993,0.883,0.993,0.987,0.841,0.958,0.978,0.602,0.99,0.748,0.879,0.996,0.996,0.932,0.993,0.928,0.679,0.976,0.695,0.376,0.972,0.709,0.697,0.992,0.989,0.995,0.992,0.976,0.993,0.556,0.8,0.965,0.97,0.488,0.986,0.665,0.98,0.959,0.96,0.994,0.997,0.995,0.918,0.815,0.552,0.603,0.527,0.42,0.99,0.984,0.66,0.998,0.987,0.72,0.995,0.991,0.87,0.99,0.933,0.991,0.723,0.986,0.997,0.725,0.566,0.993,0.684,0.58,0.399,0.948,0.918,0.996,0.48,0.988,0.995,0.954,0.996,0.992,0.433,0.991,0.97,0.983,0.995,0.587,0.94,0.452,0.989,0.975,0.877,0.563,0.989,0.996,0.188,0.896,0.997,0.99,0.665,0.555,0.995,0.905,0.577,0.996,0.995,0.997,0.92,0.997,0.987,0.558,0.964,0.927,0.992,0.675,0.99,0.94,0.982,0.99,0.992,0.967,0.996,0.981,0.508,0.99,0.857,0.557,0.995,0.968,0.707,0.993,0.551,0.632,0.617,0.986,0.823,0.608,0.98,0.503,0.832,0.997,0.995,0.987,0.981,0.984,0.998,0.99,0.962,0.993,0.997,0.978,0.998,0.96,0.564,0.972,0.987,0.611,0.997,0.716,0.89,0.763,0.956,0.964,0.524,0.848,0.994,0.948,0.632,0.933,0.902,0.771,0.563,0.994,0.416,0.996,0.918,0.906,0.965,0.989,0.526,0.926,0.958,0.971,0.938,0.998,0.958,0.995,0.973,0.567,0.962,0.996,0.583,0.58,0.988,0.977,0.676,0.99,0.956,0.994,0.998,0.999,0.993,0.98,0.619,0.951,0.978,0.992,0.556,0.99,0.977,0.603,0.989,0.967,0.974,0.995,0.933,0.978,0.983,0.989,0.987,0.936,0.947,0.994,0.507,0.811,0.992,0.988,0.471,0.996,0.995,0.967,0.59,0.995,0.71,0.997,0.954,0.993,0.998,0.999,0.921,0.974,0.935,0.986,0.993,0.927,0.995,0.948,0.612,0.569,0.911,0.37,0.631,0.616,0.915,0.85,0.962,0.511,0.98,0.567,0.731,0.859,0.995,0.953,0.873,0.981,0.524,0.527,0.76,0.589,0.992,0.954,0.886,0.576,0.529,0.996,0.848,0.968,0.99,0.956,0.608,0.975,0.445,0.987,0.994,0.75,0.987,0.915,0.585,0.997,0.464,0.676,0.945,0.978,0.588,0.984,0.927,0.974,0.998,0.994,0.954,0.992,0.993,0.722,0.96,0.932,0.9,0.984,0.596,0.744,0.45,0.967,0.925,0.706,0.543,0.982,0.934,0.976,0.913,0.991,0.629,0.91,0.51,0.979,0.994,0.319,0.553,0.575,0.978,0.983,0.986,0.571,0.236,0.917,0.639,0.963,0.836,0.994,0.992,0.857,0.746,0.553,0.993,0.963,0.996,0.802,0.404,0.994,0.624,0.995,0.994,0.653,0.959,0.983,0.953,0.795,0.589,0.503,0.54,0.644,0.865,0.51,0.784,0.685,0.502,0.57,0.986,0.987,0.992,0.995,0.997,0.98,0.494,0.99,0.872,0.479,0.995,0.905,0.751,0.582,0.975,0.948,0.608,0.992,0.67,0.684,0.962,0.636,0.543,0.988,0.748,0.718,0.992,0.991,0.973,0.989,0.962,0.988,0.457,0.93,0.665,0.97,0.992,0.653,0.849,0.985,0.484,0.966,0.983,0.682,0.627,0.951,0.991,0.881,0.529,0.385,0.989,0.991,0.672,0.964,0.991,0.996,0.989,0.617,0.979,0.993,0.66,0.997,0.994,0.832,0.991,0.972,0.965,0.994,0.911,0.632,0.944,0.868,0.997,0.613,0.569,0.967,0.983,0.998,0.825,0.992,0.628,0.856,0.755,0.996,0.994,0.986,0.997,0.983,0.515,0.839,0.523,0.991,0.943,0.553,0.554,0.569,0.997,0.979,0.973,0.982,0.991,0.425,0.354,0.993,0.989,0.987,0.931,0.947,0.759,0.524,0.986,0.753,0.627,0.541,0.972,0.997,0.994,0.993,0.977,0.803,0.99,0.594,0.717,0.974,0.998,0.96,0.99,0.564,0.471,0.966,0.976,0.981,0.949,0.548,0.99,0.989,0.873,0.974,0.607,0.72,0.571,0.689,0.995,0.997,0.991,0.966,0.63,0.987,0.956,0.765,0.908,0.892,0.983,0.97,0.58,0.707,0.669,0.563,0.865,0.991,0.963,0.986,0.552,0.876,0.924,0.798,0.59,0.642,0.833,0.944,0.963,0.915,0.998,0.999,0.553,0.955,0.973,0.982,0.61,0.992,0.902,0.959,0.994,0.714,0.991,0.635,0.998,0.649,0.978,0.572,0.972,0.972,0.975,0.977,0.91,0.658,0.922,0.863,0.995,0.659,0.986,0.943,0.435,0.998,0.421,0.588,0.999,0.932,0.946,0.986,0.966,0.945,0.996,0.997,0.994,0.996,0.981,0.987,0.972,0.997,0.994,0.693,0.712,0.985,0.729,0.979,0.989,0.591,0.852,0.988,0.693,0.942,0.991,0.627,0.676,0.989,0.994,0.994,0.528,0.921,0.998,0.962,0.938,0.733,0.989,0.868,0.54,0.973,0.996,0.604,0.857,0.626,0.539,0.866,0.994,0.929,0.985,0.713,0.981,0.945,0.997,0.521,0.579,0.931,0.996,0.538,0.52,0.983,0.826,0.831,0.888,0.701,0.986,0.555,0.994,0.918,0.627,0.542,0.958,0.402,0.883,0.999,0.967,0.944,0.994,0.995,0.998,0.659,0.985,0.879,0.93,0.963,0.994,0.996,0.977,0.995,0.992,0.987,0.922,0.941,0.864,0.735,0.971,0.941,0.782,0.997,0.969,0.985,0.933,0.733,0.673,0.964,0.725,0.53,0.926,0.59,0.991,0.901,0.998,0.993,0.974,0.983,0.99,0.888,0.993,0.941,0.925,0.979,0.937,0.716,0.715,0.995,0.974,0.655,0.997,0.966,0.977,0.995,0.802,0.998,0.618,0.481,0.577,0.989,0.931,0.664,0.994,0.828,0.991,0.767,0.906,0.99,0.983,0.99,0.508,0.687,0.667,0.678,0.976,0.919,0.886,0.614,0.997,0.835,0.801,0.587,0.996,0.64,0.612,0.746,0.992,0.983,0.939,0.698,0.559,0.792,0.63,0.973,0.914,0.724,0.596,0.949,0.525,0.472,0.845,0.931,0.669,0.641,0.999,0.991,0.693,0.987,0.997,0.976,0.742,0.988,0.958,0.402,0.99,0.955,0.954,0.534,0.971,0.758,0.746,0.547,0.984,0.997,0.991,0.991,0.645,0.986,0.996,0.451,0.982,0.869,0.996,0.605,0.722,0.999,0.988,0.989,0.976,0.993,0.988,0.484,0.993,0.934,0.517,0.996,0.914,0.994,0.993,0.996,0.976,0.693,0.992,0.815,0.652,0.935,0.985,0.991,0.981,0.647,0.988,0.6,0.763,0.559,0.519,0.992,0.995,0.975,0.979,0.978,0.969,0.661,0.995,0.813,0.912,0.648,0.903,0.598,0.991,0.841,0.927,0.994,0.974,0.997,0.802,0.994,0.574,0.992,0.943,0.959,0.962,0.978,0.995,0.826,0.744,0.974,0.988,0.923,0.787,0.447,0.993,0.907,0.651,0.996,0.85,0.955,0.978,0.619,0.658,0.996,0.756,0.996,0.472,0.958,0.801,0.995,0.557,0.835,0.649,0.774,0.688,0.966,0.985,0.991,0.986,0.793,0.992,0.748,0.864,0.559,0.813,0.972,0.593,0.626,0.581,0.657,0.461,0.645,0.985,0.993,0.931,0.99,0.995,0.689,0.939,0.933,0.882,0.98,0.99,0.567,0.843,0.912,0.843,0.75,0.997,0.993,0.92,0.997,0.98,0.971,0.977,0.75,0.974,0.608,0.972,0.978,0.884,0.986,0.933,0.996,0.787,0.984,0.702,0.707,0.685,0.644,0.797,0.956,0.695,0.903,0.67,0.502,0.652,0.741,0.848,0.56,0.732,0.651,0.908,0.978,0.883,0.995,0.753,0.533,0.654,0.799,0.909,0.721,0.751,0.912,0.671,0.644,0.637,0.993,0.994,0.898,0.372,0.744,0.423,0.986,0.901,0.803,0.58,0.998,0.688,0.501,0.831,0.946,0.678,0.726,0.953,0.794,0.498,0.966,0.531,0.996,0.711,0.901,0.967,0.851,0.992,0.517,0.735,0.914,0.76,0.772,0.987,0.695,0.681,0.609,0.941,0.862,0.886,0.592,0.96,0.985,0.989,0.718,0.985,0.995,0.661,0.362,0.617,0.81,0.995,0.993,0.959,0.955,0.801,0.98,0.948,0.815,0.775,0.996,0.499,0.954,0.64,0.996,0.739,0.87,0.522,0.986,0.942,0.746,0.517,0.566,0.979,0.856,0.966,0.56,0.992,0.487,0.47,0.976,0.526,0.529,0.867,0.598,0.949,0.831,0.766,0.719,0.742,0.817,0.764,0.67,0.932,0.58,0.768,0.916,0.768,0.971,0.989,0.587,0.988,0.979,0.99,0.871,0.998,0.891,0.88,0.945,0.956,0.997,0.821,0.655,0.55,0.58,0.836,0.362,0.855,0.818,0.456,0.597,0.974,0.735,0.987,0.99,0.674,0.997,0.617,0.693,0.323,0.611,0.994,0.968,0.542,0.827,0.987,0.996,0.982,0.943,0.831,0.996,0.936,0.669,0.986,0.756,0.995,0.687,0.855,0.777,0.993,0.898,0.996,0.707,0.486,0.616,0.734,0.995,0.991,0.706,0.962,0.576,0.828,0.97,0.984,0.888,0.431,0.985,0.699,0.996,0.992,0.551,0.951,0.98,0.605,0.847,0.63,0.689,0.919,0.903,0.832,0.67,0.632,0.576,0.586,0.911,0.646,0.819,0.983,0.974,0.994,0.858,0.536,0.968,0.645,0.994,0.646,0.977,0.571,0.861,0.877,0.977,0.566,0.58,0.948,0.718,0.863,0.997,0.707,0.548,0.588,0.953,0.993,0.779,0.988,0.632,0.455,0.607,0.771,0.994,0.653,0.987,0.838,0.655,0.794,0.984,0.714,0.898,0.755,0.726,0.506,0.997,0.709,0.846,0.623,0.833,0.864,0.984,0.998,0.675,0.994,0.78,0.548,0.809,0.8,0.609,0.73,0.939,0.764,0.932,0.998,0.671,0.594,0.625,0.943,0.716,0.606,0.987,0.544,0.643,0.953,0.985,0.787,0.972,0.868,0.645,0.745,0.596,0.727,0.716,0.736,0.738,0.67,0.711,0.666,0.979,0.632,0.6,0.602,0.808,0.726,0.983,0.683,0.692,0.783,0.686,0.671,0.671,0.817,0.635,0.771,0.594,0.664,0.983,0.481,0.861,0.8,0.788,0.634,0.985,0.665,0.801,0.78,0.66,0.786,0.583,0.726,0.14,0.88,0.965,0.614,0.627,0.571,0.962,0.772,0.658,0.952,0.962,0.62,0.565,0.582,0.741,0.626,0.816,0.996,0.89,0.814,0.781,0.651,0.429,0.917,0.774,0.812,0.592,0.708,0.994,0.713,0.549,0.485,0.79,0.771,0.65,0.614,0.699,0.935,0.595,0.81,0.953,0.62,0.989,0.965,0.603,0.648,0.98,0.562,0.819,0.526,0.985,0.937,0.706,0.71,0.827,0.763,0.583,0.977,0.843,0.679,0.99,0.632,0.695,0.959,0.996,0.981,0.964,0.789,0.955,0.992,0.704,0.922,0.721,0.901,0.514,0.964,0.968,0.715,0.639,0.962,0.997,0.507,0.814,0.705,0.987,0.996,0.989,0.752,0.988,0.913,0.991,0.927,0.97,0.533,0.703,0.507,0.985,0.527,0.953,0.617,0.94,0.738,0.855,0.88,0.808,0.993,0.944,0.787,0.991,0.999,0.902,0.747,0.991,0.549,0.974,0.984,0.27,0.995,0.598,0.585,0.933,0.691,0.952,0.998,0.887,0.978,0.976,0.59,0.474,0.526,0.892,0.677,0.623,0.588,0.944,0.556,0.633,0.992,0.638,0.974,0.942,0.993,0.98,0.667,0.608,0.984,0.312,0.885,0.959,0.991,0.973,0.667,0.821,0.923,0.992,0.955,0.959,0.969,0.458,0.901,0.534,0.684,0.973,0.61,0.903,0.596,0.783,0.984,0.521,0.916,0.739,0.758,0.976,0.992,0.914,0.573,0.994,0.384,0.947,0.783,0.672,0.96,0.99,0.99,0.851,0.805,0.975,0.578,0.574,0.645,0.594,0.888,0.646,0.628,0.989,0.964,0.698,0.972,0.994,0.586,0.576,0.975,0.645,0.961,0.83,0.968,0.575,0.984,0.49,0.685,0.62,0.559,0.833,0.633,0.566,0.633,0.577,0.954,0.877,0.933,0.965,0.978,0.674,0.925,0.983,0.599,0.59,0.611,0.994,0.971,0.961,0.932,0.986,0.918,0.948,0.984,0.936,0.598,0.957,0.962,0.901,0.983,0.56,0.578,0.993,0.676,0.582,0.954,0.815,0.994,0.989,0.983,0.933,0.927,0.99,0.967,0.968,0.861,0.792,0.618,0.503,0.895,0.782,0.94,0.583,0.949,0.964,0.572,0.665,0.74,0.623,0.983,0.879,0.907,0.871,0.975,0.593,0.597,0.985,0.983,0.703,0.978,0.576,0.65,0.836,0.633,0.667,0.974,0.86,0.771,0.974,0.981,0.921,0.775,0.97,0.644,0.917,0.957,0.775,0.989,0.983,0.772,0.897,0.972,0.839,0.921,0.87,0.839,0.66,0.634,0.514,0.567,0.767,0.988,0.826,0.541,0.972,0.922,0.546,0.911,0.844,0.799,0.612,0.601,0.582,0.792,0.978,0.973,0.544,0.976,0.77,0.712,0.575,0.637,0.57,0.655,0.901,0.683,0.77,0.698,0.993,0.593,0.655,0.585,0.905,0.557,0.79,0.78,0.96,0.803,0.58,0.678,0.693,0.72,0.983,0.927,0.643,0.688,0.863,0.919,0.924,0.666,0.864,0.837,0.884,0.779,0.977,0.862,0.775,0.794,0.951,0.871,0.917,0.753,0.859,0.909,0.888,0.861,0.75,0.554,0.523,0.647,0.95,0.702,0.574,0.851,0.662,0.888,0.595,0.693,0.922,0.579,0.631,0.861,0.821,0.865,0.821,0.682,0.689,0.517,0.575,0.735,0.846,0.859,0.662,0.828,0.867,0.816,0.947,0.691,0.87,0.896,0.55,0.792,0.835,0.9,0.564,0.834,0.673,0.927,0.823,0.83,0.526,0.851,0.876,0.839,0.794,0.831,0.827,0.83,0.798,0.552,0.842,0.885,0.835,0.621,0.75,0.7,0.76,0.62,0.771,0.767,0.743,0.848,0.751,0.508,0.626,0.566,0.924,0.79,0.849,0.801,0.71,0.866,0.803,0.393,0.704,0.618,0.858,0.696,0.584,0.944,0.922,0.964,0.91,0.984,0.765,0.903,0.962,0.811,0.916,0.833,0.932,0.879,0.891,0.871,0.882,0.757,0.794,0.69,0.874,0.632,0.785,0.93,0.751,0.775,0.771,0.721,0.96,0.926,0.946,0.631,0.842,0.926,0.978,0.97,0.878,0.946,0.864,0.595,0.842,0.67,0.74,0.907,0.77,0.768,0.748,0.916,0.857,0.597,0.86,0.701,0.912,0.887,0.678,0.714,0.878,0.766,0.789,0.652,0.874,0.889,0.727,0.576,0.633,0.907,0.651,0.889,0.737,0.544,0.85,0.625,0.904,0.693,0.545,0.871,0.872,0.607,0.813,0.905,0.676,0.909,0.947,0.9,0.697,0.817,0.905,0.892,0.952,0.658,0.672,0.868,0.792,0.671,0.649,0.62,0.784,0.861,0.541,0.639,0.613,0.579,0.94,0.879,0.675,0.686,0.673,0.79,0.733,0.766,0.535,0.642,0.885,0.633,0.743,0.943,0.791,0.84,0.684,0.775,0.917,0.734,0.894,0.802,0.813,0.793,0.625,0.872,0.833,0.889,0.723,0.8,0.751,0.72,0.629,0.718,0.674,0.82,0.845,0.917,0.906,0.826,0.779,0.873,0.831,0.813,0.951,0.767,0.76,0.656,0.641,0.774,0.872,0.887,0.723,0.713,0.631,0.677,0.72,0.889,0.86,0.786,0.639,0.709,0.698,0.745,0.649,0.901,0.906,0.787,0.863,0.943,0.809,0.542,0.726,0.824,0.649,0.797,0.818,0.654,0.633,0.799,0.73,0.714,0.713,0.7,0.697,0.655,0.666,0.756,0.858,0.758,0.94,0.924,0.618,0.821,0.882,0.837,0.824,0.658,0.729,0.738,0.68,0.809,0.633,0.83,0.811,0.548,0.73,0.732,0.754,0.637,0.785,0.644,0.715,0.908,0.656,0.757,0.948,0.83,0.633,0.858,0.8,0.805,0.664,0.583,0.731,0.743,0.841,0.788,0.612,0.65,0.925,0.911,0.803,0.764,0.69,0.595,0.645,0.733,0.584,0.681,0.902,0.687,0.816,0.57,0.667,0.507,0.859,0.787,0.808,0.584,0.773,0.74,0.711,0.708,0.885,0.805,0.668,0.504,0.619,0.712,0.774,0.81,0.773,0.732,0.774,0.825,0.816,0.761,0.845,0.77,0.791,0.788,0.744,0.761,0.655,0.674,0.772,0.762,0.792,0.818,0.825,0.891,0.85,0.708,0.816,0.604,0.768,0.916,0.851,0.622,0.841,0.622,0.834,0.692,0.663,0.657,0.657,0.683,0.657,0.781,0.787,0.833,0.74,0.787,0.809,0.833,0.917,0.638,0.864,0.746,0.731,0.67,0.788,0.724,0.651,0.704,0.837,0.707,0.746,0.893,0.671,0.691,0.759,0.832,0.683,0.862,0.694,0.751,0.756,0.764,0.746,0.841,0.793,0.585,0.611,0.764,0.811,0.802,0.755,0.771,0.688,0.64,0.858,0.729,0.835,0.741,0.772,0.628,0.777,0.478,0.598,0.623,0.707,0.822,0.77,0.816,0.735,0.854,0.826,0.799,0.699,0.807,0.745,0.605,0.731,0.71,0.747,0.769,0.68,0.676,0.712,0.648,0.584,0.748,0.699,0.702,0.785,0.885,0.854,0.69,0.83,0.779,0.803,0.71,0.759,0.735,0.681,0.799,0.742,0.74,0.723,0.784,0.638,0.783,0.749,0.774,0.805,0.757,0.677,0.627,0.842,0.799,0.725,0.853,0.743,0.752,0.858,0.816,0.737,0.884,0.777,0.84,0.796,0.782,0.656,0.789,0.707,0.741,0.765,0.751,0.684,0.688,0.808,0.532,0.802,0.714,0.703,0.707,0.857,0.797,0.776,0.759,0.597,0.868,0.864,0.745,0.742,0.75,0.739,0.789,0.804,0.863,0.725,0.754,0.687,0.708,0.734,0.646,0.719,0.862,0.842,0.787,0.805,0.869,0.704,0.731,0.72,0.754,0.773,0.741,0.747,0.665,0.798,0.777,0.736,0.688,0.813,0.859,0.793,0.806,0.744,0.732,0.784,0.703,0.673,0.777,0.731,0.684,0.666,0.777,0.832,0.717,0.794,0.774,0.777,0.729,0.818,0.626,0.687,0.681,0.64,0.77,0.7,0.756,0.827,0.62,0.714,0.637,0.768,0.751,0.793'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.5\n",
    "sub31 = pd.merge(pred_test_rf,pred_test_lgbm_cold,how='left',left_index=True,right_index=True)\n",
    "sub31['pred'] = th*sub31['pred_x']+(1-th)*sub31['pred_y']\n",
    "sub31['pred'] = sub31['pred'].apply(lambda x: round(x,3))\n",
    "sub31_txt = ''\n",
    "for prob in list(sub31['pred'].values):\n",
    "    sub31_txt = sub31_txt+','+str(prob)\n",
    "sub31_txt = sub31_txt[1:]\n",
    "\n",
    "sub31_txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
